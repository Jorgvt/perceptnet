{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 13:56:14.904561: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-12 13:56:15.018291: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-12 13:56:17.732651: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import re\n",
    "from fastcore.xtras import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Any, Callable, Sequence, Union\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "import jax\n",
    "from jax import lax, random, numpy as jnp\n",
    "import flax\n",
    "from flax.core import freeze, unfreeze, FrozenDict\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "from flax.training import orbax_utils\n",
    "\n",
    "import optax\n",
    "import orbax.checkpoint\n",
    "\n",
    "from clu import metrics\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "from einops import reduce, rearrange\n",
    "import wandb\n",
    "from iqadatasets.datasets import *\n",
    "from fxlayers.layers import *\n",
    "from fxlayers.layers import GaborLayerLogSigma_, GaussianLayerGamma, FreqGaussianGamma, OrientGaussianGamma\n",
    "from fxlayers.initializers import *\n",
    "from JaxPlayground.utils.constraints import *\n",
    "from JaxPlayground.utils.wandb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A_GDNSPATIOFREQORIENT: true\n",
       "BATCH_SIZE: 16\n",
       "CS_KERNEL_SIZE: 21\n",
       "EPOCHS: 500\n",
       "GABOR_KERNEL_SIZE: 31\n",
       "GDNGAUSSIAN_KERNEL_SIZE: 11\n",
       "GDN_CLIPPING: true\n",
       "INIT_JH: true\n",
       "LEARNING_RATE: 0.003\n",
       "NORMALIZE_ENERGY: true\n",
       "NORMALIZE_PROB: false\n",
       "N_ORIENTATIONS: 16\n",
       "N_SCALES: 4\n",
       "SEED: 42\n",
       "USE_BIAS: false\n",
       "ZERO_MEAN: true"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"BATCH_SIZE\": 16,\n",
    "    \"EPOCHS\": 500,\n",
    "    \"LEARNING_RATE\": 3e-3,\n",
    "    \"SEED\": 42,\n",
    "    \"GDN_CLIPPING\": True,\n",
    "    \"NORMALIZE_PROB\": False,\n",
    "    \"NORMALIZE_ENERGY\": True,\n",
    "    \"ZERO_MEAN\": True,\n",
    "    \"USE_BIAS\": False,\n",
    "    \"CS_KERNEL_SIZE\": 21,\n",
    "    \"GDNGAUSSIAN_KERNEL_SIZE\": 11,\n",
    "    \"GABOR_KERNEL_SIZE\": 31,\n",
    "    \"N_SCALES\": 4,\n",
    "    \"N_ORIENTATIONS\": 16,\n",
    "    \"INIT_JH\": True,\n",
    "    \"A_GDNSPATIOFREQORIENT\": True,\n",
    "}\n",
    "config = ConfigDict(config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000, 5000, 5000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# path_imgs = Path(\"/lustre/ific.uv.es/ml/uv075/Databases/Salicon/images/\")\n",
    "path_imgs = Path(\"/media/disk/databases/BBDD_video_image/Salicon/images/\")\n",
    "path_imgs_train = [str(p) for p in path_imgs.joinpath(\"train\").glob(\"*.jpg\")]\n",
    "path_imgs_val = [str(p) for p in path_imgs.joinpath(\"val\").glob(\"*.jpg\")]\n",
    "path_labels_train = [re.sub(r\"(images)|(jpg)\", lambda x: {'images': 'maps', 'jpg': 'png'}[x.group(0)], p) for p in path_imgs_train]\n",
    "path_labels_val = [re.sub(r\"(images)|(jpg)\", lambda x: {'images': 'maps', 'jpg': 'png'}[x.group(0)], p) for p in path_imgs_val]\n",
    "len(path_imgs_train), len(path_labels_train), len(path_imgs_val), len(path_labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path_img,\n",
    "               path_label,\n",
    "               ):\n",
    "    img_ref = tf.io.read_file(path_img)\n",
    "    img_dist = tf.io.read_file(path_label)\n",
    "\n",
    "    img_ref = tf.image.decode_jpeg(img_ref, channels=3)\n",
    "    img_dist = tf.image.decode_png(img_dist, channels=1)\n",
    "\n",
    "    img_ref = tf.image.convert_image_dtype(img_ref, dtype=tf.float32)\n",
    "    img_dist = tf.image.convert_image_dtype(img_dist, dtype=tf.float32)\n",
    "\n",
    "    return img_ref, img_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_train = tf.data.Dataset.from_tensor_slices((path_imgs_train, path_labels_train))\\\n",
    "                           .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dst_val = tf.data.Dataset.from_tensor_slices((path_imgs_val, path_labels_val))\\\n",
    "                         .map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_train_rdy = dst_train.shuffle(buffer_size=1000, seed=config.SEED, reshuffle_each_iteration=True)\\\n",
    "                         .batch(64, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                         .prefetch(1)\n",
    "\n",
    "dst_val_rdy = dst_val.batch(64, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                     .prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 480, 640, 3]), TensorShape([64, 480, 640, 1]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = next(iter(dst_train_rdy))\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 480, 640, 3]), TensorShape([64, 480, 640, 1]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = next(iter(dst_val_rdy))\n",
    "a.shape, b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: jorgvt. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/disk/users/vitojor/perceptnet/Notebooks/13_JaX/13_05_V19/wandb/run-20240112_155549-0wf1moiz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jorgvt/PerceptNet_Saliency/runs/0wf1moiz' target=\"_blank\">V19_128GaborFree_A_NoGDN</a></strong> to <a href='https://wandb.ai/jorgvt/PerceptNet_Saliency' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jorgvt/PerceptNet_Saliency' target=\"_blank\">https://wandb.ai/jorgvt/PerceptNet_Saliency</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jorgvt/PerceptNet_Saliency/runs/0wf1moiz' target=\"_blank\">https://wandb.ai/jorgvt/PerceptNet_Saliency/runs/0wf1moiz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "A_GDNSPATIOFREQORIENT: true\n",
       "BATCH_SIZE: 16\n",
       "CS_KERNEL_SIZE: 21\n",
       "EPOCHS: 500\n",
       "GABOR_KERNEL_SIZE: 31\n",
       "GDNGAUSSIAN_KERNEL_SIZE: 11\n",
       "GDN_CLIPPING: true\n",
       "INIT_JH: true\n",
       "LEARNING_RATE: 0.003\n",
       "NORMALIZE_ENERGY: true\n",
       "NORMALIZE_PROB: false\n",
       "N_ORIENTATIONS: 16\n",
       "N_SCALES: 4\n",
       "SEED: 42\n",
       "USE_BIAS: false\n",
       "ZERO_MEAN: true"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"PerceptNet_Saliency\",\n",
    "           name=\"V19_128GaborFree_A_NoGDN\",\n",
    "           job_type=\"training\",\n",
    "           config=config,\n",
    "           mode=\"online\",\n",
    "           )\n",
    "config = config\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model we're going to use\n",
    "\n",
    "> It's going to be a very simple model just for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDNSpatioFreqOrient(nn.Module):\n",
    "    \"\"\"Generalized Divisive Normalization.\"\"\"\n",
    "    kernel_size: Union[int, Sequence[int]]\n",
    "    strides: int = 1\n",
    "    padding: str = \"SAME\"\n",
    "    # inputs_star: float = 1.\n",
    "    # outputs_star: Union[None, float] = None\n",
    "    fs: int = 1\n",
    "    apply_independently: bool = False\n",
    "    bias_init: Callable = nn.initializers.ones_init()\n",
    "    alpha: float = 2.\n",
    "    epsilon: float = 1/2 # Exponential of the denominator\n",
    "    eps: float = 1e-6 # Numerical stability in the denominator\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 fmean,\n",
    "                 theta_mean,\n",
    "                 train=False,\n",
    "                 ):\n",
    "        b, h, w, c = inputs.shape\n",
    "        bias = self.param(\"bias\",\n",
    "                          #equal_to(inputs_star/10),\n",
    "                          self.bias_init,\n",
    "                          (c,))\n",
    "        # is_initialized = self.has_variable(\"batch_stats\", \"inputs_star\")\n",
    "        # inputs_star = self.variable(\"batch_stats\", \"inputs_star\", lambda x: jnp.ones(x)*self.inputs_star, (len(self.inputs_star),))\n",
    "        # inputs_star_ = jnp.ones_like(inputs)*inputs_star.value\n",
    "        GL = GaussianLayerGamma(features=c, kernel_size=self.kernel_size, strides=self.strides, padding=\"VALID\", fs=self.fs, xmean=self.kernel_size/self.fs/2, ymean=self.kernel_size/self.fs/2, normalize_prob=config.NORMALIZE_PROB, normalize_energy=config.NORMALIZE_ENERGY, use_bias=False, feature_group_count=c)\n",
    "        FG = FreqGaussianGamma()\n",
    "        OG = OrientGaussianGamma()\n",
    "        outputs = GL(pad_same_from_kernel_size(inputs, kernel_size=self.kernel_size, mode=self.padding)**self.alpha, train=train)#/(self.kernel_size**2)\n",
    "        outputs = FG(outputs, fmean=fmean)\n",
    "        ## Reshape so that the orientations are the innermost dimmension\n",
    "        outputs = rearrange(outputs, \"b h w (phase theta f) -> b h w (phase f theta)\", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)\n",
    "        outputs = OG(outputs, theta_mean=theta_mean)\n",
    "        ## Recover original disposition\n",
    "        denom = rearrange(outputs, \"b h w (phase f theta) -> b h w (phase theta f)\", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)\n",
    "\n",
    "        ## Coef\n",
    "        # coef = GL(inputs_star_**self.alpha, train=train)#/(self.kernel_size**2)\n",
    "        # coef = FG(coef, fmean=fmean)\n",
    "        # coef = rearrange(coef, \"b h w (phase theta f) -> b h w (phase f theta)\", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)\n",
    "        # coef = OG(coef, theta_mean=theta_mean) + bias\n",
    "        # coef = rearrange(coef, \"b h w (phase f theta) -> b h w (phase theta f)\", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)\n",
    "        # coef = jnp.clip(coef+bias, a_min=1e-5)**self.epsilon\n",
    "        # # coef = inputs_star.value * coef\n",
    "        # if self.outputs_star is not None: coef = coef/inputs_star.value*self.outputs_star\n",
    "\n",
    "        # if is_initialized and train:\n",
    "        #     inputs_star.value = (inputs_star.value + jnp.quantile(jnp.abs(inputs), q=0.95, axis=(0,1,2)))/2\n",
    "        # return coef * inputs / (jnp.clip(denom+bias, a_min=1e-5)**self.epsilon + self.eps)\n",
    "        return inputs / (jnp.clip(denom+bias, a_min=1e-5)**self.epsilon + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptNet(nn.Module):\n",
    "    \"\"\"IQA model inspired by the visual system.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs, # Assuming fs = 128 (cpd)\n",
    "                 **kwargs,\n",
    "                 ):\n",
    "        ## (Independent) Color equilibration (Gamma correction)\n",
    "        ## Might need to be the same for each number\n",
    "        ## bias = 0.1 / kernel = 0.5\n",
    "        outputs = GDN(kernel_size=(1,1), apply_independently=True)(inputs)\n",
    "        \n",
    "        ## Color (ATD) Transformation\n",
    "        outputs = nn.Conv(features=3, kernel_size=(1,1), use_bias=False)(outputs)\n",
    "        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n",
    "        \n",
    "        ## GDN Star A - T - D [Separated]\n",
    "        outputs = GDN(kernel_size=(1,1), apply_independently=True)(outputs)\n",
    "\n",
    "        ## Center Surround (DoG)\n",
    "        ## Initialized so that 3 are positives and 3 are negatives and no interaction between channels is present\n",
    "        outputs = pad_same_from_kernel_size(outputs, kernel_size=config.CS_KERNEL_SIZE, mode=\"symmetric\")\n",
    "        outputs = CenterSurroundLogSigmaK(features=3, kernel_size=config.CS_KERNEL_SIZE, fs=21, use_bias=False, padding=\"VALID\")(outputs, **kwargs)\n",
    "        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n",
    "\n",
    "        ## GDN per channel with mean substraction in T and D (Spatial Gaussian Kernel)\n",
    "        ### fs = 32 / kernel_size = (11,11) -> 0.32 > 0.02 --> OK!\n",
    "        ## TO-DO: - Spatial Gaussian Kernel (0.02 deg) -> fs = 64/2 & 0.02*64/2 = sigma (px) = 0.69\n",
    "        outputs = GDNGaussian(kernel_size=config.GDNGAUSSIAN_KERNEL_SIZE, apply_independently=True, fs=32, padding=\"symmetric\", normalize_prob=config.NORMALIZE_PROB, normalize_energy=config.NORMALIZE_ENERGY)(outputs, **kwargs)\n",
    "\n",
    "        ## GaborLayer per channel with GDN mixing only same-origin-channel information\n",
    "        ### [Gaussian] sigma = 0.2 (deg) fs = 32 / kernel_size = (21,21) -> 21/32 = 0.66 --> OK!\n",
    "        outputs = pad_same_from_kernel_size(outputs, kernel_size=config.GABOR_KERNEL_SIZE, mode=\"symmetric\")\n",
    "        outputs, fmean, theta_mean = GaborLayerLogSigma_(n_scales=config.N_SCALES, n_orientations=config.N_ORIENTATIONS, kernel_size=config.GABOR_KERNEL_SIZE, fs=32, xmean=config.GABOR_KERNEL_SIZE/32/2, ymean=config.GABOR_KERNEL_SIZE/32/2, strides=1, padding=\"VALID\", normalize_prob=config.NORMALIZE_PROB, normalize_energy=config.NORMALIZE_ENERGY, zero_mean=config.ZERO_MEAN, use_bias=config.USE_BIAS)(outputs, return_freq=True, return_theta=True, **kwargs)\n",
    "        \n",
    "        ## Final GDN mixing Gabor information (?)\n",
    "        # outputs = GDNSpatioFreqOrient(kernel_size=21, strides=1, padding=\"symmetric\", fs=32, apply_independently=False)(outputs, fmean=fmean, theta_mean=theta_mean, **kwargs)\n",
    "\n",
    "        ## 1x1 Conv to reduce to a single channel\n",
    "        outputs = nn.Conv(features=1, kernel_size=(1,1), strides=(1,1), padding=\"SAME\", kernel_init=nn.initializers.ones_init())(outputs**2)\n",
    "        outputs = nn.relu(outputs)\n",
    "        \n",
    "        ## Make the images back to the original size\n",
    "        outputs = jax.image.resize(outputs, shape=(*inputs.shape[:-1], outputs.shape[-1]), method=\"bilinear\")\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the metrics with `clu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n",
    "    loss: metrics.Average.from_output(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `TrainState` doesn't include metrics, but it's very easy to subclass it so that it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    state: FrozenDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function that initializes the `TrainState` from a module, a rng key and some optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(module, key, tx, input_shape):\n",
    "    \"\"\"Creates the initial `TrainState`.\"\"\"\n",
    "    variables = module.init(key, jnp.ones(input_shape))\n",
    "    state, params = variables.pop('params')\n",
    "    return TrainState.create(\n",
    "        apply_fn=module.apply,\n",
    "        params=params,\n",
    "        state=state,\n",
    "        tx=tx,\n",
    "        metrics=Metrics.empty()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the training step\n",
    "\n",
    "> We want to write a function that takes the `TrainState` and a batch of data can performs an optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=2)\n",
    "def train_step(state, batch, return_grads=False):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    img, label = batch\n",
    "    def loss_fn(params):\n",
    "        ## Forward pass through the model\n",
    "        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=True)\n",
    "\n",
    "        ## Calculate the distance\n",
    "        loss = optax.l2_loss(img_pred, label)\n",
    "        \n",
    "        ## Calculate pearson correlation\n",
    "        return loss.mean(), updated_state\n",
    "    \n",
    "    (loss, updated_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metrics_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    state = state.replace(state=updated_state)\n",
    "    if return_grads: return state, grads\n",
    "    else: return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their example, they don't calculate the metrics at the same time. I think it is kind of a waste because it means having to perform a new forward pass, but we'll follow as of now. Let's define a function to perform metric calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "    \"\"\"Obtaining the metrics for a given batch.\"\"\"\n",
    "    img, label = batch\n",
    "    def loss_fn(params):\n",
    "        ## Forward pass through the model\n",
    "        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=False)\n",
    "\n",
    "        ## Calculate the distance\n",
    "        loss = optax.l2_loss(img_pred, label)\n",
    "        \n",
    "        ## Calculate pearson correlation\n",
    "        return loss.mean()\n",
    "    \n",
    "    metrics_updates = state.metrics.single_from_model_output(loss=loss_fn(state.params))\n",
    "    metrics = state.metrics.merge(metrics_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(PerceptNet(), random.PRNGKey(config.SEED), optax.adam(config.LEARNING_RATE), input_shape=(1,384,512,3))\n",
    "state = state.replace(params=clip_layer(state.params, \"GDN\", a_min=0))\n",
    "state = state.replace(params=clip_param(state.params, \"A\", a_min=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, _ = state.apply_fn({\"params\": state.params, **state.state}, jnp.ones(shape=(1,384,512,3)), train=True, mutable=list(state.state.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trainable(path):\n",
    "    # return (\"A\" in path) or (\"alpha_chrom_rg\" in path) or (\"alpha_chrom_yb\" in path)\n",
    "    if not config.A_GDNSPATIOFREQORIENT:\n",
    "        return (\"GDNSpatioFreqOrient_0\" in path) and (\"A\" in path)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    GDN_0: {\n",
       "        Conv_0: {\n",
       "            kernel: 'trainable',\n",
       "            bias: 'trainable',\n",
       "        },\n",
       "    },\n",
       "    Conv_0: {\n",
       "        kernel: 'trainable',\n",
       "    },\n",
       "    GDN_1: {\n",
       "        Conv_0: {\n",
       "            kernel: 'trainable',\n",
       "            bias: 'trainable',\n",
       "        },\n",
       "    },\n",
       "    CenterSurroundLogSigmaK_0: {\n",
       "        logsigma: 'trainable',\n",
       "        K: 'trainable',\n",
       "        A: 'trainable',\n",
       "    },\n",
       "    GDNGaussian_0: {\n",
       "        GaussianLayerGamma_0: {\n",
       "            gamma: 'trainable',\n",
       "            A: 'trainable',\n",
       "            bias: 'trainable',\n",
       "        },\n",
       "    },\n",
       "    GaborLayerLogSigma__0: {\n",
       "        freq: 'trainable',\n",
       "        logsigmax2: 'trainable',\n",
       "        logsigmay2: 'trainable',\n",
       "        theta: 'trainable',\n",
       "        sigma_theta: 'trainable',\n",
       "    },\n",
       "    Conv_1: {\n",
       "        kernel: 'trainable',\n",
       "        bias: 'trainable',\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_tree = freeze(flax.traverse_util.path_aware_map(lambda path, v: \"non_trainable\" if check_trainable(path)  else \"trainable\", state.params))\n",
    "trainable_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {\n",
    "    \"trainable\": optax.adam(learning_rate=config.LEARNING_RATE),\n",
    "    \"non_trainable\": optax.set_to_zero(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = optax.multi_transform(optimizers, trainable_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(PerceptNet(), random.PRNGKey(config.SEED), tx, input_shape=(1,384,512,3))\n",
    "state = state.replace(params=clip_layer(state.params, \"GDN\", a_min=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230, 230)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(state.params))\n",
    "trainable_param_count = sum([w.size if t==\"trainable\" else 0 for w, t in zip(jax.tree_util.tree_leaves(state.params), jax.tree_util.tree_leaves(trainable_tree))])\n",
    "param_count, trainable_param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.summary[\"total_parameters\"] = param_count\n",
    "wandb.run.summary[\"trainable_parameters\"] = trainable_param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'KeyError'>",
     "evalue": "'GDNSpatioFreqOrient_0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m state\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGDNGaussian_0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGaussianLayerGamma_0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mones_like(state\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGDNGaussian_0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGaussianLayerGamma_0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m## GDNSpatioFreqOrient\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m state\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGDNSpatioFreqOrient_0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGaussianLayerGamma_0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mones_like(\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGDNSpatioFreqOrient_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGaussianLayerGamma_0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     27\u001b[0m state\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGDNSpatioFreqOrient_0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrientGaussianGamma_0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mones_like(state\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGDNSpatioFreqOrient_0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOrientGaussianGamma_0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     28\u001b[0m state\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGDNSpatioFreqOrient_0\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mtile(jnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.002\u001b[39m, \u001b[38;5;241m0.0035\u001b[39m, \u001b[38;5;241m0.01\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m, reps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mN_ORIENTATIONS\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'GDNSpatioFreqOrient_0'"
     ]
    }
   ],
   "source": [
    "state = state.replace(params=unfreeze(state.params))\n",
    "\n",
    "# ## DN 0 (Gamma)\n",
    "state.params[\"GDN_0\"][\"Conv_0\"][\"bias\"] = jnp.ones_like(state.params[\"GDN_0\"][\"Conv_0\"][\"bias\"])*0.1\n",
    "state.params[\"GDN_0\"][\"Conv_0\"][\"kernel\"] = jnp.ones_like(state.params[\"GDN_0\"][\"Conv_0\"][\"kernel\"])*0.5\n",
    "\n",
    "## Opponent color channel transformation\n",
    "if config.INIT_JH:\n",
    "    state.params[\"Conv_0\"][\"kernel\"] = jnp.array([[39.0454,30.1207,14.27948],\n",
    "                                                  [115.8404,-63.3502,41.26816],\n",
    "                                                  [16.3118,30.2934,-61.51888]])[None,None,:,:]/163.5217\n",
    "\n",
    "## Center Surround\n",
    "state.params[\"CenterSurroundLogSigmaK_0\"][\"K\"] = jnp.array([1.1,1.01,1.01,\n",
    "                                                            1.01,1.1,1.01,\n",
    "                                                            1.01,1.01,1.1])\n",
    "state.params[\"CenterSurroundLogSigmaK_0\"][\"A\"] = jnp.array([1.,0.,0.,\n",
    "                                                            0.,1.,0.,\n",
    "                                                            0.,0.,1.])\n",
    "\n",
    "## GDNGaussian\n",
    "state.params[\"GDNGaussian_0\"][\"GaussianLayerGamma_0\"][\"gamma\"] = jnp.ones_like(state.params[\"GDNGaussian_0\"][\"GaussianLayerGamma_0\"][\"gamma\"])*(1./0.04)\n",
    "state.params[\"GDNGaussian_0\"][\"GaussianLayerGamma_0\"][\"bias\"] = jnp.ones_like(state.params[\"GDNGaussian_0\"][\"GaussianLayerGamma_0\"][\"bias\"])*0.1\n",
    "\n",
    "## GDNSpatioFreqOrient\n",
    "# state.params[\"GDNSpatioFreqOrient_0\"][\"GaussianLayerGamma_0\"][\"gamma\"] = jnp.ones_like(state.params[\"GDNSpatioFreqOrient_0\"][\"GaussianLayerGamma_0\"][\"gamma\"])*(1./0.1)\n",
    "# state.params[\"GDNSpatioFreqOrient_0\"][\"OrientGaussianGamma_0\"][\"gamma\"] = jnp.ones_like(state.params[\"GDNSpatioFreqOrient_0\"][\"OrientGaussianGamma_0\"][\"gamma\"])*(1/20)\n",
    "# state.params[\"GDNSpatioFreqOrient_0\"][\"bias\"] = jnp.tile(jnp.array([0.001, 0.002, 0.0035, 0.01])/100, reps=config.N_ORIENTATIONS*2)\n",
    "\n",
    "\n",
    "state = state.replace(params=freeze(state.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before actually training the model we're going to set up the checkpointer to be able to save our trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "save_args = orbax_utils.save_args_from_target(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dst_train_rdy.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def forward(state, inputs):\n",
    "    return state.apply_fn({\"params\": state.params, **state.state}, inputs, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def forward_intermediates(state, inputs):\n",
    "    return state.apply_fn({\"params\": state.params, **state.state}, inputs, train=False, capture_intermediates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.9 s, sys: 821 ms, total: 16.7 s\n",
      "Wall time: 14.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 480, 640, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "outputs = forward(state, batch[0])\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.09 s, sys: 271 ms, total: 4.36 s\n",
      "Wall time: 1.71 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 480, 640, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "outputs, _ = forward_intermediates(state, batch[0])\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 58s, sys: 6.18 s, total: 2min 4s\n",
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s1, grads = train_step(state, batch, return_grads=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.config.update(\"jax_debug_nans\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_extra(extra):\n",
    "    def filter_intermediates(path, x):\n",
    "        path = \"/\".join(path)\n",
    "        if \"Gabor\" in path:\n",
    "            return (x[0][0],)\n",
    "        else: \n",
    "            return x\n",
    "    extra = unfreeze(extra)\n",
    "    extra[\"intermediates\"] = flax.traverse_util.path_aware_map(filter_intermediates, extra[\"intermediates\"])\n",
    "    return freeze(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'OSError'>",
     "evalue": "[Errno 122] Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:34\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/site-packages/orbax/checkpoint/checkpointer.py:77\u001b[0m, in \u001b[0;36mCheckpointer.save\u001b[0;34m(self, directory, item, force, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDestination \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     75\u001b[0m tmpdir \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mcreate_tmp_directory(directory)\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmpdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m utils\u001b[38;5;241m.\u001b[39msync_global_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCheckpointer:write\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Ensure save operation atomicity and record time saved by checkpoint.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/site-packages/orbax/checkpoint/pytree_checkpoint_handler.py:420\u001b[0m, in \u001b[0;36mPyTreeCheckpointHandler.save\u001b[0;34m(self, directory, item, *args, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m commit_futures:\n\u001b[1;32m    418\u001b[0m       future\u001b[38;5;241m.\u001b[39mresult()  \u001b[38;5;66;03m# Block on result.\u001b[39;00m\n\u001b[0;32m--> 420\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43masync_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m utils\u001b[38;5;241m.\u001b[39msync_global_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPyTreeCheckpointHandler:save\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/site-packages/nest_asyncio.py:35\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     33\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/site-packages/nest_asyncio.py:90\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/asyncio/futures.py:178\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/asyncio/tasks.py:282\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    280\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_must_cancel:\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/site-packages/orbax/checkpoint/pytree_checkpoint_handler.py:413\u001b[0m, in \u001b[0;36mPyTreeCheckpointHandler.save.<locals>.async_save\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21masync_save\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 413\u001b[0m   commit_futures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_save(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# pytype: disable=bad-return-type\u001b[39;00m\n\u001b[1;32m    414\u001b[0m   \u001b[38;5;66;03m# Futures are already running, so sequential waiting is equivalent to\u001b[39;00m\n\u001b[1;32m    415\u001b[0m   \u001b[38;5;66;03m# concurrent waiting.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m commit_futures:  \u001b[38;5;66;03m# May be None.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/site-packages/orbax/checkpoint/pytree_checkpoint_handler.py:395\u001b[0m, in \u001b[0;36mPyTreeCheckpointHandler.async_save\u001b[0;34m(self, directory, item, save_args)\u001b[0m\n\u001b[1;32m    393\u001b[0m commit_futures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mcopy_futures)\n\u001b[1;32m    394\u001b[0m commit_futures, _ \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_flatten(commit_futures)\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_aggregate_file(directory, item, param_infos, save_args)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m commit_futures\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/site-packages/orbax/checkpoint/pytree_checkpoint_handler.py:327\u001b[0m, in \u001b[0;36mPyTreeCheckpointHandler._write_aggregate_file\u001b[0;34m(self, directory, item, param_infos, save_args)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write_aggregate_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: epath\u001b[38;5;241m.\u001b[39mPath, item: PyTree,\n\u001b[1;32m    325\u001b[0m                                 param_infos: PyTree, save_args: PyTree):\n\u001b[1;32m    326\u001b[0m   ser_item \u001b[38;5;241m=\u001b[39m _get_tree_for_aggregation(param_infos, save_args, item)\n\u001b[0;32m--> 327\u001b[0m   \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_handler\u001b[38;5;241m.\u001b[39mserialize(\n\u001b[1;32m    328\u001b[0m       directory \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_filename, ser_item)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/site-packages/orbax/checkpoint/aggregate_handlers.py:59\u001b[0m, in \u001b[0;36mMsgpackHandler.serialize\u001b[0;34m(self, path, item)\u001b[0m\n\u001b[1;32m     57\u001b[0m serializable_dict \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mserialize_tree(item, keep_empty_nodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m msgpack \u001b[38;5;241m=\u001b[39m msgpack_utils\u001b[38;5;241m.\u001b[39mmsgpack_serialize(serializable_dict)\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m utils\u001b[38;5;241m.\u001b[39masync_write_bytes(path, msgpack)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/site-packages/orbax/checkpoint/utils.py:73\u001b[0m, in \u001b[0;36m_wrap.<locals>.run\u001b[0;34m(loop, executor, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m   loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[1;32m     72\u001b[0m partial_func \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mrun_in_executor(executor, partial_func)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/asyncio/futures.py:260\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/asyncio/tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/asyncio/futures.py:178\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/concurrent/futures/thread.py:57\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/site-packages/etils/epath/abstract_path.py:182\u001b[0m, in \u001b[0;36mPath.write_bytes\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Writes content as bytes.\"\"\"\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 182\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mwrite(data)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(config.EPOCHS):\n",
    "    ## Training\n",
    "    for batch in dst_train_rdy.as_numpy_iterator():\n",
    "        state, grads = train_step(state, batch, return_grads=True)\n",
    "        state = state.replace(params=clip_layer(state.params, \"GDN\", a_min=0))\n",
    "        state = state.replace(params=clip_param(state.params, \"A\", a_min=0))\n",
    "        state = state.replace(params=clip_param(state.params, \"K\", a_min=1+1e-5))\n",
    "        wandb.log({f\"{k}_grad\": wandb.Histogram(v) for k, v in flatten_params(grads).items()}, commit=False)\n",
    "        # state = compute_metrics(state=state, batch=batch)\n",
    "        # break\n",
    "\n",
    "    ## Log the metrics\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"train_{name}\"].append(value)\n",
    "    \n",
    "    ## Empty the metrics\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "    ## Evaluation\n",
    "    for batch in dst_val_rdy.as_numpy_iterator():\n",
    "        state = compute_metrics(state=state, batch=batch)\n",
    "        # break\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"val_{name}\"].append(value)\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "    \n",
    "    ## Obtain activations of last validation batch\n",
    "    _, extra = forward_intermediates(state, batch[0])\n",
    "    del _\n",
    "    extra = filter_extra(extra) ## Needed because the Gabor layer has multiple outputs\n",
    "    \n",
    "    ## Checkpointing\n",
    "    if metrics_history[\"val_loss\"][-1] <= min(metrics_history[\"val_loss\"]):\n",
    "        orbax_checkpointer.save(os.path.join(wandb.run.dir, \"model-best\"), state, save_args=save_args, force=True) # force=True means allow overwritting.\n",
    "\n",
    "    ## Logging image \n",
    "    pred = forward(state, batch[0])\n",
    "    fig, axes = plt.subplots(10,3)\n",
    "    for i, axs in enumerate(axes):\n",
    "        axs[0].imshow(batch[0][i])\n",
    "        axs[1].imshow(batch[1][i])\n",
    "        axs[2].imshow(pred[i])\n",
    "    for ax in axes.ravel(): ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    wandb.log({\"SaliencyMaps\": plt}, commit=False)\n",
    "    plt.close()\n",
    "    \n",
    "    wandb.log({f\"{k}\": wandb.Histogram(v) for k, v in flatten_params(state.params).items()}, commit=False)\n",
    "    wandb.log({f\"{k}\": wandb.Histogram(v) for k, v in flatten_params(extra[\"intermediates\"]).items()}, commit=False)\n",
    "    wandb.log({\"epoch\": epoch+1, **{name:values[-1] for name, values in metrics_history.items()}})\n",
    "    print(f'Epoch {epoch} -> [Train] Loss: {metrics_history[\"train_loss\"][-1]} [Val] Loss: {metrics_history[\"val_loss\"][-1]}')\n",
    "    # print(f'Epoch {epoch} -> [Train] Loss: {metrics_history[\"train_loss\"][-1]}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 480, 640, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = forward(state, batch[0])\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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