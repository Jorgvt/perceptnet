{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IQA tracking params and variables\n",
    "\n",
    "> When using parametric layers we have to be able to keep track of the parameters and the variables of the model (which are not going to be trained). We're going to play with this concept using our implementation of the functional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os; os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".99\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 16:01:53.083867: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-14 16:01:53.182490: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-14 16:02:45.115159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-11-14 16:03:41.970151: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-11-14 16:03:41.970207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: deep\n",
      "2023-11-14 16:03:41.970218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: deep\n",
      "2023-11-14 16:03:41.970425: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 535.104.5\n",
      "2023-11-14 16:03:41.970467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 535.104.5\n",
      "2023-11-14 16:03:41.970477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 535.104.5\n",
      "2023-11-14 16:04:13.939359: E external/xla/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from typing import Any, Callable, Sequence, Union\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "import jax\n",
    "from jax import lax, random, numpy as jnp\n",
    "from flax.core import freeze, unfreeze, FrozenDict\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "from flax.training import orbax_utils\n",
    "\n",
    "import optax\n",
    "import orbax.checkpoint\n",
    "\n",
    "from clu import metrics\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "from einops import reduce, rearrange\n",
    "import wandb\n",
    "from iqadatasets.datasets import *\n",
    "from fxlayers.layers import *\n",
    "from fxlayers.layers import GaussianLayerGamma, GaborLayerLogSigma_, GaborLayerLogSigmaCoupled_\n",
    "from fxlayers.initializers import *\n",
    "from JaxPlayground.utils.constraints import *\n",
    "from JaxPlayground.utils.wandb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.config.update(\"jax_debug_nans\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "> We're going to employ `iqadatasets` to ease the loading of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst_train = TID2008(\"/lustre/ific.uv.es/ml/uv075/Databases/IQA//TID/TID2008/\", exclude_imgs=[25])\n",
    "# dst_val = TID2013(\"/lustre/ific.uv.es/ml/uv075/Databases/IQA//TID/TID2013/\", exclude_imgs=[25])\n",
    "dst_train = TID2008(\"/media/disk/databases/BBDD_video_image/Image_Quality//TID/TID2008/\", exclude_imgs=[25])\n",
    "dst_val = TID2013(\"/media/disk/databases/BBDD_video_image/Image_Quality//TID/TID2013/\", exclude_imgs=[25])\n",
    "# dst_train = TID2008(\"/media/databases/IQA/TID/TID2008/\", exclude_imgs=[25])\n",
    "# dst_val = TID2013(\"/media/databases/IQA/TID/TID2013/\", exclude_imgs=[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 16:04:15.845410: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype double and shape [1632]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([384, 512, 3]), TensorShape([384, 512, 3]), TensorShape([]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, img_dist, mos = next(iter(dst_train.dataset))\n",
    "img.shape, img_dist.shape, mos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 16:04:16.172742: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype double and shape [2880]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([384, 512, 3]), TensorShape([384, 512, 3]), TensorShape([]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, img_dist, mos = next(iter(dst_val.dataset))\n",
    "img.shape, img_dist.shape, mos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BATCH_SIZE: 64\n",
       "EPOCHS: 500\n",
       "GDN_CLIPPING: true\n",
       "LEARNING_RATE: 0.003\n",
       "NORMALIZE_ENERGY: true\n",
       "NORMALIZE_PROB: false\n",
       "N_ORIENTATIONS: 8\n",
       "N_SCALES: 4\n",
       "SEED: 42\n",
       "USE_BIAS: false\n",
       "ZERO_MEAN: true"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"EPOCHS\": 500,\n",
    "    \"LEARNING_RATE\": 3e-3,\n",
    "    \"SEED\": 42,\n",
    "    \"GDN_CLIPPING\": True,\n",
    "    \"NORMALIZE_PROB\": False,\n",
    "    \"NORMALIZE_ENERGY\": True,\n",
    "    \"ZERO_MEAN\": True,\n",
    "    \"USE_BIAS\": False,\n",
    "    \"N_SCALES\": 4,\n",
    "    \"N_ORIENTATIONS\": 8,\n",
    "}\n",
    "config = ConfigDict(config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"PerceptNet_JaX\",\n",
    "           name=\"V2_Init_FT_Mix\",\n",
    "           job_type=\"training\",\n",
    "           config=config,\n",
    "           mode=\"online\",\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_train_rdy = dst_train.dataset.shuffle(buffer_size=100,\n",
    "                                      reshuffle_each_iteration=True,\n",
    "                                      seed=config.SEED)\\\n",
    "                                 .batch(config.BATCH_SIZE, drop_remainder=True)\n",
    "dst_val_rdy = dst_val.dataset.batch(config.BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model we're going to use\n",
    "\n",
    "> It's going to be a very simple model just for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GDNGaussianStarRunning(nn.Module):\n",
    "    \"\"\"GDN variation where x^* is obtained as a running mean of the previously obtained values.\"\"\"\n",
    "\n",
    "    kernel_size: int\n",
    "    inputs_star: float = 1.\n",
    "    outputs_star: Union[None, float] = None\n",
    "    fs: int = 1\n",
    "    apply_independently: bool = False\n",
    "    alpha: float = 2.\n",
    "    epsilon: float = 1/2\n",
    "    bias_init: Callable = nn.initializers.ones_init()\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 train=False,\n",
    "                 **kwargs,\n",
    "                 ):\n",
    "        # inputs_sign = jnp.sign(inputs)\n",
    "        # inputs = jnp.abs(inputs)\n",
    "        is_initialized = self.has_variable(\"batch_stats\", \"inputs_star\")\n",
    "        # inputs_star = self.variable(\"batch_stats\", \"inputs_star\", lambda x: x, jnp.quantile(inputs, q=0.95))\n",
    "        inputs_star = self.variable(\"batch_stats\", \"inputs_star\", lambda x: jnp.ones(x)*self.inputs_star, (1,))\n",
    "        if is_initialized and train:\n",
    "            inputs_star.value = (inputs_star.value + jnp.quantile(jnp.abs(inputs), q=0.95))/2\n",
    "        H = GaussianLayerGamma(features=inputs.shape[-1], kernel_size=self.kernel_size, use_bias=True, fs=self.fs, xmean=self.kernel_size/self.fs/2, ymean=self.kernel_size/self.fs/2, bias_init=self.bias_init, normalize_prob=config.NORMALIZE_PROB, normalize_energy=config.NORMALIZE_ENERGY)\n",
    "        inputs_star_ = jnp.ones_like(inputs)*inputs_star.value\n",
    "        denom = jnp.clip(H(inputs**self.alpha, train=train), a_min=1e-5)**self.epsilon\n",
    "        coef = (jnp.clip(H(inputs_star_**self.alpha, train=train), a_min=1e-5)**self.epsilon)#/inputs_star_\n",
    "        if self.outputs_star is not None: coef = coef/inputs_star.value*self.outputs_star\n",
    "        \n",
    "        return coef*inputs/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDNSpatioFreqOrient(nn.Module):\n",
    "    \"\"\"Generalized Divisive Normalization.\"\"\"\n",
    "    kernel_size: Union[int, Sequence[int]]\n",
    "    strides: int = 1\n",
    "    padding: str = \"SAME\"\n",
    "    inputs_star: float = 1.\n",
    "    outputs_star: Union[None, float] = None\n",
    "    fs: int = 1\n",
    "    apply_independently: bool = False\n",
    "    bias_init: Callable = nn.initializers.ones_init()\n",
    "    alpha: float = 2.\n",
    "    epsilon: float = 1/2 # Exponential of the denominator\n",
    "    eps: float = 1e-6 # Numerical stability in the denominator\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 fmean,\n",
    "                 theta_mean,\n",
    "                 train=False,\n",
    "                 ):\n",
    "        b, h, w, c = inputs.shape\n",
    "        bias = self.param(\"bias\",\n",
    "                          #equal_to(inputs_star/10),\n",
    "                          self.bias_init,\n",
    "                          (c,))\n",
    "        is_initialized = self.has_variable(\"batch_stats\", \"inputs_star\")\n",
    "        inputs_star = self.variable(\"batch_stats\", \"inputs_star\", lambda x: jnp.ones(x)*self.inputs_star, (len(self.inputs_star),))\n",
    "        inputs_star_ = jnp.ones_like(inputs)*inputs_star.value\n",
    "        GL = GaussianLayerGamma(features=c, kernel_size=self.kernel_size, strides=self.strides, padding=self.padding, fs=self.fs, xmean=self.kernel_size/self.fs/2, ymean=self.kernel_size/self.fs/2, normalize_prob=config.NORMALIZE_PROB, normalize_energy=config.NORMALIZE_ENERGY, use_bias=False, feature_group_count=c)\n",
    "        FG = FreqGaussian()\n",
    "        OG = OrientGaussian()\n",
    "        outputs = GL(inputs**self.alpha, train=train)#/(self.kernel_size**2)\n",
    "        outputs = FG(outputs, fmean=fmean)\n",
    "        ## Reshape so that the orientations are the innermost dimmension\n",
    "        outputs = rearrange(outputs, \"b h w (phase theta f) -> b h w (phase f theta)\", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)\n",
    "        outputs = OG(outputs, theta_mean=theta_mean)\n",
    "        ## Recover original disposition\n",
    "        denom = rearrange(outputs, \"b h w (phase f theta) -> b h w (phase theta f)\", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)\n",
    "\n",
    "        ## Coef\n",
    "        coef = GL(inputs_star_**self.alpha, train=train)#/(self.kernel_size**2)\n",
    "        coef = FG(coef, fmean=fmean)\n",
    "        coef = rearrange(coef, \"b h w (phase theta f) -> b h w (phase f theta)\", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)\n",
    "        coef = OG(coef, theta_mean=theta_mean) + bias\n",
    "        coef = rearrange(coef, \"b h w (phase f theta) -> b h w (phase theta f)\", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)\n",
    "        coef = jnp.clip(coef+bias, a_min=1e-5)**self.epsilon\n",
    "        # coef = inputs_star.value * coef\n",
    "        if self.outputs_star is not None: coef = coef/inputs_star.value*self.outputs_star\n",
    "\n",
    "        if is_initialized and train:\n",
    "            inputs_star.value = (inputs_star.value + jnp.quantile(jnp.abs(inputs), q=0.95, axis=(0,1,2)))/2\n",
    "        return coef * inputs / (jnp.clip(denom+bias, a_min=1e-5)**self.epsilon + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "with open(\"gabor_x_star.pkl\", \"rb\") as f:\n",
    "    gabor_x_star = load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptNet(nn.Module):\n",
    "    \"\"\"IQA model inspired by the visual system.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs, # Assuming fs = 128 (cpd)\n",
    "                 **kwargs,\n",
    "                 ):\n",
    "        ## (Independent) Color equilibration (Gamma correction)\n",
    "        ## bias = 0.1 / kernel = 0.5\n",
    "        outputs = GDNStarSign(kernel_size=(1,1), apply_independently=True, inputs_star=1.)(inputs)\n",
    "        \n",
    "        ## ATD Transformation\n",
    "        outputs = JamesonHurvich()(outputs)\n",
    "        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n",
    "        \n",
    "        ## GDN Star A - T - D [Separated]\n",
    "        ### A\n",
    "        outputs0 = GDNStarSign(kernel_size=(1,1), apply_independently=True, inputs_star=170.)(outputs[:,:,:,0:1])\n",
    "        ### T\n",
    "        outputs1 = GDNStarDisplacement(kernel_size=(1,1), apply_independently=True, inputs_star=55.)(outputs[:,:,:,1:2])\n",
    "        outputs1 = outputs1*(2*55/170)\n",
    "        ### D\n",
    "        outputs2 = GDNStarDisplacement(kernel_size=(1,1), apply_independently=True, inputs_star=55.)(outputs[:,:,:,2:3])\n",
    "        outputs2 = outputs2*(2*55/170)\n",
    "        ### Put them back together\n",
    "        outputs = jnp.concatenate([outputs0, outputs1, outputs2], axis=-1)\n",
    "\n",
    "        ## Apply CSF on Fourier\n",
    "        outputs = CSFFourier(fs=64, norm_energy=True)(outputs)\n",
    "        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n",
    "\n",
    "        ## GDN per channel with mean substraction in T and D (Spatial Gaussian Kernel)\n",
    "        ## TO-DO: - Spatial Gaussian Kernel (0.02 deg) -> fs = 64/2 & 0.02*64/2 = sigma (px) = 0.69\n",
    "        ### A\n",
    "        ### (384/4, 512/4, 1)\n",
    "        ### fs = 32 / kernel_size = (11,11) -> 0.32 > 0.02 --> OK!\n",
    "        outputs0 = GDNGaussianStarRunning(kernel_size=11, apply_independently=True, bias_init=equal_to([0.1]), inputs_star=0.3, outputs_star=None, fs=32)(outputs[:,:,:,0:1], **kwargs)\n",
    "        ### T\n",
    "        outputs1 = GDNGaussianStarRunning(kernel_size=11, apply_independently=True, bias_init=equal_to([0.01**2]), inputs_star=0.06, outputs_star=None, fs=32)(outputs[:,:,:,1:2], **kwargs)\n",
    "        ### D\n",
    "        outputs2 = GDNGaussianStarRunning(kernel_size=11, apply_independently=True, bias_init=equal_to([0.01**2]), inputs_star=0.08, outputs_star=None, fs=32)(outputs[:,:,:,2:3], **kwargs)\n",
    "        ### Put them back together\n",
    "        outputs = jnp.concatenate([outputs0, outputs1, outputs2], axis=-1)\n",
    "\n",
    "        ## GaborLayer per channel with GDN mixing only same-origin-channel information\n",
    "        ### A\n",
    "        outputs0, fmean, theta_mean = GaborLayerLogSigmaCoupled_(n_scales=config.N_SCALES, n_orientations=config.N_ORIENTATIONS, kernel_size=32, fs=32, strides=1, padding=\"SAME\", normalize_prob=config.NORMALIZE_PROB, normalize_energy=config.NORMALIZE_ENERGY, zero_mean=config.ZERO_MEAN, use_bias=config.USE_BIAS)(outputs[:,:,:,0:1], return_freq=True, return_theta=True, **kwargs)\n",
    "        ### [Gaussian] sigma = 0.2 (deg) fs = 32 / kernel_size = (21,21) -> 21/32 = 0.66 --> OK!\n",
    "        outputs0 = GDNSpatioFreqOrient(kernel_size=21, strides=1, padding=\"SAME\", fs=32, apply_independently=False, inputs_star=gabor_x_star[\"A\"])(outputs0, fmean=fmean, theta_mean=theta_mean, **kwargs)\n",
    "        ### T\n",
    "        outputs1, fmean, theta_mean = GaborLayerLogSigmaCoupled_(n_scales=config.N_SCALES, n_orientations=config.N_ORIENTATIONS, kernel_size=32, fs=32, strides=1, padding=\"SAME\", normalize_prob=config.NORMALIZE_PROB, normalize_energy=config.NORMALIZE_ENERGY, zero_mean=config.ZERO_MEAN, use_bias=config.USE_BIAS)(outputs[:,:,:,1:2], return_freq=True, return_theta=True, **kwargs)\n",
    "        ### [Gaussian] sigma = 0.2 (deg) fs = 32 / kernel_size = (21,21) -> 21/32 = 0.66 --> OK!\n",
    "        outputs1 = GDNSpatioFreqOrient(kernel_size=21, strides=1, padding=\"SAME\", fs=32, apply_independently=False, inputs_star=gabor_x_star[\"T\"])(outputs1, fmean=fmean, theta_mean=theta_mean, **kwargs)\n",
    "        ### D\n",
    "        outputs2, fmean, theta_mean = GaborLayerLogSigmaCoupled_(n_scales=config.N_SCALES, n_orientations=config.N_ORIENTATIONS, kernel_size=32, fs=32, strides=1, padding=\"SAME\", normalize_prob=config.NORMALIZE_PROB, normalize_energy=config.NORMALIZE_ENERGY, zero_mean=config.ZERO_MEAN, use_bias=config.USE_BIAS)(outputs[:,:,:,2:3], return_freq=True, return_theta=True, **kwargs)\n",
    "        ### [Gaussian] sigma = 0.2 (deg) fs = 32 / kernel_size = (21,21) -> 21/32 = 0.66 --> OK!\n",
    "        outputs2 = GDNSpatioFreqOrient(kernel_size=21, strides=1, padding=\"SAME\", fs=32, apply_independently=False, inputs_star=gabor_x_star[\"D\"])(outputs2, fmean=fmean, theta_mean=theta_mean, **kwargs)\n",
    "\n",
    "        ## Put them back together\n",
    "        outputs = jnp.concatenate([outputs0, outputs1, outputs2], axis=-1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_gabors(gabors):\n",
    "    gabors_r = []\n",
    "    for i in range(64):\n",
    "        gabor_r = gabors[:,:,:,i::64]\n",
    "        gabors_r.append(gabor_r)\n",
    "    gabors_r = jnp.concatenate(gabors_r, axis=-1)\n",
    "    return gabors_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTunner(nn.Module):\n",
    "    def setup(self):\n",
    "        self.perceptnet = PerceptNet()\n",
    "        self.ft = nn.Conv(features=64, kernel_size=(1,1), feature_group_count=64, use_bias=False)\n",
    "        \n",
    "    \n",
    "    def __call__(self, inputs, **kwargs):\n",
    "        outputs = self.perceptnet(inputs, **kwargs)\n",
    "        outputs = rearrange_gabors(outputs)\n",
    "        outputs = self.ft(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the metrics with `clu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n",
    "    loss: metrics.Average.from_output(\"loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `TrainState` doesn't include metrics, but it's very easy to subclass it so that it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    state: FrozenDict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function that initializes the `TrainState` from a module, a rng key and some optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(module, key, tx, input_shape):\n",
    "    \"\"\"Creates the initial `TrainState`.\"\"\"\n",
    "    variables = module.init(key, jnp.ones(input_shape))\n",
    "    state, params = variables.pop('params')\n",
    "    return TrainState.create(\n",
    "        apply_fn=module.apply,\n",
    "        params=params,\n",
    "        state=state,\n",
    "        tx=tx,\n",
    "        metrics=Metrics.empty()\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the training step\n",
    "\n",
    "> We want to write a function that takes the `TrainState` and a batch of data can performs an optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(vec1, vec2):\n",
    "    vec1 = vec1.squeeze()\n",
    "    vec2 = vec2.squeeze()\n",
    "    vec1_mean = vec1.mean()\n",
    "    vec2_mean = vec2.mean()\n",
    "    num = vec1-vec1_mean\n",
    "    num *= vec2-vec2_mean\n",
    "    num = num.sum()\n",
    "    denom = jnp.sqrt(jnp.sum((vec1-vec1_mean)**2))\n",
    "    denom *= jnp.sqrt(jnp.sum((vec2-vec2_mean)**2))\n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    img, img_dist, mos = batch\n",
    "    def loss_fn(params):\n",
    "        ## Forward pass through the model\n",
    "        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=True)\n",
    "        img_dist_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img_dist, mutable=list(state.state.keys()), train=True)\n",
    "\n",
    "        ## Calculate the distance\n",
    "        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n",
    "        \n",
    "        ## Calculate pearson correlation\n",
    "        return pearson_correlation(dist, mos), updated_state\n",
    "    \n",
    "    (loss, updated_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metrics_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    state = state.replace(state=updated_state)\n",
    "    return state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their example, they don't calculate the metrics at the same time. I think it is kind of a waste because it means having to perform a new forward pass, but we'll follow as of now. Let's define a function to perform metric calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "    \"\"\"Obtaining the metrics for a given batch.\"\"\"\n",
    "    img, img_dist, mos = batch\n",
    "    def loss_fn(params):\n",
    "        ## Forward pass through the model\n",
    "        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=False)\n",
    "        img_dist_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img_dist, mutable=list(state.state.keys()), train=False)\n",
    "\n",
    "        ## Calculate the distance\n",
    "        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n",
    "        \n",
    "        ## Calculate pearson correlation\n",
    "        return pearson_correlation(dist, mos)\n",
    "    \n",
    "    metrics_updates = state.metrics.single_from_model_output(loss=loss_fn(state.params))\n",
    "    metrics = state.metrics.merge(metrics_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(FineTunner(), random.PRNGKey(config.SEED), optax.adam(config.LEARNING_RATE), input_shape=(1,384,512,3))\n",
    "state = state.replace(params=clip_layer(state.params, \"GDN\", a_min=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozen_dict_keys(['perceptnet', 'ft'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.params.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trainable(path):\n",
    "    return \"perceptnet\" in path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    perceptnet: {\n",
       "        GDNStarSign_0: {\n",
       "            Conv_0: {\n",
       "                kernel: 'non_trainable',\n",
       "                bias: 'non_trainable',\n",
       "            },\n",
       "        },\n",
       "        GDNStarSign_1: {\n",
       "            Conv_0: {\n",
       "                kernel: 'non_trainable',\n",
       "                bias: 'non_trainable',\n",
       "            },\n",
       "        },\n",
       "        GDNStarDisplacement_0: {\n",
       "            Conv_0: {\n",
       "                kernel: 'non_trainable',\n",
       "                bias: 'non_trainable',\n",
       "            },\n",
       "        },\n",
       "        GDNStarDisplacement_1: {\n",
       "            Conv_0: {\n",
       "                kernel: 'non_trainable',\n",
       "                bias: 'non_trainable',\n",
       "            },\n",
       "        },\n",
       "        CSFFourier_0: {\n",
       "            alpha_achrom: 'non_trainable',\n",
       "            alpha_chrom_rg: 'non_trainable',\n",
       "            alpha_chrom_yb: 'non_trainable',\n",
       "            beta_achrom: 'non_trainable',\n",
       "            beta_chrom: 'non_trainable',\n",
       "            fm: 'non_trainable',\n",
       "            s: 'non_trainable',\n",
       "        },\n",
       "        GDNGaussianStarRunning_0: {\n",
       "            GaussianLayerGamma_0: {\n",
       "                gamma: 'non_trainable',\n",
       "                A: 'non_trainable',\n",
       "                bias: 'non_trainable',\n",
       "            },\n",
       "        },\n",
       "        GDNGaussianStarRunning_1: {\n",
       "            GaussianLayerGamma_0: {\n",
       "                gamma: 'non_trainable',\n",
       "                A: 'non_trainable',\n",
       "                bias: 'non_trainable',\n",
       "            },\n",
       "        },\n",
       "        GDNGaussianStarRunning_2: {\n",
       "            GaussianLayerGamma_0: {\n",
       "                gamma: 'non_trainable',\n",
       "                A: 'non_trainable',\n",
       "                bias: 'non_trainable',\n",
       "            },\n",
       "        },\n",
       "        GaborLayerLogSigmaCoupled__0: {\n",
       "            freq: 'non_trainable',\n",
       "            logsigma2: 'non_trainable',\n",
       "            theta: 'non_trainable',\n",
       "        },\n",
       "        GDNSpatioFreqOrient_0: {\n",
       "            bias: 'non_trainable',\n",
       "            GaussianLayerGamma_0: {\n",
       "                gamma: 'non_trainable',\n",
       "                A: 'non_trainable',\n",
       "            },\n",
       "            FreqGaussian_0: {\n",
       "                sigma: 'non_trainable',\n",
       "            },\n",
       "            OrientGaussian_0: {\n",
       "                sigma: 'non_trainable',\n",
       "            },\n",
       "        },\n",
       "        GaborLayerLogSigmaCoupled__1: {\n",
       "            freq: 'non_trainable',\n",
       "            logsigma2: 'non_trainable',\n",
       "            theta: 'non_trainable',\n",
       "        },\n",
       "        GDNSpatioFreqOrient_1: {\n",
       "            bias: 'non_trainable',\n",
       "            GaussianLayerGamma_0: {\n",
       "                gamma: 'non_trainable',\n",
       "                A: 'non_trainable',\n",
       "            },\n",
       "            FreqGaussian_0: {\n",
       "                sigma: 'non_trainable',\n",
       "            },\n",
       "            OrientGaussian_0: {\n",
       "                sigma: 'non_trainable',\n",
       "            },\n",
       "        },\n",
       "        GaborLayerLogSigmaCoupled__2: {\n",
       "            freq: 'non_trainable',\n",
       "            logsigma2: 'non_trainable',\n",
       "            theta: 'non_trainable',\n",
       "        },\n",
       "        GDNSpatioFreqOrient_2: {\n",
       "            bias: 'non_trainable',\n",
       "            GaussianLayerGamma_0: {\n",
       "                gamma: 'non_trainable',\n",
       "                A: 'non_trainable',\n",
       "            },\n",
       "            FreqGaussian_0: {\n",
       "                sigma: 'non_trainable',\n",
       "            },\n",
       "            OrientGaussian_0: {\n",
       "                sigma: 'non_trainable',\n",
       "            },\n",
       "        },\n",
       "    },\n",
       "    ft: {\n",
       "        kernel: 'trainable',\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_tree = freeze(flax.traverse_util.path_aware_map(lambda path, v: \"non_trainable\" if check_trainable(path)  else \"trainable\", state.params))\n",
    "trainable_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {\n",
    "    \"trainable\": optax.adam(learning_rate=config.LEARNING_RATE),\n",
    "    \"non_trainable\": optax.set_to_zero(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = optax.multi_transform(optimizers, trainable_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(FineTunner(), random.PRNGKey(config.SEED), tx, input_shape=(1,384,512,3))\n",
    "state = state.replace(params=clip_layer(state.params, \"GDN\", a_min=0))\n",
    "state = state.replace(params=clip_layer(state.params, \"alpha_achrom\", a_min=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(880, 192)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(state.params))\n",
    "trainable_param_count = sum([w.size if t==\"trainable\" else 0 for w, t in zip(jax.tree_util.tree_leaves(state.params), jax.tree_util.tree_leaves(trainable_tree))])\n",
    "param_count, trainable_param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.summary[\"total_parameters\"] = param_count\n",
    "wandb.run.summary[\"trainable_parameters\"] = trainable_param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = state.replace(params=unfreeze(state.params))\n",
    "\n",
    "## DN 0\n",
    "state.params[\"perceptnet\"][\"GDNStarSign_0\"][\"Conv_0\"][\"bias\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNStarSign_0\"][\"Conv_0\"][\"bias\"])*0.1\n",
    "state.params[\"perceptnet\"][\"GDNStarSign_0\"][\"Conv_0\"][\"kernel\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNStarSign_0\"][\"Conv_0\"][\"kernel\"])*0.5\n",
    "\n",
    "## DN J&H\n",
    "state.params[\"perceptnet\"][\"GDNStarSign_1\"][\"Conv_0\"][\"bias\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNStarSign_1\"][\"Conv_0\"][\"bias\"])*30.**2\n",
    "state.params[\"perceptnet\"][\"GDNStarSign_1\"][\"Conv_0\"][\"kernel\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNStarSign_1\"][\"Conv_0\"][\"kernel\"])*0.5\n",
    "\n",
    "state.params[\"perceptnet\"][\"GDNStarDisplacement_0\"][\"Conv_0\"][\"bias\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNStarDisplacement_0\"][\"Conv_0\"][\"bias\"])*10.**2\n",
    "state.params[\"perceptnet\"][\"GDNStarDisplacement_0\"][\"Conv_0\"][\"kernel\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNStarDisplacement_0\"][\"Conv_0\"][\"kernel\"])*0.5\n",
    "\n",
    "state.params[\"perceptnet\"][\"GDNStarDisplacement_1\"][\"Conv_0\"][\"bias\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNStarDisplacement_1\"][\"Conv_0\"][\"bias\"])*10.**2\n",
    "state.params[\"perceptnet\"][\"GDNStarDisplacement_1\"][\"Conv_0\"][\"kernel\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNStarDisplacement_1\"][\"Conv_0\"][\"kernel\"])*0.5\n",
    "\n",
    "state.params[\"perceptnet\"][\"GDNGaussianStarRunning_0\"][\"GaussianLayerGamma_0\"][\"gamma\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNGaussianStarRunning_0\"][\"GaussianLayerGamma_0\"][\"gamma\"])*(1./0.04)\n",
    "state.params[\"perceptnet\"][\"GDNGaussianStarRunning_1\"][\"GaussianLayerGamma_0\"][\"gamma\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNGaussianStarRunning_1\"][\"GaussianLayerGamma_0\"][\"gamma\"])*(1./0.04)\n",
    "state.params[\"perceptnet\"][\"GDNGaussianStarRunning_2\"][\"GaussianLayerGamma_0\"][\"gamma\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNGaussianStarRunning_2\"][\"GaussianLayerGamma_0\"][\"gamma\"])*(1./0.04)\n",
    "\n",
    "state.params[\"perceptnet\"][\"GDNSpatioFreqOrient_0\"][\"GaussianLayerGamma_0\"][\"gamma\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNSpatioFreqOrient_0\"][\"GaussianLayerGamma_0\"][\"gamma\"])*(1./0.1)\n",
    "state.params[\"perceptnet\"][\"GDNSpatioFreqOrient_1\"][\"GaussianLayerGamma_0\"][\"gamma\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNSpatioFreqOrient_1\"][\"GaussianLayerGamma_0\"][\"gamma\"])*(1./0.1)\n",
    "state.params[\"perceptnet\"][\"GDNSpatioFreqOrient_2\"][\"GaussianLayerGamma_0\"][\"gamma\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNSpatioFreqOrient_2\"][\"GaussianLayerGamma_0\"][\"gamma\"])*(1./0.1)\n",
    "\n",
    "state.params[\"perceptnet\"][\"GDNSpatioFreqOrient_0\"][\"OrientGaussian_0\"][\"sigma\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNSpatioFreqOrient_0\"][\"OrientGaussian_0\"][\"sigma\"])*20\n",
    "state.params[\"perceptnet\"][\"GDNSpatioFreqOrient_1\"][\"OrientGaussian_0\"][\"sigma\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNSpatioFreqOrient_1\"][\"OrientGaussian_0\"][\"sigma\"])*20\n",
    "state.params[\"perceptnet\"][\"GDNSpatioFreqOrient_2\"][\"OrientGaussian_0\"][\"sigma\"] = jnp.ones_like(state.params[\"perceptnet\"][\"GDNSpatioFreqOrient_2\"][\"OrientGaussian_0\"][\"sigma\"])*20\n",
    "\n",
    "state.params[\"perceptnet\"][\"GDNSpatioFreqOrient_0\"][\"bias\"] = jnp.tile(jnp.array([0.001, 0.002, 0.0035, 0.01])/100, reps=config.N_ORIENTATIONS*2)\n",
    "state.params[\"perceptnet\"][\"GDNSpatioFreqOrient_1\"][\"bias\"] = jnp.tile(jnp.array([0.001, 0.002, 0.0035, 0.01])/100, reps=config.N_ORIENTATIONS*2)\n",
    "state.params[\"perceptnet\"][\"GDNSpatioFreqOrient_2\"][\"bias\"] = jnp.tile(jnp.array([0.001, 0.002, 0.0035, 0.01])/100, reps=config.N_ORIENTATIONS*2)\n",
    "\n",
    "\n",
    "state = state.replace(params=freeze(state.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before actually training the model we're going to set up the checkpointer to be able to save our trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "save_args = orbax_utils.save_args_from_target(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dst_train_rdy.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def forward(state, inputs):\n",
    "    return state.apply_fn({\"params\": state.params, **state.state}, inputs, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "outputs = forward(state, batch[0])\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s1 = train_step(state, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.config.update(\"jax_debug_nans\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(config.EPOCHS):\n",
    "    ## Training\n",
    "    for batch in dst_train_rdy.as_numpy_iterator():\n",
    "        state = train_step(state, batch)\n",
    "        state = state.replace(params=clip_layer(state.params, \"GDN\", a_min=0))\n",
    "        state = state.replace(params=clip_layer(state.params, \"alpha_achrom\", a_min=1))\n",
    "        # state = compute_metrics(state=state, batch=batch)\n",
    "        # break\n",
    "\n",
    "    ## Log the metrics\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"train_{name}\"].append(value)\n",
    "    \n",
    "    ## Empty the metrics\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "    ## Evaluation\n",
    "    for batch in dst_val_rdy.as_numpy_iterator():\n",
    "        state = compute_metrics(state=state, batch=batch)\n",
    "        # break\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"val_{name}\"].append(value)\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "    \n",
    "    ## Checkpointing\n",
    "    if metrics_history[\"val_loss\"][-1] <= min(metrics_history[\"val_loss\"]):\n",
    "        orbax_checkpointer.save(os.path.join(wandb.run.dir, \"model-best\"), state, save_args=save_args, force=True) # force=True means allow overwritting.\n",
    "\n",
    "    wandb.log({f\"{k}\": wandb.Histogram(v) for k, v in flatten_params(state.params).items()}, commit=False)\n",
    "    wandb.log({\"epoch\": epoch+1, **{name:values[-1] for name, values in metrics_history.items()}})\n",
    "    print(f'Epoch {epoch} -> [Train] Loss: {metrics_history[\"train_loss\"][-1]} [Val] Loss: {metrics_history[\"val_loss\"][-1]}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the final model as well in case we want to keep training from it or whatever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbax_checkpointer.save(os.path.join(wandb.run.dir, \"model-final\"), state, save_args=save_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8,8, figsize=(15,15))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(state.state[\"precalc_filter\"][\"GaborLayerLogSigmaCoupled__0\"][\"kernel\"][:,:,0,i])\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = state.state[\"precalc_filter\"][\"GaborLayerLogSigmaCoupled__2\"][\"kernel\"]\n",
    "kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_f_fft = jnp.fft.fftn(kernel[:,:,0,:], axes=(0,1))\n",
    "kernel_f_fft = jnp.fft.fftshift(kernel_f_fft)\n",
    "kernel_f_fft_abs_sum = jnp.abs(kernel_f_fft).sum(axis=-1)\n",
    "kernel_f_fft.shape, kernel_f_fft_abs_sum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8,8, figsize=(15,15))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(jnp.abs(kernel_f_fft[:,:,i]))\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(kernel_f_fft_abs_sum)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.params[\"CSFFourier_0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csf_sso, fx, fy = CSFFourier.csf_sso(fs=64, Nx=512//2, Ny=384//2, alpha=state.params[\"CSFFourier_0\"][\"alpha_achrom\"],\n",
    "                   beta=state.params[\"CSFFourier_0\"][\"beta_achrom\"], g=330.74, fm=state.params[\"CSFFourier_0\"][\"fm\"], \n",
    "                    l=0.837, s=state.params[\"CSFFourier_0\"][\"s\"], w=1.0, os=6.664)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csf_chrom_rg, csf_chrom_yb, fx, fy = CSFFourier.csf_chrom(fs=64, Nx=512//2, Ny=384//2, alpha_rg=state.params[\"CSFFourier_0\"][\"alpha_chrom_rg\"],\n",
    "                   alpha_yb=state.params[\"CSFFourier_0\"][\"alpha_chrom_yb\"],\n",
    "                   beta=state.params[\"CSFFourier_0\"][\"beta_chrom\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_csf(csf_a, csf_rg, csf_yb):\n",
    "    csfs = jnp.stack([csf_a, csf_rg, csf_yb], axis=-1)\n",
    "    E1 = jnp.sum(jnp.ones_like(csfs)**2)#**(1/2)\n",
    "    E_CSF = jnp.sum(csfs**2)#**(1/2)\n",
    "    csfs = (csfs/E_CSF)*E1\n",
    "    return csfs[:,:,0], csfs[:,:,1], csfs[:,:,2], csfs.min(), csfs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csf_sso, csf_chrom_rg, csf_chrom_yb, m, M = scale_csf(csf_sso, csf_chrom_rg, csf_chrom_yb)\n",
    "# csfs = scale_csf(csf_sso, csf_chrom_rg, csf_chrom_yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(csf_sso, vmin=m, vmax=M)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(csf_chrom_rg, vmin=m, vmax=M)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(csf_chrom_yb, vmin=m, vmax=M)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(state.params[\"GDNSpatioFreqOrient_0\"][\"bias\"])\n",
    "plt.plot(state.params[\"GDNSpatioFreqOrient_1\"][\"bias\"])\n",
    "plt.plot(state.params[\"GDNSpatioFreqOrient_2\"][\"bias\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def forward_intermediates(state, inputs):\n",
    "    return state.apply_fn({\"params\": state.params, **state.state}, inputs, train=False, capture_intermediates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ref, extra_ref = forward_intermediates(state, batch[0][:1])\n",
    "pred_dist, extra_dist = forward_intermediates(state, batch[1][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_tree = jax.tree_util.tree_map(lambda x: [], extra_ref[\"intermediates\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_trees(tree1, tree2):\n",
    "    return jax.tree_util.tree_map(lambda x,y: jnp.sum((x-y)**2)**(1/2), tree1, tree2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = mse_trees(extra_ref[\"intermediates\"], extra_dist[\"intermediates\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tree_values(*trees):\n",
    "    return jax.tree_util.tree_map(lambda *args: [*args], *trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_train_rdy_eval = dst_train.dataset.batch(1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees, moses = [], []\n",
    "for batch in tqdm(dst_train_rdy_eval.as_numpy_iterator()):\n",
    "    pred_ref, extra_ref = forward_intermediates(state, batch[0])\n",
    "    pred_dist, extra_dist = forward_intermediates(state, batch[1])\n",
    "    diff = mse_trees(extra_ref[\"intermediates\"], extra_dist[\"intermediates\"])\n",
    "    trees.append(diff)\n",
    "    moses.extend(batch[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = join_tree_values(*trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs_2 = jax.tree_util.tree_map(lambda x: x[0], diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(a, b):\n",
    "    try:\n",
    "        return stats.pearsonr(a, b)[0]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(diffs[\"GDNStarSign_0\"][\"__call__\"][0], moses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(diffs[\"JamesonHurvich_0\"][\"__call__\"][0], moses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(diffs[\"GDNStarSign_1\"][\"__call__\"][0], moses), correlation(diffs[\"GDNStarDisplacement_1\"][\"__call__\"][0], moses), correlation(diffs[\"GDNStarDisplacement_1\"][\"__call__\"][0], moses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(diffs[\"CSFFourier_0\"][\"__call__\"][0], moses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(diffs[\"GDNGaussianStarRunning_0\"][\"__call__\"][0], moses), correlation(diffs[\"GDNGaussianStarRunning_1\"][\"__call__\"][0], moses), correlation(diffs[\"GDNGaussianStarRunning_2\"][\"__call__\"][0], moses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(diffs[\"GaborLayerLogSigmaCoupled__0\"][\"__call__\"][0][0], moses), correlation(diffs[\"GaborLayerLogSigmaCoupled__1\"][\"__call__\"][0][0], moses), correlation(diffs[\"GaborLayerLogSigmaCoupled__2\"][\"__call__\"][0][0], moses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(diffs[\"GDNSpatioFreqOrient_0\"][\"__call__\"][0], moses), correlation(diffs[\"GDNSpatioFreqOrient_1\"][\"__call__\"][0], moses), correlation(diffs[\"GDNSpatioFreqOrient_2\"][\"__call__\"][0], moses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(diffs[\"__call__\"][0], moses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
