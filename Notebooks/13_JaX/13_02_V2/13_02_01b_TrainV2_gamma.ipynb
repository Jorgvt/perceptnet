{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IQA tracking params and variables\n",
    "\n",
    "> When using parametric layers we have to be able to keep track of the parameters and the variables of the model (which are not going to be trained). We're going to play with this concept using our implementation of the functional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os; os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".99\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 16:37:32.048513: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-31 16:37:32.105854: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-31 16:37:33.733245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from typing import Any, Callable, Sequence, Union\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "import jax\n",
    "from jax import lax, random, numpy as jnp\n",
    "from flax.core import freeze, unfreeze, FrozenDict\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "from flax.training import orbax_utils\n",
    "\n",
    "import optax\n",
    "import orbax.checkpoint\n",
    "\n",
    "from clu import metrics\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "from einops import reduce, rearrange\n",
    "import wandb\n",
    "from iqadatasets.datasets import *\n",
    "from fxlayers.layers import *\n",
    "from fxlayers.layers import GaussianLayerGamma, GaborLayerLogSigma_\n",
    "from fxlayers.initializers import *\n",
    "from JaxPlayground.utils.constraints import *\n",
    "from JaxPlayground.utils.wandb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.config.update(\"jax_debug_nans\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "> We're going to employ `iqadatasets` to ease the loading of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst_train = TID2008(\"/lustre/ific.uv.es/ml/uv075/Databases/IQA//TID/TID2008/\", exclude_imgs=[25])\n",
    "# dst_val = TID2013(\"/lustre/ific.uv.es/ml/uv075/Databases/IQA//TID/TID2013/\", exclude_imgs=[25])\n",
    "dst_train = TID2008(\"/media/disk/databases/BBDD_video_image/Image_Quality//TID/TID2008/\", exclude_imgs=[25])\n",
    "dst_val = TID2013(\"/media/disk/databases/BBDD_video_image/Image_Quality//TID/TID2013/\", exclude_imgs=[25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([384, 512, 3]), TensorShape([384, 512, 3]), TensorShape([]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, img_dist, mos = next(iter(dst_train.dataset))\n",
    "img.shape, img_dist.shape, mos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([384, 512, 3]), TensorShape([384, 512, 3]), TensorShape([]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, img_dist, mos = next(iter(dst_val.dataset))\n",
    "img.shape, img_dist.shape, mos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BATCH_SIZE: 64\n",
       "EPOCHS: 500\n",
       "GDN_CLIPPING: true\n",
       "LEARNING_RATE: 0.003\n",
       "NORMALIZE_ENERGY: true\n",
       "NORMALIZE_PROB: false\n",
       "N_ORIENTATIONS: 8\n",
       "N_SCALES: 4\n",
       "SEED: 42\n",
       "USE_BIAS: false\n",
       "ZERO_MEAN: true"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"EPOCHS\": 500,\n",
    "    \"LEARNING_RATE\": 3e-3,\n",
    "    \"SEED\": 42,\n",
    "    \"GDN_CLIPPING\": True,\n",
    "    \"NORMALIZE_PROB\": False,\n",
    "    \"NORMALIZE_ENERGY\": True,\n",
    "    \"ZERO_MEAN\": True,\n",
    "    \"USE_BIAS\": False,\n",
    "    \"N_SCALES\": 4,\n",
    "    \"N_ORIENTATIONS\": 8,\n",
    "}\n",
    "config = ConfigDict(config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: jorgvt. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/disk/users/vitojor/perceptnet/Notebooks/13_JaX/13_02_V2/wandb/run-20231031_164951-4t8vdwbp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jorgvt/PerceptNet_JaX/runs/4t8vdwbp' target=\"_blank\">V2_Init</a></strong> to <a href='https://wandb.ai/jorgvt/PerceptNet_JaX' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jorgvt/PerceptNet_JaX' target=\"_blank\">https://wandb.ai/jorgvt/PerceptNet_JaX</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jorgvt/PerceptNet_JaX/runs/4t8vdwbp' target=\"_blank\">https://wandb.ai/jorgvt/PerceptNet_JaX/runs/4t8vdwbp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BATCH_SIZE: 64\n",
       "EPOCHS: 500\n",
       "GDN_CLIPPING: true\n",
       "LEARNING_RATE: 0.003\n",
       "NORMALIZE_ENERGY: true\n",
       "NORMALIZE_PROB: false\n",
       "N_ORIENTATIONS: 8\n",
       "N_SCALES: 4\n",
       "SEED: 42\n",
       "USE_BIAS: false\n",
       "ZERO_MEAN: true"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"PerceptNet_JaX\",\n",
    "           name=\"V2_Init\",\n",
    "           job_type=\"training\",\n",
    "           config=config,\n",
    "           mode=\"online\",\n",
    "           )\n",
    "config = config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_train_rdy = dst_train.dataset.shuffle(buffer_size=100,\n",
    "                                      reshuffle_each_iteration=True,\n",
    "                                      seed=config.SEED)\\\n",
    "                                 .batch(config.BATCH_SIZE, drop_remainder=True)\n",
    "dst_val_rdy = dst_val.dataset.batch(config.BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model we're going to use\n",
    "\n",
    "> It's going to be a very simple model just for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GDNGaussianStarRunning(nn.Module):\n",
    "    \"\"\"GDN variation where x^* is obtained as a running mean of the previously obtained values.\"\"\"\n",
    "\n",
    "    kernel_size: int\n",
    "    inputs_star: float = 1.\n",
    "    outputs_star: Union[None, float] = None\n",
    "    fs: int = 1\n",
    "    apply_independently: bool = False\n",
    "    alpha: float = 2.\n",
    "    epsilon: float = 1/2\n",
    "    bias_init: Callable = nn.initializers.ones_init()\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 train=False,\n",
    "                 **kwargs,\n",
    "                 ):\n",
    "        # inputs_sign = jnp.sign(inputs)\n",
    "        # inputs = jnp.abs(inputs)\n",
    "        is_initialized = self.has_variable(\"batch_stats\", \"inputs_star\")\n",
    "        # inputs_star = self.variable(\"batch_stats\", \"inputs_star\", lambda x: x, jnp.quantile(inputs, q=0.95))\n",
    "        inputs_star = self.variable(\"batch_stats\", \"inputs_star\", lambda x: jnp.ones(x)*self.inputs_star, (1,))\n",
    "        if is_initialized and train:\n",
    "            inputs_star.value = (inputs_star.value + jnp.quantile(jnp.abs(inputs), q=0.95))/2\n",
    "        H = GaussianLayerGamma(features=inputs.shape[-1], kernel_size=self.kernel_size, use_bias=True, fs=self.fs, xmean=self.kernel_size/self.fs/2, ymean=self.kernel_size/self.fs/2, bias_init=self.bias_init, normalize_prob=config.NORMALIZE_PROB, normalize_energy=config.NORMALIZE_ENERGY)\n",
    "        inputs_star_ = jnp.ones_like(inputs)*inputs_star.value\n",
    "        denom = jnp.clip(H(inputs**self.alpha, train=train), a_min=1e-5)**self.epsilon\n",
    "        coef = (jnp.clip(H(inputs_star_**self.alpha, train=train), a_min=1e-5)**self.epsilon)#/inputs_star_\n",
    "        if self.outputs_star is not None: coef = coef/inputs_star.value*self.outputs_star\n",
    "        \n",
    "        return coef*inputs/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDNSpatioFreqOrient(nn.Module):\n",
    "    \"\"\"Generalized Divisive Normalization.\"\"\"\n",
    "    kernel_size: Union[int, Sequence[int]]\n",
    "    strides: int = 1\n",
    "    padding: str = \"SAME\"\n",
    "    inputs_star: float = 1.\n",
    "    outputs_star: Union[None, float] = None\n",
    "    fs: int = 1\n",
    "    apply_independently: bool = False\n",
    "    bias_init: Callable = nn.initializers.ones_init()\n",
    "    alpha: float = 2.\n",
    "    epsilon: float = 1/2 # Exponential of the denominator\n",
    "    eps: float = 1e-6 # Numerical stability in the denominator\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 fmean,\n",
    "                 theta_mean,\n",
    "                 train=False,\n",
    "                 ):\n",
    "        b, h, w, c = inputs.shape\n",
    "        bias = self.param(\"bias\",\n",
    "                          #equal_to(inputs_star/10),\n",
    "                          self.bias_init,\n",
    "                          (c,))\n",
    "        is_initialized = self.has_variable(\"batch_stats\", \"inputs_star\")\n",
    "        inputs_star = self.variable(\"batch_stats\", \"inputs_star\", lambda x: jnp.ones(x)*self.inputs_star, (len(self.inputs_star),))\n",
    "        inputs_star_ = jnp.ones_like(inputs)*inputs_star.value\n",
    "        GL = GaussianLayerGamma(features=c, kernel_size=self.kernel_size, strides=self.strides, padding=self.padding, fs=self.fs, xmean=self.kernel_size/self.fs/2, ymean=self.kernel_size/self.fs/2, normalize_prob=config.NORMALIZE_PROB, normalize_energy=config.NORMALIZE_ENERGY, use_bias=False, feature_group_count=c)\n",
    "        FG = FreqGaussian()\n",
    "        OG = OrientGaussian()\n",
    "        outputs = GL(inputs**self.alpha, train=train)#/(self.kernel_size**2)\n",
    "        outputs = FG(outputs, fmean=fmean)\n",
    "        ## Reshape so that the orientations are the innermost dimmension\n",
    "        outputs = rearrange(outputs, \"b h w (phase theta f) -> b h w (phase f theta)\", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)\n",
    "        outputs = OG(outputs, theta_mean=theta_mean)\n",
    "        ## Recover original disposition\n",
    "        denom = rearrange(outputs, \"b h w (phase f theta) -> b h w (phase theta f)\", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)\n",
    "\n",
    "        ## Coef\n",
    "        coef = GL(inputs_star_**self.alpha, train=train)#/(self.kernel_size**2)\n",
    "        coef = FG(coef, fmean=fmean)\n",
    "        coef = rearrange(coef, \"b h w (phase theta f) -> b h w (phase f theta)\", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)\n",
    "        coef = OG(coef, theta_mean=theta_mean) + bias\n",
    "        coef = rearrange(coef, \"b h w (phase f theta) -> b h w (phase theta f)\", b=b, h=h, w=w, phase=2, f=config.N_SCALES, theta=config.N_ORIENTATIONS)\n",
    "        coef = jnp.clip(coef+bias, a_min=1e-5)**self.epsilon\n",
    "        # coef = inputs_star.value * coef\n",
    "        if self.outputs_star is not None: coef = coef/inputs_star.value*self.outputs_star\n",
    "\n",
    "        if is_initialized and train:\n",
    "            inputs_star.value = (inputs_star.value + jnp.quantile(jnp.abs(inputs), q=0.95, axis=(0,1,2)))/2\n",
    "        return coef * inputs / (jnp.clip(denom+bias, a_min=1e-5)**self.epsilon + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "with open(\"gabor_x_star.pkl\", \"rb\") as f:\n",
    "    gabor_x_star = load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptNet(nn.Module):\n",
    "    \"\"\"IQA model inspired by the visual system.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs, # Assuming fs = 128 (cpd)\n",
    "                 **kwargs,\n",
    "                 ):\n",
    "        ## (Independent) Color equilibration (Gamma correction)\n",
    "        ## bias = 0.1 / kernel = 0.5\n",
    "        outputs = GDNStarSign(kernel_size=(1,1), apply_independently=True, inputs_star=1.)(inputs)\n",
    "        \n",
    "        ## ATD Transformation\n",
    "        outputs = JamesonHurvich()(outputs)\n",
    "        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n",
    "        \n",
    "        ## GDN Star A - T - D [Separated]\n",
    "        ### A\n",
    "        outputs0 = GDNStarSign(kernel_size=(1,1), apply_independently=True, inputs_star=170.)(outputs[:,:,:,0:1])\n",
    "        ### T\n",
    "        outputs1 = GDNStarDisplacement(kernel_size=(1,1), apply_independently=True, inputs_star=55.)(outputs[:,:,:,1:2])\n",
    "        outputs1 = outputs1*(2*55/170)\n",
    "        ### D\n",
    "        outputs2 = GDNStarDisplacement(kernel_size=(1,1), apply_independently=True, inputs_star=55.)(outputs[:,:,:,2:3])\n",
    "        outputs2 = outputs2*(2*55/170)\n",
    "        ### Put them back together\n",
    "        outputs = jnp.concatenate([outputs0, outputs1, outputs2], axis=-1)\n",
    "\n",
    "        ## Apply CSF on Fourier\n",
    "        outputs = CSFFourier(fs=64, norm_energy=True)(outputs)\n",
    "        outputs = nn.max_pool(outputs, window_shape=(2,2), strides=(2,2))\n",
    "\n",
    "        ## GDN per channel with mean substraction in T and D (Spatial Gaussian Kernel)\n",
    "        ## TO-DO: - Spatial Gaussian Kernel (0.02 deg) -> fs = 64/2 & 0.02*64/2 = sigma (px) = 0.69\n",
    "        ### A\n",
    "        ### (384/4, 512/4, 1)\n",
    "        ### fs = 32 / kernel_size = (11,11) -> 0.32 > 0.02 --> OK!\n",
    "        outputs0 = GDNGaussianStarRunning(kernel_size=11, apply_independently=True, bias_init=equal_to([0.1]), inputs_star=0.3, outputs_star=None, fs=32)(outputs[:,:,:,0:1], **kwargs)\n",
    "        ### T\n",
    "        outputs1 = GDNGaussianStarRunning(kernel_size=11, apply_independently=True, bias_init=equal_to([0.01**2]), inputs_star=0.06, outputs_star=None, fs=32)(outputs[:,:,:,1:2], **kwargs)\n",
    "        ### D\n",
    "        outputs2 = GDNGaussianStarRunning(kernel_size=11, apply_independently=True, bias_init=equal_to([0.01**2]), inputs_star=0.08, outputs_star=None, fs=32)(outputs[:,:,:,2:3], **kwargs)\n",
    "        ### Put them back together\n",
    "        outputs = jnp.concatenate([outputs0, outputs1, outputs2], axis=-1)\n",
    "\n",
    "        ## GaborLayer per channel with GDN mixing only same-origin-channel information\n",
    "        ### A\n",
    "        outputs0, fmean, theta_mean = GaborLayerLogSigma_(n_scales=config.N_SCALES, n_orientations=config.N_ORIENTATIONS, kernel_size=32, fs=32, strides=1, padding=\"SAME\", normalize_prob=config.NORMALIZE_PROB, normalize_energy=config.NORMALIZE_ENERGY, zero_mean=config.ZERO_MEAN, use_bias=config.USE_BIAS)(outputs[:,:,:,0:1], return_freq=True, return_theta=True, **kwargs)\n",
    "        ### [Gaussian] sigma = 0.2 (deg) fs = 32 / kernel_size = (21,21) -> 21/32 = 0.66 --> OK!\n",
    "        outputs0 = GDNSpatioFreqOrient(kernel_size=21, strides=1, padding=\"SAME\", fs=32, apply_independently=False, inputs_star=gabor_x_star[\"A\"])(outputs0, fmean=fmean, theta_mean=theta_mean, **kwargs)\n",
    "        ### T\n",
    "        outputs1, fmean, theta_mean = GaborLayerLogSigma_(n_scales=config.N_SCALES, n_orientations=config.N_ORIENTATIONS, kernel_size=32, fs=32, strides=1, padding=\"SAME\", normalize_prob=config.NORMALIZE_PROB, normalize_energy=config.NORMALIZE_ENERGY, zero_mean=config.ZERO_MEAN, use_bias=config.USE_BIAS)(outputs[:,:,:,1:2], return_freq=True, return_theta=True, **kwargs)\n",
    "        ### [Gaussian] sigma = 0.2 (deg) fs = 32 / kernel_size = (21,21) -> 21/32 = 0.66 --> OK!\n",
    "        outputs1 = GDNSpatioFreqOrient(kernel_size=21, strides=1, padding=\"SAME\", fs=32, apply_independently=False, inputs_star=gabor_x_star[\"T\"])(outputs1, fmean=fmean, theta_mean=theta_mean, **kwargs)\n",
    "        ### D\n",
    "        outputs2, fmean, theta_mean = GaborLayerLogSigma_(n_scales=config.N_SCALES, n_orientations=config.N_ORIENTATIONS, kernel_size=32, fs=32, strides=1, padding=\"SAME\", normalize_prob=config.NORMALIZE_PROB, normalize_energy=config.NORMALIZE_ENERGY, zero_mean=config.ZERO_MEAN, use_bias=config.USE_BIAS)(outputs[:,:,:,2:3], return_freq=True, return_theta=True, **kwargs)\n",
    "        ### [Gaussian] sigma = 0.2 (deg) fs = 32 / kernel_size = (21,21) -> 21/32 = 0.66 --> OK!\n",
    "        outputs2 = GDNSpatioFreqOrient(kernel_size=21, strides=1, padding=\"SAME\", fs=32, apply_independently=False, inputs_star=gabor_x_star[\"D\"])(outputs2, fmean=fmean, theta_mean=theta_mean, **kwargs)\n",
    "\n",
    "        ## Put them back together\n",
    "        outputs = jnp.concatenate([outputs0, outputs1, outputs2], axis=-1)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the metrics with `clu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n",
    "    loss: metrics.Average.from_output(\"loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `TrainState` doesn't include metrics, but it's very easy to subclass it so that it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    state: FrozenDict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function that initializes the `TrainState` from a module, a rng key and some optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(module, key, tx, input_shape):\n",
    "    \"\"\"Creates the initial `TrainState`.\"\"\"\n",
    "    variables = module.init(key, jnp.ones(input_shape))\n",
    "    state, params = variables.pop('params')\n",
    "    return TrainState.create(\n",
    "        apply_fn=module.apply,\n",
    "        params=params,\n",
    "        state=state,\n",
    "        tx=tx,\n",
    "        metrics=Metrics.empty()\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the training step\n",
    "\n",
    "> We want to write a function that takes the `TrainState` and a batch of data can performs an optimization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correlation(vec1, vec2):\n",
    "    vec1 = vec1.squeeze()\n",
    "    vec2 = vec2.squeeze()\n",
    "    vec1_mean = vec1.mean()\n",
    "    vec2_mean = vec2.mean()\n",
    "    num = vec1-vec1_mean\n",
    "    num *= vec2-vec2_mean\n",
    "    num = num.sum()\n",
    "    denom = jnp.sqrt(jnp.sum((vec1-vec1_mean)**2))\n",
    "    denom *= jnp.sqrt(jnp.sum((vec2-vec2_mean)**2))\n",
    "    return num/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    img, img_dist, mos = batch\n",
    "    def loss_fn(params):\n",
    "        ## Forward pass through the model\n",
    "        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=True)\n",
    "        img_dist_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img_dist, mutable=list(state.state.keys()), train=True)\n",
    "\n",
    "        ## Calculate the distance\n",
    "        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n",
    "        \n",
    "        ## Calculate pearson correlation\n",
    "        return pearson_correlation(dist, mos), updated_state\n",
    "    \n",
    "    (loss, updated_state), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics_updates = state.metrics.single_from_model_output(loss=loss)\n",
    "    metrics = state.metrics.merge(metrics_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    state = state.replace(state=updated_state)\n",
    "    return state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In their example, they don't calculate the metrics at the same time. I think it is kind of a waste because it means having to perform a new forward pass, but we'll follow as of now. Let's define a function to perform metric calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_metrics(*, state, batch):\n",
    "    \"\"\"Obtaining the metrics for a given batch.\"\"\"\n",
    "    img, img_dist, mos = batch\n",
    "    def loss_fn(params):\n",
    "        ## Forward pass through the model\n",
    "        img_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img, mutable=list(state.state.keys()), train=False)\n",
    "        img_dist_pred, updated_state = state.apply_fn({\"params\": params, **state.state}, img_dist, mutable=list(state.state.keys()), train=False)\n",
    "\n",
    "        ## Calculate the distance\n",
    "        dist = ((img_pred - img_dist_pred)**2).sum(axis=(1,2,3))**(1/2)\n",
    "        \n",
    "        ## Calculate pearson correlation\n",
    "        return pearson_correlation(dist, mos)\n",
    "    \n",
    "    metrics_updates = state.metrics.single_from_model_output(loss=loss_fn(state.params))\n",
    "    metrics = state.metrics.merge(metrics_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(PerceptNet(), random.PRNGKey(config.SEED), optax.adam(config.LEARNING_RATE), input_shape=(1,384,512,3))\n",
    "state = state.replace(params=clip_layer(state.params, \"GDN\", a_min=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trainable(path):\n",
    "    return (\"A\" in path) or (\"alpha_achrom\" in path) or (\"alpha_chrom_rg\" in path) or (\"alpha_chrom_yb\" in path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    GDNStarSign_0: {\n",
       "        Conv_0: {\n",
       "            kernel: 'trainable',\n",
       "            bias: 'trainable',\n",
       "        },\n",
       "    },\n",
       "    GDNStarSign_1: {\n",
       "        Conv_0: {\n",
       "            kernel: 'trainable',\n",
       "            bias: 'trainable',\n",
       "        },\n",
       "    },\n",
       "    GDNStarDisplacement_0: {\n",
       "        Conv_0: {\n",
       "            kernel: 'trainable',\n",
       "            bias: 'trainable',\n",
       "        },\n",
       "    },\n",
       "    GDNStarDisplacement_1: {\n",
       "        Conv_0: {\n",
       "            kernel: 'trainable',\n",
       "            bias: 'trainable',\n",
       "        },\n",
       "    },\n",
       "    CSFFourier_0: {\n",
       "        alpha_achrom: 'non_trainable',\n",
       "        alpha_chrom_rg: 'non_trainable',\n",
       "        alpha_chrom_yb: 'non_trainable',\n",
       "        beta_achrom: 'trainable',\n",
       "        beta_chrom: 'trainable',\n",
       "        fm: 'trainable',\n",
       "        s: 'trainable',\n",
       "    },\n",
       "    GDNGaussianStarRunning_0: {\n",
       "        GaussianLayerGamma_0: {\n",
       "            gamma: 'trainable',\n",
       "            A: 'non_trainable',\n",
       "            bias: 'trainable',\n",
       "        },\n",
       "    },\n",
       "    GDNGaussianStarRunning_1: {\n",
       "        GaussianLayerGamma_0: {\n",
       "            gamma: 'trainable',\n",
       "            A: 'non_trainable',\n",
       "            bias: 'trainable',\n",
       "        },\n",
       "    },\n",
       "    GDNGaussianStarRunning_2: {\n",
       "        GaussianLayerGamma_0: {\n",
       "            gamma: 'trainable',\n",
       "            A: 'non_trainable',\n",
       "            bias: 'trainable',\n",
       "        },\n",
       "    },\n",
       "    GaborLayerLogSigma__0: {\n",
       "        freq: 'trainable',\n",
       "        logsigmax2: 'trainable',\n",
       "        logsigmay2: 'trainable',\n",
       "        theta: 'trainable',\n",
       "        sigma_theta: 'trainable',\n",
       "    },\n",
       "    GDNSpatioFreqOrient_0: {\n",
       "        bias: 'trainable',\n",
       "        GaussianLayerGamma_0: {\n",
       "            gamma: 'trainable',\n",
       "            A: 'non_trainable',\n",
       "        },\n",
       "        FreqGaussian_0: {\n",
       "            sigma: 'trainable',\n",
       "        },\n",
       "        OrientGaussian_0: {\n",
       "            sigma: 'trainable',\n",
       "        },\n",
       "    },\n",
       "    GaborLayerLogSigma__1: {\n",
       "        freq: 'trainable',\n",
       "        logsigmax2: 'trainable',\n",
       "        logsigmay2: 'trainable',\n",
       "        theta: 'trainable',\n",
       "        sigma_theta: 'trainable',\n",
       "    },\n",
       "    GDNSpatioFreqOrient_1: {\n",
       "        bias: 'trainable',\n",
       "        GaussianLayerGamma_0: {\n",
       "            gamma: 'trainable',\n",
       "            A: 'non_trainable',\n",
       "        },\n",
       "        FreqGaussian_0: {\n",
       "            sigma: 'trainable',\n",
       "        },\n",
       "        OrientGaussian_0: {\n",
       "            sigma: 'trainable',\n",
       "        },\n",
       "    },\n",
       "    GaborLayerLogSigma__2: {\n",
       "        freq: 'trainable',\n",
       "        logsigmax2: 'trainable',\n",
       "        logsigmay2: 'trainable',\n",
       "        theta: 'trainable',\n",
       "        sigma_theta: 'trainable',\n",
       "    },\n",
       "    GDNSpatioFreqOrient_2: {\n",
       "        bias: 'trainable',\n",
       "        GaussianLayerGamma_0: {\n",
       "            gamma: 'trainable',\n",
       "            A: 'non_trainable',\n",
       "        },\n",
       "        FreqGaussian_0: {\n",
       "            sigma: 'trainable',\n",
       "        },\n",
       "        OrientGaussian_0: {\n",
       "            sigma: 'trainable',\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_tree = freeze(flax.traverse_util.path_aware_map(lambda path, v: \"non_trainable\" if check_trainable(path)  else \"trainable\", state.params))\n",
    "trainable_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {\n",
    "    \"trainable\": optax.adam(learning_rate=config.LEARNING_RATE),\n",
    "    \"non_trainable\": optax.set_to_zero(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = optax.multi_transform(optimizers, trainable_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(PerceptNet(), random.PRNGKey(config.SEED), tx, input_shape=(1,384,512,3))\n",
    "state = state.replace(params=clip_layer(state.params, \"GDN\", a_min=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(state.params))\n",
    "param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.summary[\"trainable_parameters\"] = param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = state.replace(params=unfreeze(state.params))\n",
    "\n",
    "## DN 0\n",
    "state.params[\"GDNStarSign_0\"][\"Conv_0\"][\"bias\"] = jnp.ones_like(state.params[\"GDNStarSign_0\"][\"Conv_0\"][\"bias\"])*0.1\n",
    "state.params[\"GDNStarSign_0\"][\"Conv_0\"][\"kernel\"] = jnp.ones_like(state.params[\"GDNStarSign_0\"][\"Conv_0\"][\"kernel\"])*0.5\n",
    "\n",
    "## DN J&H\n",
    "state.params[\"GDNStarSign_1\"][\"Conv_0\"][\"bias\"] = jnp.ones_like(state.params[\"GDNStarSign_1\"][\"Conv_0\"][\"bias\"])*30.**2\n",
    "state.params[\"GDNStarSign_1\"][\"Conv_0\"][\"kernel\"] = jnp.ones_like(state.params[\"GDNStarSign_1\"][\"Conv_0\"][\"kernel\"])*0.5\n",
    "\n",
    "state.params[\"GDNStarDisplacement_0\"][\"Conv_0\"][\"bias\"] = jnp.ones_like(state.params[\"GDNStarDisplacement_0\"][\"Conv_0\"][\"bias\"])*10.**2\n",
    "state.params[\"GDNStarDisplacement_0\"][\"Conv_0\"][\"kernel\"] = jnp.ones_like(state.params[\"GDNStarDisplacement_0\"][\"Conv_0\"][\"kernel\"])*0.5\n",
    "\n",
    "state.params[\"GDNStarDisplacement_1\"][\"Conv_0\"][\"bias\"] = jnp.ones_like(state.params[\"GDNStarDisplacement_1\"][\"Conv_0\"][\"bias\"])*10.**2\n",
    "state.params[\"GDNStarDisplacement_1\"][\"Conv_0\"][\"kernel\"] = jnp.ones_like(state.params[\"GDNStarDisplacement_1\"][\"Conv_0\"][\"kernel\"])*0.5\n",
    "\n",
    "state.params[\"GDNGaussianStarRunning_0\"][\"GaussianLayerGamma_0\"][\"gamma\"] = jnp.ones_like(state.params[\"GDNGaussianStarRunning_0\"][\"GaussianLayerGamma_0\"][\"gamma\"])*(1./0.04)\n",
    "state.params[\"GDNGaussianStarRunning_1\"][\"GaussianLayerGamma_0\"][\"gamma\"] = jnp.ones_like(state.params[\"GDNGaussianStarRunning_1\"][\"GaussianLayerGamma_0\"][\"gamma\"])*(1./0.04)\n",
    "state.params[\"GDNGaussianStarRunning_2\"][\"GaussianLayerGamma_0\"][\"gamma\"] = jnp.ones_like(state.params[\"GDNGaussianStarRunning_2\"][\"GaussianLayerGamma_0\"][\"gamma\"])*(1./0.04)\n",
    "\n",
    "state.params[\"GDNSpatioFreqOrient_0\"][\"GaussianLayerGamma_0\"][\"gamma\"] = jnp.ones_like(state.params[\"GDNSpatioFreqOrient_0\"][\"GaussianLayerGamma_0\"][\"gamma\"])*(1./0.1)\n",
    "state.params[\"GDNSpatioFreqOrient_1\"][\"GaussianLayerGamma_0\"][\"gamma\"] = jnp.ones_like(state.params[\"GDNSpatioFreqOrient_1\"][\"GaussianLayerGamma_0\"][\"gamma\"])*(1./0.1)\n",
    "state.params[\"GDNSpatioFreqOrient_2\"][\"GaussianLayerGamma_0\"][\"gamma\"] = jnp.ones_like(state.params[\"GDNSpatioFreqOrient_2\"][\"GaussianLayerGamma_0\"][\"gamma\"])*(1./0.1)\n",
    "\n",
    "state.params[\"GDNSpatioFreqOrient_0\"][\"OrientGaussian_0\"][\"sigma\"] = jnp.ones_like(state.params[\"GDNSpatioFreqOrient_0\"][\"OrientGaussian_0\"][\"sigma\"])*20\n",
    "state.params[\"GDNSpatioFreqOrient_1\"][\"OrientGaussian_0\"][\"sigma\"] = jnp.ones_like(state.params[\"GDNSpatioFreqOrient_1\"][\"OrientGaussian_0\"][\"sigma\"])*20\n",
    "state.params[\"GDNSpatioFreqOrient_2\"][\"OrientGaussian_0\"][\"sigma\"] = jnp.ones_like(state.params[\"GDNSpatioFreqOrient_2\"][\"OrientGaussian_0\"][\"sigma\"])*20\n",
    "\n",
    "state.params[\"GDNSpatioFreqOrient_0\"][\"bias\"] = jnp.tile(jnp.array([0.001, 0.002, 0.0035, 0.01])/100, reps=config.N_ORIENTATIONS*2)\n",
    "state.params[\"GDNSpatioFreqOrient_1\"][\"bias\"] = jnp.tile(jnp.array([0.001, 0.002, 0.0035, 0.01])/100, reps=config.N_ORIENTATIONS*2)\n",
    "state.params[\"GDNSpatioFreqOrient_2\"][\"bias\"] = jnp.tile(jnp.array([0.001, 0.002, 0.0035, 0.01])/100, reps=config.N_ORIENTATIONS*2)\n",
    "\n",
    "\n",
    "state = state.replace(params=freeze(state.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before actually training the model we're going to set up the checkpointer to be able to save our trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "save_args = orbax_utils.save_args_from_target(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 16:38:15.448732: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [1632]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dst_train_rdy.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def forward(state, inputs):\n",
    "    return state.apply_fn({\"params\": state.params, **state.state}, inputs, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.1 s, sys: 4.13 s, total: 28.2 s\n",
      "Wall time: 19.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 96, 128, 192)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "outputs = forward(state, batch[0])\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 31s, sys: 8.21 s, total: 3min 39s\n",
      "Wall time: 2min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "s1 = train_step(state, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.config.update(\"jax_debug_nans\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'ValueError'>",
     "evalue": "autodetected range of [nan, nan] is not finite",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:28\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:28\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/site-packages/wandb/sdk/data_types/histogram.py:76\u001b[0m, in \u001b[0;36mHistogram.__init__\u001b[0;34m(self, sequence, np_histogram, num_bins)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     np \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mget_module(\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, required\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuto creation of histograms requires numpy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m     )\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistogram, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbins \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_bins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistogram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistogram\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbins\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/site-packages/numpy/lib/histograms.py:793\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, normed, weights, density)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03mCompute the histogram of a dataset.\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m \n\u001b[1;32m    790\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    791\u001b[0m a, weights \u001b[38;5;241m=\u001b[39m _ravel_and_check_weights(a, weights)\n\u001b[0;32m--> 793\u001b[0m bin_edges, uniform_bins \u001b[38;5;241m=\u001b[39m \u001b[43m_get_bin_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;66;03m# Histogram is an integer or a float array depending on the weights.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/site-packages/numpy/lib/histograms.py:426\u001b[0m, in \u001b[0;36m_get_bin_edges\u001b[0;34m(a, bins, range, weights)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_equal_bins \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins` must be positive, when an integer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 426\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m \u001b[43m_get_outer_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(bins) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    429\u001b[0m     bin_edges \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(bins)\n",
      "File \u001b[0;32m~/miniconda3/envs/deep_gpu_tf/lib/python3.8/site-packages/numpy/lib/histograms.py:323\u001b[0m, in \u001b[0;36m_get_outer_edges\u001b[0;34m(a, range)\u001b[0m\n\u001b[1;32m    321\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mmin(), a\u001b[38;5;241m.\u001b[39mmax()\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (np\u001b[38;5;241m.\u001b[39misfinite(first_edge) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(last_edge)):\n\u001b[0;32m--> 323\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautodetected range of [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] is not finite\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(first_edge, last_edge))\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# expand empty range to avoid divide by zero\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_edge \u001b[38;5;241m==\u001b[39m last_edge:\n",
      "\u001b[0;31mValueError\u001b[0m: autodetected range of [nan, nan] is not finite"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -> [Train] Loss: -0.46230649948120117 [Val] Loss: -0.6404291391372681\n",
      "Epoch 1 -> [Train] Loss: -0.5066695809364319 [Val] Loss: -0.6715699434280396\n",
      "Epoch 2 -> [Train] Loss: -0.5312695503234863 [Val] Loss: -0.6914645433425903\n",
      "Epoch 3 -> [Train] Loss: -0.5660589337348938 [Val] Loss: -0.71083003282547\n",
      "Epoch 4 -> [Train] Loss: -0.5968101620674133 [Val] Loss: -0.7331307530403137\n",
      "Epoch 5 -> [Train] Loss: -0.6504687070846558 [Val] Loss: -0.7602219581604004\n",
      "Epoch 6 -> [Train] Loss: -0.6882992386817932 [Val] Loss: -0.7862057089805603\n",
      "Epoch 7 -> [Train] Loss: -0.7348223924636841 [Val] Loss: -0.8190333843231201\n",
      "Epoch 8 -> [Train] Loss: -0.759041965007782 [Val] Loss: -0.8245800733566284\n",
      "Epoch 9 -> [Train] Loss: -0.7554469704627991 [Val] Loss: -0.8192509412765503\n",
      "Epoch 10 -> [Train] Loss: -0.7640332579612732 [Val] Loss: -0.8232132792472839\n",
      "Epoch 11 -> [Train] Loss: -0.7599337697029114 [Val] Loss: -0.8247334957122803\n",
      "Epoch 12 -> [Train] Loss: -0.7729227542877197 [Val] Loss: -0.8264163136482239\n",
      "Epoch 13 -> [Train] Loss: -0.7748399972915649 [Val] Loss: -0.8296445608139038\n",
      "Epoch 14 -> [Train] Loss: -0.7703438401222229 [Val] Loss: -0.8301324844360352\n",
      "Epoch 15 -> [Train] Loss: -0.7810249328613281 [Val] Loss: -0.8315504193305969\n",
      "Epoch 16 -> [Train] Loss: -0.7782710194587708 [Val] Loss: -0.834337055683136\n",
      "Epoch 17 -> [Train] Loss: -0.7836156487464905 [Val] Loss: -0.8348211050033569\n",
      "Epoch 18 -> [Train] Loss: -0.788429319858551 [Val] Loss: -0.8382349610328674\n",
      "Epoch 19 -> [Train] Loss: -0.7896750569343567 [Val] Loss: -0.8313338756561279\n",
      "Epoch 20 -> [Train] Loss: -0.795792281627655 [Val] Loss: -0.8418998122215271\n",
      "Epoch 21 -> [Train] Loss: -0.8059840202331543 [Val] Loss: -0.8463351130485535\n",
      "Epoch 22 -> [Train] Loss: -0.8085750341415405 [Val] Loss: -0.8482328057289124\n",
      "Epoch 23 -> [Train] Loss: -0.8139320015907288 [Val] Loss: -0.8501112461090088\n",
      "Epoch 24 -> [Train] Loss: -0.8214324712753296 [Val] Loss: -0.8517827391624451\n",
      "Epoch 25 -> [Train] Loss: -0.8169152736663818 [Val] Loss: -0.8511649370193481\n",
      "Epoch 26 -> [Train] Loss: -0.8147245049476624 [Val] Loss: -0.8511943817138672\n",
      "Epoch 27 -> [Train] Loss: -0.8105646371841431 [Val] Loss: -0.8524163365364075\n",
      "Epoch 28 -> [Train] Loss: -0.8205981254577637 [Val] Loss: -0.852517306804657\n",
      "Epoch 29 -> [Train] Loss: -0.8164458870887756 [Val] Loss: -0.8532260656356812\n",
      "Epoch 30 -> [Train] Loss: -0.8225022554397583 [Val] Loss: -0.8547159433364868\n",
      "Epoch 31 -> [Train] Loss: -0.8217597007751465 [Val] Loss: -0.8549154996871948\n",
      "Epoch 32 -> [Train] Loss: -0.8184237480163574 [Val] Loss: -0.8569760918617249\n",
      "Epoch 33 -> [Train] Loss: -0.8213362693786621 [Val] Loss: -0.8547407388687134\n",
      "Epoch 34 -> [Train] Loss: -0.8210954666137695 [Val] Loss: -0.857356607913971\n",
      "Epoch 35 -> [Train] Loss: -0.8301331996917725 [Val] Loss: -0.8569180369377136\n",
      "Epoch 36 -> [Train] Loss: -0.8275803327560425 [Val] Loss: -0.8575151562690735\n",
      "Epoch 37 -> [Train] Loss: -0.8264265060424805 [Val] Loss: -0.8569594621658325\n",
      "Epoch 38 -> [Train] Loss: -0.8292796611785889 [Val] Loss: -0.8585204482078552\n",
      "Epoch 39 -> [Train] Loss: -0.8275144100189209 [Val] Loss: -0.8592928647994995\n",
      "Epoch 40 -> [Train] Loss: -0.831078290939331 [Val] Loss: -0.8587947487831116\n",
      "Epoch 41 -> [Train] Loss: -0.8336575627326965 [Val] Loss: -0.8598527908325195\n",
      "Epoch 42 -> [Train] Loss: -0.8318774700164795 [Val] Loss: -0.8599694967269897\n",
      "Epoch 43 -> [Train] Loss: -0.8213449716567993 [Val] Loss: -0.8407572507858276\n",
      "Epoch 44 -> [Train] Loss: -0.8081344962120056 [Val] Loss: -0.8464856743812561\n",
      "Epoch 45 -> [Train] Loss: -0.8137570023536682 [Val] Loss: -0.8504233956336975\n",
      "Epoch 46 -> [Train] Loss: -0.8223967552185059 [Val] Loss: -0.8517693877220154\n",
      "Epoch 47 -> [Train] Loss: -0.8201631903648376 [Val] Loss: -0.8544580340385437\n",
      "Epoch 48 -> [Train] Loss: -0.8231505155563354 [Val] Loss: -0.852484405040741\n",
      "Epoch 49 -> [Train] Loss: -0.8271158337593079 [Val] Loss: -0.8558029532432556\n",
      "Epoch 50 -> [Train] Loss: -0.8233235478401184 [Val] Loss: -0.8538809418678284\n",
      "Epoch 51 -> [Train] Loss: -0.8262259364128113 [Val] Loss: -0.8556157350540161\n",
      "Epoch 52 -> [Train] Loss: -0.8266189098358154 [Val] Loss: -0.8564888834953308\n",
      "Epoch 53 -> [Train] Loss: -0.8276190757751465 [Val] Loss: -0.8572913408279419\n",
      "Epoch 54 -> [Train] Loss: -0.8253601789474487 [Val] Loss: -0.8574262261390686\n",
      "Epoch 55 -> [Train] Loss: -0.8294199705123901 [Val] Loss: -0.8581400513648987\n",
      "Epoch 56 -> [Train] Loss: -0.8276466131210327 [Val] Loss: -0.8586939573287964\n",
      "Epoch 57 -> [Train] Loss: -0.8270049095153809 [Val] Loss: -0.8499906063079834\n",
      "Epoch 58 -> [Train] Loss: -0.8086203336715698 [Val] Loss: -0.8521395325660706\n",
      "Epoch 59 -> [Train] Loss: -0.8265463709831238 [Val] Loss: -0.8603755831718445\n",
      "Epoch 60 -> [Train] Loss: -0.8311061859130859 [Val] Loss: -0.8627355098724365\n",
      "Epoch 61 -> [Train] Loss: -0.8365156650543213 [Val] Loss: -0.86350017786026\n",
      "Epoch 62 -> [Train] Loss: -0.8371239900588989 [Val] Loss: -0.8641369342803955\n",
      "Epoch 63 -> [Train] Loss: -0.8379588723182678 [Val] Loss: -0.863747239112854\n",
      "Epoch 64 -> [Train] Loss: -0.8403286337852478 [Val] Loss: -0.8646544218063354\n",
      "Epoch 65 -> [Train] Loss: -0.83829265832901 [Val] Loss: -0.8640620708465576\n",
      "Epoch 66 -> [Train] Loss: -0.8375434875488281 [Val] Loss: -0.8642955422401428\n",
      "Epoch 67 -> [Train] Loss: -0.8379659652709961 [Val] Loss: -0.8649696707725525\n",
      "Epoch 68 -> [Train] Loss: -0.8369669914245605 [Val] Loss: -0.8639822602272034\n",
      "Epoch 69 -> [Train] Loss: -0.8418310284614563 [Val] Loss: -0.8652560114860535\n",
      "Epoch 70 -> [Train] Loss: -0.8406800627708435 [Val] Loss: -0.864965558052063\n",
      "Epoch 71 -> [Train] Loss: -0.8388307690620422 [Val] Loss: -0.8651108741760254\n",
      "Epoch 72 -> [Train] Loss: -0.8424161076545715 [Val] Loss: -0.8656325936317444\n",
      "Epoch 73 -> [Train] Loss: -0.8424481153488159 [Val] Loss: -0.8667981624603271\n",
      "Epoch 74 -> [Train] Loss: -0.8419028520584106 [Val] Loss: -0.8653067350387573\n",
      "Epoch 75 -> [Train] Loss: -0.8471085429191589 [Val] Loss: -0.8666533827781677\n",
      "Epoch 76 -> [Train] Loss: -0.8448911309242249 [Val] Loss: -0.8653332591056824\n",
      "Epoch 77 -> [Train] Loss: -0.8449617028236389 [Val] Loss: -0.8662926554679871\n",
      "Epoch 78 -> [Train] Loss: -0.8460205793380737 [Val] Loss: -0.8671650290489197\n",
      "Epoch 79 -> [Train] Loss: -0.8480696678161621 [Val] Loss: -0.86832195520401\n",
      "Epoch 80 -> [Train] Loss: -0.8413418531417847 [Val] Loss: -0.8666865229606628\n",
      "Epoch 81 -> [Train] Loss: -0.8442773222923279 [Val] Loss: -0.8685521483421326\n",
      "Epoch 82 -> [Train] Loss: -0.8387472629547119 [Val] Loss: -0.8533118367195129\n",
      "Epoch 83 -> [Train] Loss: -0.8286632299423218 [Val] Loss: -0.8577003479003906\n",
      "Epoch 84 -> [Train] Loss: -0.8302951455116272 [Val] Loss: -0.858989417552948\n",
      "Epoch 85 -> [Train] Loss: -0.8306639790534973 [Val] Loss: -0.8596435785293579\n",
      "Epoch 86 -> [Train] Loss: -0.8378893136978149 [Val] Loss: -0.8612219095230103\n",
      "Epoch 87 -> [Train] Loss: -0.8348125219345093 [Val] Loss: -0.8623083829879761\n",
      "Epoch 88 -> [Train] Loss: -0.8424873948097229 [Val] Loss: -0.8613094091415405\n",
      "Epoch 89 -> [Train] Loss: -0.83945232629776 [Val] Loss: -0.8618001341819763\n",
      "Epoch 90 -> [Train] Loss: -0.840971052646637 [Val] Loss: -0.8624006509780884\n",
      "Epoch 91 -> [Train] Loss: -0.8399505615234375 [Val] Loss: -0.8634337782859802\n",
      "Epoch 92 -> [Train] Loss: -0.8418697118759155 [Val] Loss: -0.8613505363464355\n",
      "Epoch 93 -> [Train] Loss: -0.8387479186058044 [Val] Loss: -0.8627350926399231\n",
      "Epoch 94 -> [Train] Loss: -0.8398092985153198 [Val] Loss: -0.8631258010864258\n",
      "Epoch 95 -> [Train] Loss: -0.8403350114822388 [Val] Loss: -0.8633444905281067\n",
      "Epoch 96 -> [Train] Loss: -0.8398972153663635 [Val] Loss: -0.8634356260299683\n",
      "Epoch 97 -> [Train] Loss: -0.8388261198997498 [Val] Loss: -0.8624767065048218\n",
      "Epoch 98 -> [Train] Loss: -0.8380122780799866 [Val] Loss: -0.8641772270202637\n",
      "Epoch 99 -> [Train] Loss: -0.8409772515296936 [Val] Loss: -0.8635263442993164\n",
      "Epoch 100 -> [Train] Loss: -0.8415974378585815 [Val] Loss: -0.8643908500671387\n",
      "Epoch 101 -> [Train] Loss: -0.8416098356246948 [Val] Loss: -0.8640998005867004\n",
      "Epoch 102 -> [Train] Loss: -0.8442776799201965 [Val] Loss: -0.8656833171844482\n",
      "Epoch 103 -> [Train] Loss: -0.8439392447471619 [Val] Loss: -0.8633825778961182\n",
      "Epoch 104 -> [Train] Loss: -0.839199423789978 [Val] Loss: -0.8646319508552551\n",
      "Epoch 105 -> [Train] Loss: -0.8420997262001038 [Val] Loss: -0.8636976480484009\n",
      "Epoch 106 -> [Train] Loss: -0.8429650664329529 [Val] Loss: -0.8665685057640076\n",
      "Epoch 107 -> [Train] Loss: -0.8400942087173462 [Val] Loss: -0.8642029166221619\n",
      "Epoch 108 -> [Train] Loss: -0.8421509265899658 [Val] Loss: -0.8648486733436584\n",
      "Epoch 109 -> [Train] Loss: -0.8380269408226013 [Val] Loss: -0.8644608855247498\n",
      "Epoch 110 -> [Train] Loss: -0.8422102928161621 [Val] Loss: -0.8647187948226929\n",
      "Epoch 111 -> [Train] Loss: -0.8467556834220886 [Val] Loss: -0.86512291431427\n",
      "Epoch 112 -> [Train] Loss: -0.8414800763130188 [Val] Loss: -0.8656716346740723\n",
      "Epoch 113 -> [Train] Loss: -0.8394321799278259 [Val] Loss: -0.865543782711029\n",
      "Epoch 114 -> [Train] Loss: -0.8422142863273621 [Val] Loss: -0.8656924366950989\n",
      "Epoch 115 -> [Train] Loss: -0.8434193134307861 [Val] Loss: -0.8656455278396606\n",
      "Epoch 116 -> [Train] Loss: -0.843632698059082 [Val] Loss: -0.8647335767745972\n",
      "Epoch 117 -> [Train] Loss: -0.8450066447257996 [Val] Loss: -0.8662715554237366\n",
      "Epoch 118 -> [Train] Loss: -0.8464459180831909 [Val] Loss: -0.8656654357910156\n",
      "Epoch 119 -> [Train] Loss: -0.844386875629425 [Val] Loss: -0.8669214248657227\n",
      "Epoch 120 -> [Train] Loss: -0.8475959300994873 [Val] Loss: -0.8647443056106567\n",
      "Epoch 121 -> [Train] Loss: -0.8475615978240967 [Val] Loss: -0.8662364482879639\n",
      "Epoch 122 -> [Train] Loss: -0.8452074527740479 [Val] Loss: -0.8664002418518066\n",
      "Epoch 123 -> [Train] Loss: -0.8419198393821716 [Val] Loss: -0.8658066987991333\n",
      "Epoch 124 -> [Train] Loss: -0.8492830991744995 [Val] Loss: -0.8651754856109619\n",
      "Epoch 125 -> [Train] Loss: -0.8492696285247803 [Val] Loss: -0.8673125505447388\n",
      "Epoch 126 -> [Train] Loss: -0.8436129689216614 [Val] Loss: -0.8650465607643127\n",
      "Epoch 127 -> [Train] Loss: -0.8448194861412048 [Val] Loss: -0.8646594882011414\n",
      "Epoch 128 -> [Train] Loss: -0.8468179106712341 [Val] Loss: -0.8660250306129456\n",
      "Epoch 129 -> [Train] Loss: -0.845703661441803 [Val] Loss: -0.8675819039344788\n",
      "Epoch 130 -> [Train] Loss: -0.8475264310836792 [Val] Loss: -0.8662566542625427\n",
      "Epoch 131 -> [Train] Loss: -0.845487654209137 [Val] Loss: -0.8666828870773315\n",
      "Epoch 132 -> [Train] Loss: -0.844652533531189 [Val] Loss: -0.8679441809654236\n",
      "Epoch 133 -> [Train] Loss: -0.8498336672782898 [Val] Loss: -0.8653756976127625\n",
      "Epoch 134 -> [Train] Loss: -0.8486181497573853 [Val] Loss: -0.8676567077636719\n",
      "Epoch 135 -> [Train] Loss: -0.8496283292770386 [Val] Loss: -0.8667954802513123\n",
      "Epoch 136 -> [Train] Loss: -0.8483783006668091 [Val] Loss: -0.8667529225349426\n",
      "Epoch 137 -> [Train] Loss: -0.8494214415550232 [Val] Loss: -0.8677271604537964\n",
      "Epoch 138 -> [Train] Loss: -0.8502140641212463 [Val] Loss: -0.8655571341514587\n",
      "Epoch 139 -> [Train] Loss: -0.8452292680740356 [Val] Loss: -0.8671083450317383\n",
      "Epoch 140 -> [Train] Loss: -0.8471965789794922 [Val] Loss: -0.8666516542434692\n",
      "Epoch 141 -> [Train] Loss: -0.8478174209594727 [Val] Loss: -0.8667193055152893\n",
      "Epoch 142 -> [Train] Loss: -0.8492612838745117 [Val] Loss: -0.8682085871696472\n",
      "Epoch 143 -> [Train] Loss: -0.8521211743354797 [Val] Loss: -0.8666676878929138\n",
      "Epoch 144 -> [Train] Loss: -0.8464207053184509 [Val] Loss: -0.8669961094856262\n",
      "Epoch 145 -> [Train] Loss: -0.8500394225120544 [Val] Loss: -0.8674409985542297\n",
      "Epoch 146 -> [Train] Loss: -0.8454198241233826 [Val] Loss: -0.8661604523658752\n",
      "Epoch 147 -> [Train] Loss: -0.8419037461280823 [Val] Loss: -0.8692267537117004\n",
      "Epoch 148 -> [Train] Loss: -0.8466492295265198 [Val] Loss: -0.8667263388633728\n",
      "Epoch 149 -> [Train] Loss: -0.8461129069328308 [Val] Loss: -0.8672232031822205\n",
      "Epoch 150 -> [Train] Loss: -0.8509783744812012 [Val] Loss: -0.8686760067939758\n",
      "Epoch 151 -> [Train] Loss: -0.8454864025115967 [Val] Loss: -0.8673157691955566\n",
      "Epoch 152 -> [Train] Loss: -0.846197247505188 [Val] Loss: -0.8689603209495544\n",
      "Epoch 153 -> [Train] Loss: -0.8461076021194458 [Val] Loss: -0.8678897023200989\n",
      "Epoch 154 -> [Train] Loss: -0.851265013217926 [Val] Loss: -0.8689402341842651\n",
      "Epoch 155 -> [Train] Loss: -0.8482109904289246 [Val] Loss: -0.8669925332069397\n",
      "Epoch 156 -> [Train] Loss: -0.8496116995811462 [Val] Loss: -0.8687667846679688\n",
      "Epoch 157 -> [Train] Loss: -0.849064290523529 [Val] Loss: -0.8693028092384338\n",
      "Epoch 158 -> [Train] Loss: -0.8466322422027588 [Val] Loss: -0.8684847950935364\n",
      "Epoch 159 -> [Train] Loss: -0.8483855724334717 [Val] Loss: -0.8694170713424683\n",
      "Epoch 160 -> [Train] Loss: -0.8551474809646606 [Val] Loss: -0.8690052032470703\n",
      "Epoch 161 -> [Train] Loss: -0.8442253470420837 [Val] Loss: -0.8589795827865601\n",
      "Epoch 162 -> [Train] Loss: -0.8348707556724548 [Val] Loss: -0.8643516898155212\n",
      "Epoch 163 -> [Train] Loss: -0.8471817374229431 [Val] Loss: -0.8667162656784058\n",
      "Epoch 164 -> [Train] Loss: -0.848494827747345 [Val] Loss: -0.8676741719245911\n",
      "Epoch 165 -> [Train] Loss: -0.8498227000236511 [Val] Loss: -0.8676801323890686\n",
      "Epoch 166 -> [Train] Loss: -0.852448046207428 [Val] Loss: -0.8687673807144165\n",
      "Epoch 167 -> [Train] Loss: -0.8494036197662354 [Val] Loss: -0.867779552936554\n",
      "Epoch 168 -> [Train] Loss: -0.8516135811805725 [Val] Loss: -0.86758953332901\n",
      "Epoch 169 -> [Train] Loss: -0.8476298451423645 [Val] Loss: -0.86708664894104\n",
      "Epoch 170 -> [Train] Loss: -0.8466848731040955 [Val] Loss: -0.8681135773658752\n",
      "Epoch 171 -> [Train] Loss: -0.8515467643737793 [Val] Loss: -0.8682271242141724\n",
      "Epoch 172 -> [Train] Loss: -0.8532124161720276 [Val] Loss: -0.8693479895591736\n",
      "Epoch 173 -> [Train] Loss: -0.8535447716712952 [Val] Loss: -0.8691040873527527\n",
      "Epoch 174 -> [Train] Loss: -0.8537620306015015 [Val] Loss: -0.8686255812644958\n",
      "Epoch 175 -> [Train] Loss: -0.8499529957771301 [Val] Loss: -0.8695535063743591\n",
      "Epoch 176 -> [Train] Loss: -0.8524690270423889 [Val] Loss: -0.8704662919044495\n",
      "Epoch 177 -> [Train] Loss: -0.8521097302436829 [Val] Loss: -0.8691707253456116\n",
      "Epoch 178 -> [Train] Loss: -0.8515070676803589 [Val] Loss: -0.8702180981636047\n",
      "Epoch 179 -> [Train] Loss: -0.8478277325630188 [Val] Loss: -0.8671204447746277\n",
      "Epoch 180 -> [Train] Loss: -0.8421086668968201 [Val] Loss: -0.8655776977539062\n",
      "Epoch 181 -> [Train] Loss: -0.8432241678237915 [Val] Loss: -0.8678315281867981\n",
      "Epoch 182 -> [Train] Loss: -0.8482670187950134 [Val] Loss: -0.8675510287284851\n",
      "Epoch 183 -> [Train] Loss: -0.845825731754303 [Val] Loss: -0.8691591024398804\n",
      "Epoch 184 -> [Train] Loss: -0.849025547504425 [Val] Loss: -0.8693795204162598\n",
      "Epoch 185 -> [Train] Loss: -0.8511916995048523 [Val] Loss: -0.8705957531929016\n",
      "Epoch 186 -> [Train] Loss: -0.8542769551277161 [Val] Loss: -0.8712218403816223\n",
      "Epoch 187 -> [Train] Loss: -0.853754460811615 [Val] Loss: -0.8708420991897583\n",
      "Epoch 188 -> [Train] Loss: -0.8521045446395874 [Val] Loss: -0.8719573616981506\n",
      "Epoch 189 -> [Train] Loss: -0.8527781367301941 [Val] Loss: -0.8715018033981323\n",
      "Epoch 190 -> [Train] Loss: -0.8523423075675964 [Val] Loss: -0.873344361782074\n",
      "Epoch 191 -> [Train] Loss: -0.8558638691902161 [Val] Loss: -0.8730737566947937\n",
      "Epoch 192 -> [Train] Loss: -0.859321117401123 [Val] Loss: -0.8731328248977661\n",
      "Epoch 193 -> [Train] Loss: -0.8556201457977295 [Val] Loss: -0.8739021420478821\n",
      "Epoch 194 -> [Train] Loss: -0.8572575449943542 [Val] Loss: -0.8733406066894531\n",
      "Epoch 195 -> [Train] Loss: -0.859417200088501 [Val] Loss: -0.8733165264129639\n",
      "Epoch 196 -> [Train] Loss: -0.858161449432373 [Val] Loss: -0.8749192953109741\n",
      "Epoch 197 -> [Train] Loss: -0.8558582067489624 [Val] Loss: -0.8739340305328369\n",
      "Epoch 198 -> [Train] Loss: -0.8597960472106934 [Val] Loss: -0.8740900158882141\n",
      "Epoch 199 -> [Train] Loss: -0.8597210645675659 [Val] Loss: -0.8740108013153076\n",
      "Epoch 200 -> [Train] Loss: -0.861860990524292 [Val] Loss: -0.8744214773178101\n",
      "Epoch 201 -> [Train] Loss: -0.8585121631622314 [Val] Loss: -0.8739681243896484\n",
      "Epoch 202 -> [Train] Loss: -0.8559694290161133 [Val] Loss: -0.8737886548042297\n",
      "Epoch 203 -> [Train] Loss: -0.8612527251243591 [Val] Loss: -0.8743526339530945\n",
      "Epoch 204 -> [Train] Loss: -0.8605260252952576 [Val] Loss: -0.8749802112579346\n",
      "Epoch 205 -> [Train] Loss: -0.8584396243095398 [Val] Loss: -0.8751236796379089\n",
      "Epoch 206 -> [Train] Loss: -0.8575699925422668 [Val] Loss: -0.8746022582054138\n",
      "Epoch 207 -> [Train] Loss: -0.8624284267425537 [Val] Loss: -0.875661313533783\n",
      "Epoch 208 -> [Train] Loss: -0.8624325394630432 [Val] Loss: -0.8759503960609436\n",
      "Epoch 209 -> [Train] Loss: -0.8649987578392029 [Val] Loss: -0.8772528767585754\n",
      "Epoch 210 -> [Train] Loss: -0.8666360378265381 [Val] Loss: -0.8781309723854065\n",
      "Epoch 211 -> [Train] Loss: -0.8638812899589539 [Val] Loss: -0.8791118264198303\n",
      "Epoch 212 -> [Train] Loss: -0.8679007291793823 [Val] Loss: -0.8791382908821106\n",
      "Epoch 213 -> [Train] Loss: -0.867041289806366 [Val] Loss: -0.8789495229721069\n",
      "Epoch 214 -> [Train] Loss: -0.8660522103309631 [Val] Loss: -0.8792946934700012\n",
      "Epoch 215 -> [Train] Loss: -0.8638131022453308 [Val] Loss: -0.8798319697380066\n",
      "Epoch 216 -> [Train] Loss: -0.8633041977882385 [Val] Loss: -0.8797610402107239\n",
      "Epoch 217 -> [Train] Loss: -0.8640906810760498 [Val] Loss: -0.8811076879501343\n",
      "Epoch 218 -> [Train] Loss: -0.8679482340812683 [Val] Loss: -0.8810490965843201\n",
      "Epoch 219 -> [Train] Loss: -0.8711222410202026 [Val] Loss: -0.8811516165733337\n",
      "Epoch 220 -> [Train] Loss: -0.8684208393096924 [Val] Loss: -0.8817731142044067\n",
      "Epoch 221 -> [Train] Loss: -0.8688098788261414 [Val] Loss: -0.8817647695541382\n",
      "Epoch 222 -> [Train] Loss: -0.8712539672851562 [Val] Loss: -0.8827431201934814\n",
      "Epoch 223 -> [Train] Loss: -0.8709960579872131 [Val] Loss: -0.8830148577690125\n",
      "Epoch 224 -> [Train] Loss: -0.8712891340255737 [Val] Loss: -0.8830638527870178\n",
      "Epoch 225 -> [Train] Loss: -0.8703123331069946 [Val] Loss: -0.883611261844635\n",
      "Epoch 226 -> [Train] Loss: -0.8735818266868591 [Val] Loss: -0.8837082982063293\n",
      "Epoch 227 -> [Train] Loss: -0.8712736368179321 [Val] Loss: -0.8839484453201294\n",
      "Epoch 228 -> [Train] Loss: -0.8736016750335693 [Val] Loss: -0.884377658367157\n",
      "Epoch 229 -> [Train] Loss: -0.8739524483680725 [Val] Loss: -0.8851359486579895\n",
      "Epoch 230 -> [Train] Loss: -0.8707305788993835 [Val] Loss: -0.8841794729232788\n",
      "Epoch 231 -> [Train] Loss: -0.8728011250495911 [Val] Loss: -0.8848788142204285\n",
      "Epoch 232 -> [Train] Loss: -0.8735129237174988 [Val] Loss: -0.8851003050804138\n",
      "Epoch 233 -> [Train] Loss: -0.8705164194107056 [Val] Loss: -0.8841020464897156\n",
      "Epoch 234 -> [Train] Loss: -0.8739548325538635 [Val] Loss: -0.8857224583625793\n",
      "Epoch 235 -> [Train] Loss: -0.8770668506622314 [Val] Loss: -0.8857583403587341\n",
      "Epoch 236 -> [Train] Loss: -0.8778048753738403 [Val] Loss: -0.8859200477600098\n",
      "Epoch 237 -> [Train] Loss: -0.8766652345657349 [Val] Loss: -0.8859412670135498\n",
      "Epoch 238 -> [Train] Loss: -0.874673068523407 [Val] Loss: -0.8862155675888062\n",
      "Epoch 239 -> [Train] Loss: -0.875262975692749 [Val] Loss: -0.8862388730049133\n",
      "Epoch 240 -> [Train] Loss: -0.8720963597297668 [Val] Loss: -0.8862761855125427\n",
      "Epoch 241 -> [Train] Loss: -0.8779555559158325 [Val] Loss: -0.8864139914512634\n",
      "Epoch 242 -> [Train] Loss: -0.8758583664894104 [Val] Loss: -0.8869335055351257\n",
      "Epoch 243 -> [Train] Loss: -0.8789371252059937 [Val] Loss: -0.8865364193916321\n",
      "Epoch 244 -> [Train] Loss: -0.8769757747650146 [Val] Loss: -0.8867299556732178\n",
      "Epoch 245 -> [Train] Loss: -0.8790509104728699 [Val] Loss: -0.8866596817970276\n",
      "Epoch 246 -> [Train] Loss: -0.877275288105011 [Val] Loss: -0.8871559500694275\n",
      "Epoch 247 -> [Train] Loss: -0.8778103590011597 [Val] Loss: -0.8877286314964294\n",
      "Epoch 248 -> [Train] Loss: -0.8771950602531433 [Val] Loss: -0.8874050974845886\n",
      "Epoch 249 -> [Train] Loss: -0.8783428072929382 [Val] Loss: -0.8871830701828003\n",
      "Epoch 250 -> [Train] Loss: -0.8782654404640198 [Val] Loss: -0.8877072930335999\n",
      "Epoch 251 -> [Train] Loss: -0.8794875144958496 [Val] Loss: -0.8880199193954468\n",
      "Epoch 252 -> [Train] Loss: -0.8789419531822205 [Val] Loss: -0.8879802227020264\n",
      "Epoch 253 -> [Train] Loss: -0.8777080178260803 [Val] Loss: -0.8874952793121338\n",
      "Epoch 254 -> [Train] Loss: -0.8744155168533325 [Val] Loss: -0.8882420063018799\n",
      "Epoch 255 -> [Train] Loss: -0.8794283866882324 [Val] Loss: -0.8877027034759521\n",
      "Epoch 256 -> [Train] Loss: -0.8801988959312439 [Val] Loss: -0.8883880972862244\n",
      "Epoch 257 -> [Train] Loss: -0.8794196844100952 [Val] Loss: -0.8871186971664429\n",
      "Epoch 258 -> [Train] Loss: -0.8787723183631897 [Val] Loss: -0.8882443904876709\n",
      "Epoch 259 -> [Train] Loss: -0.8813348412513733 [Val] Loss: -0.8883498311042786\n",
      "Epoch 260 -> [Train] Loss: -0.8795698881149292 [Val] Loss: -0.8881976008415222\n",
      "Epoch 261 -> [Train] Loss: -0.8800054788589478 [Val] Loss: -0.8885281085968018\n",
      "Epoch 262 -> [Train] Loss: -0.8805417418479919 [Val] Loss: -0.8878309726715088\n",
      "Epoch 263 -> [Train] Loss: -0.88319331407547 [Val] Loss: -0.8883556127548218\n",
      "Epoch 264 -> [Train] Loss: -0.879669725894928 [Val] Loss: -0.8879799842834473\n",
      "Epoch 265 -> [Train] Loss: -0.880931556224823 [Val] Loss: -0.8884130120277405\n",
      "Epoch 266 -> [Train] Loss: -0.8796494603157043 [Val] Loss: -0.888599157333374\n",
      "Epoch 267 -> [Train] Loss: -0.8785637617111206 [Val] Loss: -0.8878269791603088\n",
      "Epoch 268 -> [Train] Loss: -0.8813384771347046 [Val] Loss: -0.888364851474762\n",
      "Epoch 269 -> [Train] Loss: -0.8813603520393372 [Val] Loss: -0.889035165309906\n",
      "Epoch 270 -> [Train] Loss: -0.8781745433807373 [Val] Loss: -0.8886509537696838\n",
      "Epoch 271 -> [Train] Loss: -0.8826455473899841 [Val] Loss: -0.888370931148529\n",
      "Epoch 272 -> [Train] Loss: -0.8772532939910889 [Val] Loss: -0.8884215950965881\n",
      "Epoch 273 -> [Train] Loss: -0.8792722821235657 [Val] Loss: -0.8888905048370361\n",
      "Epoch 274 -> [Train] Loss: -0.8810015916824341 [Val] Loss: -0.8892079591751099\n",
      "Epoch 275 -> [Train] Loss: -0.881615936756134 [Val] Loss: -0.8890205025672913\n",
      "Epoch 276 -> [Train] Loss: -0.8822672963142395 [Val] Loss: -0.8890474438667297\n",
      "Epoch 277 -> [Train] Loss: -0.8785550594329834 [Val] Loss: -0.8885772228240967\n",
      "Epoch 278 -> [Train] Loss: -0.8810231685638428 [Val] Loss: -0.8888401389122009\n",
      "Epoch 279 -> [Train] Loss: -0.8823196291923523 [Val] Loss: -0.888770580291748\n",
      "Epoch 280 -> [Train] Loss: -0.8846315741539001 [Val] Loss: -0.8888605833053589\n",
      "Epoch 281 -> [Train] Loss: -0.8792588710784912 [Val] Loss: -0.8891658782958984\n",
      "Epoch 282 -> [Train] Loss: -0.8794377446174622 [Val] Loss: -0.8885872960090637\n",
      "Epoch 283 -> [Train] Loss: -0.8840159177780151 [Val] Loss: -0.889099657535553\n",
      "Epoch 284 -> [Train] Loss: -0.8797115683555603 [Val] Loss: -0.8898890614509583\n",
      "Epoch 285 -> [Train] Loss: -0.8822499513626099 [Val] Loss: -0.8887362480163574\n",
      "Epoch 286 -> [Train] Loss: -0.8830043077468872 [Val] Loss: -0.8893698453903198\n",
      "Epoch 287 -> [Train] Loss: -0.8847912549972534 [Val] Loss: -0.8892757296562195\n",
      "Epoch 288 -> [Train] Loss: -0.8834258317947388 [Val] Loss: -0.8887497186660767\n",
      "Epoch 289 -> [Train] Loss: -0.8795608282089233 [Val] Loss: -0.8887280821800232\n",
      "Epoch 290 -> [Train] Loss: -0.8844792246818542 [Val] Loss: -0.8892477750778198\n",
      "Epoch 291 -> [Train] Loss: -0.8826749920845032 [Val] Loss: -0.8895523548126221\n",
      "Epoch 292 -> [Train] Loss: -0.8813307285308838 [Val] Loss: -0.8889195322990417\n",
      "Epoch 293 -> [Train] Loss: -0.8751816153526306 [Val] Loss: -0.8881516456604004\n",
      "Epoch 294 -> [Train] Loss: -0.8824541568756104 [Val] Loss: -0.8889434337615967\n",
      "Epoch 295 -> [Train] Loss: -0.8877820372581482 [Val] Loss: -0.8884339332580566\n",
      "Epoch 296 -> [Train] Loss: -0.8855714797973633 [Val] Loss: -0.8889076709747314\n",
      "Epoch 297 -> [Train] Loss: -0.8813514709472656 [Val] Loss: -0.8889818787574768\n",
      "Epoch 298 -> [Train] Loss: -0.8827611207962036 [Val] Loss: -0.8895882964134216\n",
      "Epoch 299 -> [Train] Loss: -0.8807892203330994 [Val] Loss: -0.8890087604522705\n",
      "Epoch 300 -> [Train] Loss: -0.8845528364181519 [Val] Loss: -0.889578104019165\n",
      "Epoch 301 -> [Train] Loss: -0.8873915076255798 [Val] Loss: -0.8899453282356262\n",
      "Epoch 302 -> [Train] Loss: -0.8827581405639648 [Val] Loss: -0.889031171798706\n",
      "Epoch 303 -> [Train] Loss: -0.8849392533302307 [Val] Loss: -0.8898193836212158\n",
      "Epoch 304 -> [Train] Loss: -0.8845517635345459 [Val] Loss: -0.8895944356918335\n",
      "Epoch 305 -> [Train] Loss: -0.8872705101966858 [Val] Loss: -0.8893893957138062\n",
      "Epoch 306 -> [Train] Loss: -0.8559909462928772 [Val] Loss: -0.8626145720481873\n",
      "Epoch 307 -> [Train] Loss: -0.8541733622550964 [Val] Loss: -0.8707519173622131\n",
      "Epoch 308 -> [Train] Loss: -0.8626143932342529 [Val] Loss: -0.8737621307373047\n",
      "Epoch 309 -> [Train] Loss: -0.8613338470458984 [Val] Loss: -0.876180112361908\n",
      "Epoch 310 -> [Train] Loss: -0.8661532402038574 [Val] Loss: -0.8774482011795044\n",
      "Epoch 311 -> [Train] Loss: -0.8682178854942322 [Val] Loss: -0.8777796030044556\n",
      "Epoch 312 -> [Train] Loss: -0.8720037937164307 [Val] Loss: -0.8785496354103088\n",
      "Epoch 313 -> [Train] Loss: -0.8700448274612427 [Val] Loss: -0.8792521953582764\n",
      "Epoch 314 -> [Train] Loss: -0.8688373565673828 [Val] Loss: -0.8798085451126099\n",
      "Epoch 315 -> [Train] Loss: -0.8723627328872681 [Val] Loss: -0.8804652094841003\n",
      "Epoch 316 -> [Train] Loss: -0.8712927103042603 [Val] Loss: -0.881270706653595\n",
      "Epoch 317 -> [Train] Loss: -0.873153805732727 [Val] Loss: -0.8828174471855164\n",
      "Epoch 318 -> [Train] Loss: -0.8783940076828003 [Val] Loss: -0.8845605254173279\n",
      "Epoch 319 -> [Train] Loss: -0.8765683770179749 [Val] Loss: -0.8855997323989868\n",
      "Epoch 320 -> [Train] Loss: -0.8790802359580994 [Val] Loss: -0.886459469795227\n",
      "Epoch 321 -> [Train] Loss: -0.8821194171905518 [Val] Loss: -0.8872550129890442\n",
      "Epoch 322 -> [Train] Loss: -0.8830995559692383 [Val] Loss: -0.8879894018173218\n",
      "Epoch 323 -> [Train] Loss: -0.8809035420417786 [Val] Loss: -0.8883347511291504\n",
      "Epoch 324 -> [Train] Loss: -0.8858853578567505 [Val] Loss: -0.888891875743866\n",
      "Epoch 325 -> [Train] Loss: -0.8822048902511597 [Val] Loss: -0.8876157999038696\n",
      "Epoch 326 -> [Train] Loss: -0.8851220607757568 [Val] Loss: -0.8893104791641235\n",
      "Epoch 327 -> [Train] Loss: -0.883633553981781 [Val] Loss: -0.8880769610404968\n",
      "Epoch 328 -> [Train] Loss: -0.880224883556366 [Val] Loss: -0.889380693435669\n",
      "Epoch 329 -> [Train] Loss: -0.8860954642295837 [Val] Loss: -0.8895477652549744\n",
      "Epoch 330 -> [Train] Loss: -0.8825701475143433 [Val] Loss: -0.8888962864875793\n",
      "Epoch 331 -> [Train] Loss: -0.8863092660903931 [Val] Loss: -0.8904298543930054\n",
      "Epoch 332 -> [Train] Loss: -0.8862125277519226 [Val] Loss: -0.8893998265266418\n",
      "Epoch 333 -> [Train] Loss: -0.8864659667015076 [Val] Loss: -0.8900026082992554\n",
      "Epoch 334 -> [Train] Loss: -0.888146698474884 [Val] Loss: -0.8895785212516785\n",
      "Epoch 335 -> [Train] Loss: -0.8838192820549011 [Val] Loss: -0.889659583568573\n",
      "Epoch 336 -> [Train] Loss: -0.8847231268882751 [Val] Loss: -0.8903505206108093\n",
      "Epoch 337 -> [Train] Loss: -0.8846918940544128 [Val] Loss: -0.8898613452911377\n",
      "Epoch 338 -> [Train] Loss: -0.8820248246192932 [Val] Loss: -0.890150785446167\n",
      "Epoch 339 -> [Train] Loss: -0.8848540782928467 [Val] Loss: -0.889923095703125\n",
      "Epoch 340 -> [Train] Loss: -0.8863103985786438 [Val] Loss: -0.8902317881584167\n",
      "Epoch 341 -> [Train] Loss: -0.8863899111747742 [Val] Loss: -0.8895462155342102\n",
      "Epoch 342 -> [Train] Loss: -0.864315390586853 [Val] Loss: -0.8781182169914246\n",
      "Epoch 343 -> [Train] Loss: -0.8780354261398315 [Val] Loss: -0.8887823820114136\n",
      "Epoch 344 -> [Train] Loss: -0.8846054077148438 [Val] Loss: -0.8896667957305908\n",
      "Epoch 345 -> [Train] Loss: -0.887516438961029 [Val] Loss: -0.890165388584137\n",
      "Epoch 346 -> [Train] Loss: -0.8859959244728088 [Val] Loss: -0.8899669647216797\n",
      "Epoch 347 -> [Train] Loss: -0.8864599466323853 [Val] Loss: -0.8902904987335205\n",
      "Epoch 348 -> [Train] Loss: -0.8831138610839844 [Val] Loss: -0.8773263692855835\n",
      "Epoch 349 -> [Train] Loss: -0.8840548992156982 [Val] Loss: -0.890109121799469\n",
      "Epoch 350 -> [Train] Loss: -0.887747049331665 [Val] Loss: -0.889380931854248\n",
      "Epoch 351 -> [Train] Loss: -0.8696231842041016 [Val] Loss: -0.8740851879119873\n",
      "Epoch 352 -> [Train] Loss: -0.883392333984375 [Val] Loss: -0.8890629410743713\n",
      "Epoch 353 -> [Train] Loss: -0.8624086380004883 [Val] Loss: -0.8721157908439636\n",
      "Epoch 354 -> [Train] Loss: -0.864661455154419 [Val] Loss: -0.8756916522979736\n",
      "Epoch 355 -> [Train] Loss: -0.868657648563385 [Val] Loss: -0.8781874775886536\n",
      "Epoch 356 -> [Train] Loss: -0.8801381587982178 [Val] Loss: -0.886975109577179\n",
      "Epoch 357 -> [Train] Loss: -0.8856000900268555 [Val] Loss: -0.8895329833030701\n",
      "Epoch 358 -> [Train] Loss: -0.8871528506278992 [Val] Loss: -0.8900347352027893\n",
      "Epoch 359 -> [Train] Loss: -0.8858240246772766 [Val] Loss: -0.8902684450149536\n",
      "Epoch 360 -> [Train] Loss: -0.8853566646575928 [Val] Loss: -0.8901069164276123\n",
      "Epoch 361 -> [Train] Loss: -0.8895047307014465 [Val] Loss: -0.8905466794967651\n",
      "Epoch 362 -> [Train] Loss: -0.8887863159179688 [Val] Loss: -0.890047550201416\n",
      "Epoch 363 -> [Train] Loss: -0.8854864239692688 [Val] Loss: -0.8903942704200745\n",
      "Epoch 364 -> [Train] Loss: -0.867066502571106 [Val] Loss: -0.8738532662391663\n",
      "Epoch 365 -> [Train] Loss: -0.8684900999069214 [Val] Loss: -0.8818515539169312\n",
      "Epoch 366 -> [Train] Loss: -0.8691058158874512 [Val] Loss: -0.8765808939933777\n",
      "Epoch 367 -> [Train] Loss: -0.8681480288505554 [Val] Loss: -0.8758034706115723\n",
      "Epoch 368 -> [Train] Loss: -0.8708915710449219 [Val] Loss: -0.8763883113861084\n",
      "Epoch 369 -> [Train] Loss: -0.8671951293945312 [Val] Loss: -0.8771817684173584\n",
      "Epoch 370 -> [Train] Loss: -0.8701456189155579 [Val] Loss: -0.8795469999313354\n",
      "Epoch 371 -> [Train] Loss: -0.8730332255363464 [Val] Loss: -0.8802604079246521\n",
      "Epoch 372 -> [Train] Loss: -0.8753799200057983 [Val] Loss: -0.8828943967819214\n",
      "Epoch 373 -> [Train] Loss: -0.8792011737823486 [Val] Loss: -0.885869562625885\n",
      "Epoch 374 -> [Train] Loss: -0.8823941349983215 [Val] Loss: -0.8865581154823303\n",
      "Epoch 375 -> [Train] Loss: -0.8835283517837524 [Val] Loss: -0.8874408602714539\n",
      "Epoch 376 -> [Train] Loss: -0.8844054341316223 [Val] Loss: -0.8880728483200073\n",
      "Epoch 377 -> [Train] Loss: -0.8844269514083862 [Val] Loss: -0.888701319694519\n",
      "Epoch 378 -> [Train] Loss: -0.8838908076286316 [Val] Loss: -0.8891709446907043\n",
      "Epoch 379 -> [Train] Loss: -0.8836439847946167 [Val] Loss: -0.8891239166259766\n",
      "Epoch 380 -> [Train] Loss: -0.8882016539573669 [Val] Loss: -0.8897993564605713\n",
      "Epoch 381 -> [Train] Loss: -0.887642502784729 [Val] Loss: -0.8904255628585815\n",
      "Epoch 382 -> [Train] Loss: -0.8866217136383057 [Val] Loss: -0.8901013135910034\n",
      "Epoch 383 -> [Train] Loss: -0.8877307176589966 [Val] Loss: -0.8899447917938232\n",
      "Epoch 384 -> [Train] Loss: -0.8854349851608276 [Val] Loss: -0.8895034790039062\n",
      "Epoch 385 -> [Train] Loss: -0.8864365816116333 [Val] Loss: -0.8906526565551758\n",
      "Epoch 386 -> [Train] Loss: -0.8885721564292908 [Val] Loss: -0.890614926815033\n",
      "Epoch 387 -> [Train] Loss: -0.8899612426757812 [Val] Loss: -0.89112788438797\n",
      "Epoch 388 -> [Train] Loss: -0.8906933665275574 [Val] Loss: -0.8907131552696228\n",
      "Epoch 389 -> [Train] Loss: -0.8875882625579834 [Val] Loss: -0.8910303711891174\n",
      "Epoch 390 -> [Train] Loss: -0.8846350908279419 [Val] Loss: -0.8908545970916748\n",
      "Epoch 391 -> [Train] Loss: -0.8917847275733948 [Val] Loss: -0.8906567096710205\n",
      "Epoch 392 -> [Train] Loss: -0.8652334213256836 [Val] Loss: -0.8661187887191772\n",
      "Epoch 393 -> [Train] Loss: -0.8535222411155701 [Val] Loss: -0.8660944700241089\n",
      "Epoch 394 -> [Train] Loss: -0.8593703508377075 [Val] Loss: -0.8690639138221741\n",
      "Epoch 395 -> [Train] Loss: -0.8621342778205872 [Val] Loss: -0.8721165060997009\n",
      "Epoch 396 -> [Train] Loss: -0.8666744232177734 [Val] Loss: -0.8740406632423401\n",
      "Epoch 397 -> [Train] Loss: -0.8702488541603088 [Val] Loss: -0.8753277659416199\n",
      "Epoch 398 -> [Train] Loss: -0.8714034557342529 [Val] Loss: -0.8766266703605652\n",
      "Epoch 399 -> [Train] Loss: -0.8724981546401978 [Val] Loss: -0.8787403702735901\n",
      "Epoch 400 -> [Train] Loss: -0.8778972625732422 [Val] Loss: -0.8879068493843079\n",
      "Epoch 401 -> [Train] Loss: -0.8856015801429749 [Val] Loss: -0.8893618583679199\n",
      "Epoch 402 -> [Train] Loss: -0.8852559328079224 [Val] Loss: -0.8895387053489685\n",
      "Epoch 403 -> [Train] Loss: -0.885762095451355 [Val] Loss: -0.8903630971908569\n",
      "Epoch 404 -> [Train] Loss: -0.8871277570724487 [Val] Loss: -0.8903263807296753\n",
      "Epoch 405 -> [Train] Loss: -0.8897397518157959 [Val] Loss: -0.890668511390686\n",
      "Epoch 406 -> [Train] Loss: -0.8912215232849121 [Val] Loss: -0.8911600708961487\n",
      "Epoch 407 -> [Train] Loss: -0.8902932405471802 [Val] Loss: -0.891467809677124\n",
      "Epoch 408 -> [Train] Loss: -0.8901392221450806 [Val] Loss: -0.8912191987037659\n",
      "Epoch 409 -> [Train] Loss: -0.8900833129882812 [Val] Loss: -0.8912264704704285\n",
      "Epoch 410 -> [Train] Loss: -0.8916367292404175 [Val] Loss: -0.8914703726768494\n",
      "Epoch 411 -> [Train] Loss: -0.8897110819816589 [Val] Loss: -0.8920172452926636\n",
      "Epoch 412 -> [Train] Loss: -0.8895959854125977 [Val] Loss: -0.8911659717559814\n",
      "Epoch 413 -> [Train] Loss: -0.8928661346435547 [Val] Loss: -0.891285240650177\n",
      "Epoch 414 -> [Train] Loss: -0.8906258940696716 [Val] Loss: -0.8916507363319397\n",
      "Epoch 415 -> [Train] Loss: -0.8837234377861023 [Val] Loss: -0.8910186290740967\n",
      "Epoch 416 -> [Train] Loss: -0.863969087600708 [Val] Loss: -0.8680645823478699\n",
      "Epoch 417 -> [Train] Loss: -0.8538658022880554 [Val] Loss: -0.8690270781517029\n",
      "Epoch 418 -> [Train] Loss: -0.8668749332427979 [Val] Loss: -0.8746936321258545\n",
      "Epoch 419 -> [Train] Loss: -0.8730096220970154 [Val] Loss: -0.8763964772224426\n",
      "Epoch 420 -> [Train] Loss: -0.8703053593635559 [Val] Loss: -0.8781595826148987\n",
      "Epoch 421 -> [Train] Loss: -0.8747806549072266 [Val] Loss: -0.8801437616348267\n",
      "Epoch 422 -> [Train] Loss: -0.8705809712409973 [Val] Loss: -0.8813990354537964\n",
      "Epoch 423 -> [Train] Loss: -0.8671619892120361 [Val] Loss: -0.8763905167579651\n",
      "Epoch 424 -> [Train] Loss: -0.8703124523162842 [Val] Loss: -0.8777732253074646\n",
      "Epoch 425 -> [Train] Loss: -0.8685463666915894 [Val] Loss: -0.8788979053497314\n",
      "Epoch 426 -> [Train] Loss: -0.8729753494262695 [Val] Loss: -0.8797164559364319\n",
      "Epoch 427 -> [Train] Loss: -0.8743184208869934 [Val] Loss: -0.8801205158233643\n",
      "Epoch 428 -> [Train] Loss: -0.8765348792076111 [Val] Loss: -0.8807631731033325\n",
      "Epoch 429 -> [Train] Loss: -0.8751780986785889 [Val] Loss: -0.8810936808586121\n",
      "Epoch 430 -> [Train] Loss: -0.8753178715705872 [Val] Loss: -0.8813623189926147\n",
      "Epoch 431 -> [Train] Loss: -0.875751793384552 [Val] Loss: -0.8817616105079651\n",
      "Epoch 432 -> [Train] Loss: -0.8758126497268677 [Val] Loss: -0.8818039894104004\n",
      "Epoch 433 -> [Train] Loss: -0.8767898082733154 [Val] Loss: -0.881865382194519\n",
      "Epoch 434 -> [Train] Loss: -0.8777392506599426 [Val] Loss: -0.8822147250175476\n",
      "Epoch 435 -> [Train] Loss: -0.8767787218093872 [Val] Loss: -0.8826175332069397\n",
      "Epoch 436 -> [Train] Loss: -0.8756117820739746 [Val] Loss: -0.8831934928894043\n",
      "Epoch 437 -> [Train] Loss: -0.8782774209976196 [Val] Loss: -0.8837453126907349\n",
      "Epoch 438 -> [Train] Loss: -0.8816174268722534 [Val] Loss: -0.8855610489845276\n",
      "Epoch 439 -> [Train] Loss: -0.8852054476737976 [Val] Loss: -0.8911367058753967\n",
      "Epoch 440 -> [Train] Loss: -0.8843307495117188 [Val] Loss: -0.8903239965438843\n",
      "Epoch 441 -> [Train] Loss: -0.886353075504303 [Val] Loss: -0.8906068801879883\n",
      "Epoch 442 -> [Train] Loss: -0.8893476128578186 [Val] Loss: -0.8905215859413147\n",
      "Epoch 443 -> [Train] Loss: -0.8904024958610535 [Val] Loss: -0.8902801871299744\n",
      "Epoch 444 -> [Train] Loss: -0.8909178972244263 [Val] Loss: -0.8910443186759949\n",
      "Epoch 445 -> [Train] Loss: -0.8878867030143738 [Val] Loss: -0.8917962908744812\n",
      "Epoch 446 -> [Train] Loss: -0.8893067240715027 [Val] Loss: -0.8921085000038147\n",
      "Epoch 447 -> [Train] Loss: -0.8939686417579651 [Val] Loss: -0.8919659852981567\n",
      "Epoch 448 -> [Train] Loss: -0.8906108736991882 [Val] Loss: -0.8919028639793396\n",
      "Epoch 449 -> [Train] Loss: -0.890150785446167 [Val] Loss: -0.8919982314109802\n",
      "Epoch 450 -> [Train] Loss: -0.8924647569656372 [Val] Loss: -0.8922923803329468\n",
      "Epoch 451 -> [Train] Loss: -0.8958482146263123 [Val] Loss: -0.8916140198707581\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(config.EPOCHS):\n",
    "    ## Training\n",
    "    for batch in dst_train_rdy.as_numpy_iterator():\n",
    "        state = train_step(state, batch)\n",
    "        state = state.replace(params=clip_layer(state.params, \"GDN\", a_min=0))\n",
    "        # state = compute_metrics(state=state, batch=batch)\n",
    "        # break\n",
    "\n",
    "    ## Log the metrics\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"train_{name}\"].append(value)\n",
    "    \n",
    "    ## Empty the metrics\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "    ## Evaluation\n",
    "    for batch in dst_val_rdy.as_numpy_iterator():\n",
    "        state = compute_metrics(state=state, batch=batch)\n",
    "        # break\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"val_{name}\"].append(value)\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "    \n",
    "    ## Checkpointing\n",
    "    if metrics_history[\"val_loss\"][-1] <= min(metrics_history[\"val_loss\"]):\n",
    "        orbax_checkpointer.save(os.path.join(wandb.run.dir, \"model-best\"), state, save_args=save_args, force=True) # force=True means allow overwritting.\n",
    "\n",
    "    wandb.log({f\"{k}\": wandb.Histogram(v) for k, v in flatten_params(state.params).items()}, commit=False)\n",
    "    wandb.log({\"epoch\": epoch+1, **{name:values[-1] for name, values in metrics_history.items()}})\n",
    "    print(f'Epoch {epoch} -> [Train] Loss: {metrics_history[\"train_loss\"][-1]} [Val] Loss: {metrics_history[\"val_loss\"][-1]}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the final model as well in case we want to keep training from it or whatever:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbax_checkpointer.save(os.path.join(wandb.run.dir, \"model-final\"), state, save_args=save_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ce431dd29940ffaa582ece1e6771a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.454 MB of 2.259 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.200902…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>452</td></tr><tr><td>train_loss</td><td>-0.89585</td></tr><tr><td>trainable_parameters</td><td>724</td></tr><tr><td>val_loss</td><td>-0.89161</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">V2_Init</strong> at: <a href='https://wandb.ai/jorgvt/PerceptNet_JaX/runs/4t8vdwbp' target=\"_blank\">https://wandb.ai/jorgvt/PerceptNet_JaX/runs/4t8vdwbp</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231031_164951-4t8vdwbp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ0AAASXCAYAAABLKw4hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfrElEQVR4nO3YwQkAIBDAMHX/nc8lCoIkE/TdPTOzAAAAACB0XgcAAAAA8B/TCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMiZTgAAAADkTCcAAAAAcqYTAAAAADnTCQAAAICc6QQAAABAznQCAAAAIGc6AQAAAJAznQAAAADImU4AAAAA5EwnAAAAAHKmEwAAAAA50wkAAACAnOkEAAAAQM50AgAAACBnOgEAAACQM50AAAAAyJlOAAAAAORMJwAAAAByphMAAAAAOdMJAAAAgJzpBAAAAEDOdAIAAAAgZzoBAAAAkDOdAAAAAMhdcqoNKrWyqIgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x1500 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(8,8, figsize=(15,15))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(state.state[\"precalc_filter\"][\"GaborLayerLogSigma__0\"][\"kernel\"][:,:,0,i])\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
