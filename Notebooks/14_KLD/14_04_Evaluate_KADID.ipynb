{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 11:19:17.434391: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-23 11:19:17.473167: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-23 11:19:17.473718: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-23 11:19:18.061294: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-23 11:19:18.878421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-23 11:19:18.878742: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from typing import Any, Callable, Sequence, Union\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "import jax\n",
    "from jax import lax, random, numpy as jnp\n",
    "import flax\n",
    "from flax.core import freeze, unfreeze, FrozenDict\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "from flax.training import orbax_utils\n",
    "\n",
    "import optax\n",
    "import orbax.checkpoint\n",
    "\n",
    "from clu import metrics\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "from einops import reduce, rearrange\n",
    "import wandb\n",
    "\n",
    "from iqadatasets.datasets import *\n",
    "from fxlayers.layers import *\n",
    "from fxlayers.layers import GaborLayerLogSigma_, GaussianLayerGamma, FreqGaussianGamma, OrientGaussianGamma\n",
    "from fxlayers.initializers import *\n",
    "from JaxPlayground.utils.constraints import *\n",
    "from JaxPlayground.utils.wandb import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst_train = TID2008(\"/lustre/ific.uv.es/ml/uv075/Databases/IQA//TID/TID2008/\", exclude_imgs=[25])\n",
    "# dst_val = TID2013(\"/lustre/ific.uv.es/ml/uv075/Databases/IQA//TID/TID2013/\", exclude_imgs=[25])\n",
    "# dst_train = TID2008(\"/media/disk/databases/BBDD_video_image/Image_Quality//TID/TID2008/\", exclude_imgs=[25])\n",
    "# dst_val = TID2013(\"/media/disk/databases/BBDD_video_image/Image_Quality//TID/TID2013/\", exclude_imgs=[25])\n",
    "# dst = KADIK10K(\"/media/disk/databases/BBDD_video_image/Image_Quality/KADIK10K/\")\n",
    "# dst = PIPAL(\"/media/disk/databases/BBDD_video_image/Image_Quality/PIPAL/\")\n",
    "# dst = KADIK10K(\"/lustre/ific.uv.es/ml/uv075/Databases/IQA/KADIK10K/\")\n",
    "dst = KADIK10K(\"/media/databases/IQA/KADIK10K/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([384, 512, 3]), TensorShape([384, 512, 3]), TensorShape([]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, img_dist, mos = next(iter(dst.dataset))\n",
    "img.shape, img_dist.shape, mos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = \"7zqnh75q\" #Â VerySimple (KLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "prev_run = api.run(f\"jorgvt/PerceptNet_KLD/{id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BATCH_SIZE: 32\n",
       "EPOCHS: 30\n",
       "LEARNING_RATE: 0.0003\n",
       "SEED: 42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConfigDict(prev_run.config[\"_fields\"])\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in prev_run.files():\n",
    "    file.download(root=prev_run.dir, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjorgvt\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jorge/perceptnet/Notebooks/14_KLD/wandb/run-20240423_112017-ept3p1pg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jorgvt/PerceptNet_KLD/runs/ept3p1pg' target=\"_blank\">VerySimple</a></strong> to <a href='https://wandb.ai/jorgvt/PerceptNet_KLD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jorgvt/PerceptNet_KLD' target=\"_blank\">https://wandb.ai/jorgvt/PerceptNet_KLD</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jorgvt/PerceptNet_KLD/runs/ept3p1pg' target=\"_blank\">https://wandb.ai/jorgvt/PerceptNet_KLD/runs/ept3p1pg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "BATCH_SIZE: 32\n",
       "EPOCHS: 30\n",
       "LEARNING_RATE: 0.0003\n",
       "SEED: 42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"PerceptNet_KLD\",\n",
    "           name=prev_run.name,\n",
    "           job_type=\"evaluate\",\n",
    "           mode=\"online\",\n",
    "           )\n",
    "config = config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(img, img_dist, mos):\n",
    "    h, w = 384, 512\n",
    "    img = tf.image.resize(img, (h//8, w//8))\n",
    "    img_dist = tf.image.resize(img_dist, (h//8, w//8))\n",
    "    return img, img_dist, mos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_rdy = dst.dataset.batch(config.BATCH_SIZE, num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                     .map(resize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model we're going to use\n",
    "\n",
    "> It's going to be a very simple model just for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PerceptNet(nn.Module):\n",
    "#     \"\"\"IQA model inspired by the visual system.\"\"\"\n",
    "\n",
    "#     @nn.compact\n",
    "#     def __call__(self,\n",
    "#                  inputs,\n",
    "#                  **kwargs,\n",
    "#                  ):\n",
    "#         outputs = rearrange(inputs, \"b h w c -> b (h w c)\")\n",
    "#         if args[\"kld\"]:\n",
    "#             mean = nn.Dense(features=2)(outputs)\n",
    "#             std = nn.Dense(features=2)(outputs)\n",
    "#             return mean, std\n",
    "#         elif args[\"mse\"]:\n",
    "#             return nn.Dense(features=2)(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptNet(nn.Module):\n",
    "    \"\"\"IQA model inspired by the visual system.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 **kwargs,\n",
    "                 ):\n",
    "        outputs = rearrange(inputs, \"b h w c -> b (h w c)\")\n",
    "        mean = nn.Dense(features=2)(outputs)\n",
    "        std = nn.Dense(features=2)(outputs)\n",
    "        return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the metrics with `clu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n",
    "    loss: metrics.Average.from_output(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `TrainState` doesn't include metrics, but it's very easy to subclass it so that it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    state: FrozenDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function that initializes the `TrainState` from a module, a rng key and some optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(module, key, tx, input_shape):\n",
    "    \"\"\"Creates the initial `TrainState`.\"\"\"\n",
    "    variables = module.init(key, jnp.ones(input_shape))\n",
    "    state, params = variables.pop('params')\n",
    "    return TrainState.create(\n",
    "        apply_fn=module.apply,\n",
    "        params=params,\n",
    "        state=state,\n",
    "        tx=tx,\n",
    "        metrics=Metrics.empty()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kld(mean_p, std_p, mean_q, std_q, axis=(1)):\n",
    "    \"\"\"Assume diagonal covariance matrix and that the input is the logvariance.\"\"\"\n",
    "    std_p, std_q = jnp.exp(std_p), jnp.exp(std_q)\n",
    "    def safe_div(a, b): return a/b #jnp.where(a == b, 1, a/b)\n",
    "    det_p = jnp.prod(std_p, axis=axis) + 1e-5\n",
    "    det_q = jnp.prod(std_q, axis=axis) + 1e-5\n",
    "    \n",
    "    return jnp.log(safe_div(det_p, det_q)) + jnp.sum((1/std_q)*(mean_p - mean_q)**2, axis=axis) + jnp.sum(std_p/std_q, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def compute_distance(*, state, batch):\n",
    "    \"\"\"Obtaining the metrics for a given batch.\"\"\"\n",
    "    img, img_dist, mos = batch\n",
    "    \n",
    "    (img_mean, img_std), updated_state = state.apply_fn({\"params\": state.params, **state.state}, img, mutable=list(state.state.keys()), train=False)\n",
    "    (img_dist_mean, img_dist_std), updated_state = state.apply_fn({\"params\": state.params, **state.state}, img_dist, mutable=list(state.state.keys()), train=False)\n",
    "\n",
    "    ##Â Calculate the KLD\n",
    "    dist = kld(img_mean, img_std, img_dist_mean, img_dist_std)\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pretrained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 11:22:05.802294: W external/xla/xla/service/gpu/nvptx_compiler.cc:596] The NVIDIA driver's CUDA version is 12.2 which is older than the ptxas CUDA version (12.3.52). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "state = create_train_state(PerceptNet(), random.PRNGKey(config.SEED), optax.adam(config.LEARNING_RATE), input_shape=(1,384,512,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_trainable(path):\n",
    "    return False\n",
    "    # return (\"A\" in path) or (\"alpha_achrom\" in path) or (\"alpha_chrom_rg\" in path) or (\"alpha_chrom_yb\" in path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_tree = freeze(flax.traverse_util.path_aware_map(lambda path, v: \"non_trainable\" if check_trainable(path)  else \"trainable\", state.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2359300, 2359300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(state.params))\n",
    "trainable_param_count = sum([w.size if t==\"trainable\" else 0 for w, t in zip(jax.tree_util.tree_leaves(state.params), jax.tree_util.tree_leaves(trainable_tree))])\n",
    "param_count, trainable_param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.summary[\"total_parameters\"] = param_count\n",
    "wandb.run.summary[\"trainable_parameters\"] = trainable_param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {\n",
    "    \"trainable\": optax.adam(learning_rate=config.LEARNING_RATE),\n",
    "    \"non_trainable\": optax.set_to_zero(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = optax.multi_transform(optimizers, trainable_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(PerceptNet(), random.PRNGKey(config.SEED), tx, input_shape=(1,384,512,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before actually training the model we're going to set up the checkpointer to be able to save our trained models:\n",
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "save_args = orbax_utils.save_args_from_target(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "state = orbax_checkpointer.restore(os.path.join(prev_run.dir,\"model-best\"), item=state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = {\n",
    "    \"distance\": [],\n",
    "    \"mos\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d3250f7e964e3e9e88f2930a34f7d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-23 11:24:07.577815: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /media/databases/IQA/KADIK10K/images/I23_07_01.png; No such file or directory\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} /media/databases/IQA/KADIK10K/images/I23_07_01.png; No such file or directory\n\t [[{{node ReadFile_1}}]] [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/tqdm/notebook.py:249\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:4700\u001b[0m, in \u001b[0;36m_NumpyIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4697\u001b[0m     numpy\u001b[38;5;241m.\u001b[39msetflags(write\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   4698\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m numpy\n\u001b[0;32m-> 4700\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mmap_structure(to_numpy, \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:814\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    813\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:777\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 777\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3028\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3028\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3030\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:6656\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   6655\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 6656\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} /media/databases/IQA/KADIK10K/images/I23_07_01.png; No such file or directory\n\t [[{{node ReadFile_1}}]] [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for batch in tqdm(dst_rdy.as_numpy_iterator()):\n",
    "    img, img_dist, mos = batch\n",
    "    distance = compute_distance(state=state, batch=batch)\n",
    "    metrics_history[\"distance\"].extend(distance)\n",
    "    metrics_history[\"mos\"].extend(mos)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(metrics_history[\"distance\"]) == len(dst.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PearsonRResult(statistic=-0.09987253410685878, pvalue=1.5208760413125765e-07),\n",
       " SignificanceResult(statistic=-0.09445223820020526, pvalue=6.913891870512814e-07))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.pearsonr(metrics_history[\"distance\"], metrics_history[\"mos\"]), stats.spearmanr(metrics_history[\"distance\"], metrics_history[\"mos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist_img</th>\n",
       "      <th>ref_img</th>\n",
       "      <th>dmos</th>\n",
       "      <th>var</th>\n",
       "      <th>Distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I01_01_01.png</td>\n",
       "      <td>I01.png</td>\n",
       "      <td>4.57</td>\n",
       "      <td>0.496</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I01_01_02.png</td>\n",
       "      <td>I01.png</td>\n",
       "      <td>4.33</td>\n",
       "      <td>0.869</td>\n",
       "      <td>30.293993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I01_01_03.png</td>\n",
       "      <td>I01.png</td>\n",
       "      <td>2.67</td>\n",
       "      <td>0.789</td>\n",
       "      <td>72.83459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I01_01_04.png</td>\n",
       "      <td>I01.png</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.596</td>\n",
       "      <td>124.22675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I01_01_05.png</td>\n",
       "      <td>I01.png</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.300</td>\n",
       "      <td>224.45325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dist_img  ref_img  dmos    var   Distance\n",
       "0  I01_01_01.png  I01.png  4.57  0.496        0.0\n",
       "1  I01_01_02.png  I01.png  4.33  0.869  30.293993\n",
       "2  I01_01_03.png  I01.png  2.67  0.789   72.83459\n",
       "3  I01_01_04.png  I01.png  1.67  0.596  124.22675\n",
       "4  I01_01_05.png  I01.png  1.10  0.300  224.45325"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = dst.data.copy()\n",
    "results[\"Distance\"] = metrics_history[\"distance\"]\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "<class 'wandb.errors.Error'>",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKADID10K\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKADID10K_pearson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpearsonr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics_history\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics_history\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mKADID10K_spearman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspearmanr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics_history\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics_history\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m           \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/testing_gpu/lib/python3.8/site-packages/wandb/sdk/lib/preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "wandb.log({\"KADID10K\": wandb.Table(dataframe=results),\n",
    "           \"KADID10K_pearson\": stats.pearsonr(metrics_history[\"distance\"], metrics_history[\"mos\"])[0],\n",
    "           \"KADID10K_spearman\": stats.spearmanr(metrics_history[\"distance\"], metrics_history[\"mos\"])[0],\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899b3528d0b04a0fa12764b92424b25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.012 MB uploaded\\r'), FloatProgress(value=0.23050245987579643, max=1.â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_parameters</td><td>2359300</td></tr><tr><td>trainable_parameters</td><td>2359300</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">VerySimple</strong> at: <a href='https://wandb.ai/jorgvt/PerceptNet_KLD/runs/ept3p1pg' target=\"_blank\">https://wandb.ai/jorgvt/PerceptNet_KLD/runs/ept3p1pg</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240423_112017-ept3p1pg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
