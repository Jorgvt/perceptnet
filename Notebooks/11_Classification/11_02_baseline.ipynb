{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os; os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jax.config import config\n",
    "# config.update(\"jax_debug_nans\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.set_visible_devices([], device_type='GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from typing import Any, Callable, Sequence, Union\n",
    "import numpy as np\n",
    "from fastcore.xtras import Path\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import jax\n",
    "from jax import lax, random, numpy as jnp\n",
    "from flax.core import freeze, unfreeze, FrozenDict\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "from flax.training import train_state\n",
    "from flax.training import orbax_utils\n",
    "\n",
    "import optax\n",
    "import orbax.checkpoint\n",
    "\n",
    "from clu import metrics\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "from einops import reduce, rearrange\n",
    "import wandb\n",
    "from iqadatasets.datasets import *\n",
    "from fxlayers.layers import *\n",
    "from fxlayers.initializers import mean\n",
    "from JaxPlayground.utils.constraints import *\n",
    "from JaxPlayground.utils.wandb import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'epochs':500,\n",
    "        'learning_rate':3e-4,\n",
    "        'batch_size':64,\n",
    "        'kernel_initializer':'ones',\n",
    "        'gdn_kernel_size':1,\n",
    "        'learnable_undersampling':False,\n",
    "        'verbose': 0,\n",
    "        'dataset': 'imagenette', # imagenet / imagenette / cifar10 / cifar100,\n",
    "        'validation_split': 0.2,\n",
    "        'seed': 42,\n",
    "        'GAP': False,\n",
    "        'use_bias': True,\n",
    "        \"dropout_rate\": 0.0,\n",
    "        \"l1\": False,\n",
    "        \"LAMBDA\": 0.0005,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: jorgvt. Use `wandb login --relogin` to force relogin\n",
      "wandb: wandb version 0.16.3 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n",
      "wandb: Tracking run with wandb version 0.16.0\n",
      "wandb: Run data is saved locally in /home/jorge/perceptnet/Notebooks/11_Classification/wandb/run-20240214_190307-3g72mp5c\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run NoVisionModel\n",
      "wandb:  View project at https://wandb.ai/jorgvt/PerceptNetClassification_JaX\n",
      "wandb:  View run at https://wandb.ai/jorgvt/PerceptNetClassification_JaX/runs/3g72mp5c\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project='PerceptNetClassification_JaX',\n",
    "            notes=\"\",\n",
    "            tags=[],\n",
    "            name = 'NoVisionModel',\n",
    "            config=config,\n",
    "            job_type=\"training\",\n",
    "            mode=\"online\",\n",
    "            )\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imagenet():\n",
    "    path_data = Path(\"/lustre/ific.uv.es/ml/uv075/Databases/imagenet_images/\")\n",
    "    dst_train = tf.keras.utils.image_dataset_from_directory(\n",
    "                path_data,\n",
    "                validation_split=config.validation_split,\n",
    "                subset=\"training\",\n",
    "                seed=config.seed,\n",
    "                shuffle=True,\n",
    "                # image_size=(img_height, img_width),\n",
    "                batch_size=config.batch_size)\n",
    "    dst_val = tf.keras.utils.image_dataset_from_directory(\n",
    "                path_data,\n",
    "                validation_split=config.validation_split,\n",
    "                subset=\"validation\",\n",
    "                seed=config.seed,\n",
    "                shuffle=False,\n",
    "                # image_size=(img_height, img_width),\n",
    "                batch_size=config.batch_size)\n",
    "    return dst_train, dst_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imagenette():\n",
    "    import tensorflow_datasets as tfds\n",
    "\n",
    "    dst_train, info = tfds.load(\"imagenette/320px-v2\", split=f\"train[:{(1-config.validation_split)*100:.0f}%]\", with_info=True, shuffle_files=True)\n",
    "    dst_val = tfds.load(\"imagenette/320px-v2\", split=f\"train[{(1-config.validation_split)*100:.0f}%:]\", with_info=False, shuffle_files=False)\n",
    "    def prepare_tfds(item):\n",
    "        x, y = item[\"image\"], item[\"label\"]\n",
    "        x = tf.image.resize_with_crop_or_pad(x, 256, 256)\n",
    "        return x, y\n",
    "    dst_train = dst_train.map(prepare_tfds)\n",
    "    dst_val = dst_val.map(prepare_tfds)\n",
    "\n",
    "    return dst_train.batch(config.batch_size), dst_val.batch(config.batch_size), info.features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10():\n",
    "    from tensorflow.keras.datasets import cifar10\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    (X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=config.validation_split, random_state=config.seed)\n",
    "    dst_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "    dst_val = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\n",
    "\n",
    "    return dst_train.batch(config.batch_size), dst_val.batch(config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar100():\n",
    "    from tensorflow.keras.datasets import cifar100\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    (X_train, Y_train), (X_test, Y_test) = cifar100.load_data()\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=config.validation_split, random_state=config.seed)\n",
    "    dst_train = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "    dst_val = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\n",
    "\n",
    "    return dst_train.batch(config.batch_size), dst_val.batch(config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on imagenette with 10 classes.\n"
     ]
    }
   ],
   "source": [
    "if config.dataset == \"imagenet\":\n",
    "    dst_train, dst_val = load_imagenet()\n",
    "    dst_train = dst_train.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y))\n",
    "    dst_val = dst_val.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y))\n",
    "    N_CLASSES = len(dst_train.class_names)\n",
    "elif config.dataset == \"cifar10\":\n",
    "    dst_train, dst_val = load_cifar10()\n",
    "    dst_train = dst_train.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y[:,0]))\n",
    "    dst_val = dst_val.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y[:,0]))\n",
    "    N_CLASSES = 10\n",
    "elif config.dataset == \"cifar100\":\n",
    "    dst_train, dst_val = load_cifar100()\n",
    "    dst_train = dst_train.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y[:,0]))\n",
    "    dst_val = dst_val.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y[:,0]))\n",
    "    N_CLASSES = 100\n",
    "elif config.dataset == \"imagenette\":\n",
    "    dst_train, dst_val, N_CLASSES = load_imagenette()\n",
    "    dst_train = dst_train.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y))\n",
    "    dst_val = dst_val.map(lambda x,y: (tf.cast(x, tf.float32)/255.0, y))\n",
    "else:\n",
    "    raise ValueError(\"Dataset parameter not allowed.\")\n",
    "print(f\"Training on {config.dataset} with {N_CLASSES} classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256, 256, 3), (64,))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dst_train.as_numpy_iterator()))\n",
    "input_shape = x[0].shape\n",
    "input_shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.summary[\"N_CLASSES\"] = N_CLASSES\n",
    "wandb.run.summary[\"Input_Shape\"] = tuple(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst_tid2013 = TID2013(\"/lustre/ific.uv.es/ml/uv075/Databases/IQA/TID/TID2013\").dataset\\\n",
    "#                                                                               .batch(config.batch_size)\\\n",
    "#                                                                               .prefetch(1)\n",
    "dst_tid2013 = TID2013(\"/media/databases/IQA/TID/TID2013\").dataset\\\n",
    "                                                         .batch(config.batch_size)\\\n",
    "                                                         .prefetch(1)                                                                              "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "dst_train_rdy = dst_train.cache().prefetch(buffer_size=1)\n",
    "dst_val_rdy = dst_val.cache().prefetch(buffer_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Identity(nn.Module):\n",
    "\n",
    "#     @nn.compact\n",
    "#     def __call__(self,\n",
    "#                  inputs,\n",
    "#                  **kwargs,\n",
    "#                  ):\n",
    "#         return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptNet(nn.Module):\n",
    "    \"\"\"IQA model inspired by the visual system.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 **kwargs,\n",
    "                 ):\n",
    "        # return Identity()(inputs, **kwargs)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    N_CLASSES: int\n",
    "    GAP: bool = False\n",
    "    dropout_rate: float = 0.5\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 train=False,\n",
    "                 ):\n",
    "        outputs = reduce(inputs, \"b h w c -> b c\", reduction=\"mean\") if self.GAP else rearrange(inputs, \"b h w c -> b (h w c)\")\n",
    "        outputs = nn.Dropout(rate=self.dropout_rate, deterministic=not train)(outputs) if self.dropout_rate > 0.0 else outputs\n",
    "        outputs = nn.Dense(self.N_CLASSES)(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerceptNetClassifier(nn.Module):\n",
    "    \"\"\"Classifier with a PerceptNet backbone.\"\"\"\n",
    "\n",
    "    def setup(self):\n",
    "        self.perceptnet = PerceptNet()\n",
    "        self.cls = Classifier(N_CLASSES=N_CLASSES, GAP=config.GAP, dropout_rate=config.dropout_rate)\n",
    "\n",
    "    def __call__(self,\n",
    "                 inputs,\n",
    "                 train=False,\n",
    "                 ):\n",
    "        outputs = self.perceptnet(inputs, train=train)\n",
    "        outputs = self.cls(outputs, train=train)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@struct.dataclass\n",
    "class Metrics(metrics.Collection):\n",
    "    \"\"\"Collection of metrics to be tracked during training.\"\"\"\n",
    "    accuracy: metrics.Accuracy\n",
    "    loss: metrics.Average.from_output(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState(train_state.TrainState):\n",
    "    metrics: Metrics\n",
    "    state: FrozenDict\n",
    "    key: jax.Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(module, key, tx, input_shape):\n",
    "    \"\"\"Creates the initial `TrainState`.\"\"\"\n",
    "    variables = module.init(key, jnp.ones(input_shape), train=False)\n",
    "    _, dropout_key = random.split(random.PRNGKey(42))\n",
    "    state, params = variables.pop('params')\n",
    "    \n",
    "    params = unfreeze(params)\n",
    "    params[\"perceptnet\"] = {}\n",
    "    params = freeze(params)\n",
    "    \n",
    "    return TrainState.create(\n",
    "        apply_fn=module.apply,\n",
    "        params=params,\n",
    "        state=state,\n",
    "        key=dropout_key,\n",
    "        tx=tx,\n",
    "        metrics=Metrics.empty()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = create_train_state(PerceptNetClassifier(), random.PRNGKey(config.seed), optax.adam(config.learning_rate), input_shape=(1,*(x.shape[1:])))\n",
    "state = state.replace(params=clip_layer(state.params, \"GDN\", a_min=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = unfreeze(state.params)\n",
    "# params[\"perceptnet\"] = {}\n",
    "# state = state.replace(params=freeze(params))\n",
    "# state.params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log the number of trainable weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1966090"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(state.params))\n",
    "param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.summary[\"trainable_parameters\"] = param_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "save_args = orbax_utils.save_args_from_target(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    dropout_train_key = random.fold_in(key=state.key, data=state.step)\n",
    "    img, label = batch\n",
    "    def loss_fn(params):\n",
    "        ## Forward pass through the model\n",
    "        img_pred = state.apply_fn({\"params\": params, **state.state}, img, train=True, rngs={\"dropout\": dropout_train_key})\n",
    "\n",
    "        ## Calculate crossentropy\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(img_pred, label).mean()\n",
    "\n",
    "        ## Add L1 regularization\n",
    "        if config.l1: loss += config.LAMBDA*jnp.abs(state.params[\"cls\"][\"Dense_0\"][\"kernel\"]).mean()\n",
    "        \n",
    "        return loss, img_pred\n",
    "    \n",
    "    (loss, dist_diff), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics_updates = state.metrics.single_from_model_output(loss=loss, logits=dist_diff, labels=jnp.round(label).astype(int))\n",
    "    metrics = state.metrics.merge(metrics_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def val_step(state, batch):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    img, label = batch\n",
    "    def loss_fn(params):\n",
    "        ## Forward pass through the model\n",
    "        img_pred = state.apply_fn({\"params\": params, **state.state}, img, train=False)\n",
    "\n",
    "        ## Calculate crossentropy\n",
    "        return optax.softmax_cross_entropy_with_integer_labels(img_pred, label).mean(), img_pred\n",
    "    \n",
    "    loss, dist_diff = loss_fn(state.params)\n",
    "    metrics_updates = state.metrics.single_from_model_output(loss=loss, logits=dist_diff, labels=jnp.round(label).astype(int))\n",
    "    metrics = state.metrics.merge(metrics_updates)\n",
    "    state = state.replace(metrics=metrics)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(state, img):\n",
    "    img_pred = PerceptNet().apply({\"params\": state.params[\"perceptnet\"]}, img)\n",
    "    return img_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(a, b): return jnp.sqrt(jnp.sum((a-b)**2, axis=(1,2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def obtain_distances(state, batch):\n",
    "    ref, dist, mos = batch\n",
    "    pred_ref = forward_pass(state, ref)\n",
    "    pred_dist = forward_pass(state, dist)\n",
    "    distance = rmse(pred_ref, pred_dist)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_correlation(state, dst):\n",
    "    distances, moses = [], []\n",
    "    for batch in dst:\n",
    "        distance = obtain_distances(state, batch)\n",
    "        distances.extend(distance)\n",
    "        moses.extend(batch[2])\n",
    "        # break\n",
    "    return stats.pearsonr(distances, moses)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_accuracy\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_accuracy\": [],\n",
    "    \"correlation\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -> [Train] Loss: 7.648 Acc: 0.199 [Val] Loss: 4.222 Acc: 0.243 || Corr: -0.598\n",
      "Epoch 1 -> [Train] Loss: 4.365 Acc: 0.253 [Val] Loss: 5.298 Acc: 0.230 || Corr: -0.598\n",
      "Epoch 2 -> [Train] Loss: 3.393 Acc: 0.312 [Val] Loss: 4.376 Acc: 0.197 || Corr: -0.598\n",
      "Epoch 3 -> [Train] Loss: 4.685 Acc: 0.279 [Val] Loss: 3.983 Acc: 0.233 || Corr: -0.598\n",
      "Epoch 4 -> [Train] Loss: 3.418 Acc: 0.329 [Val] Loss: 5.208 Acc: 0.216 || Corr: -0.598\n",
      "Epoch 5 -> [Train] Loss: 3.936 Acc: 0.329 [Val] Loss: 4.708 Acc: 0.260 || Corr: -0.598\n",
      "Epoch 6 -> [Train] Loss: 3.061 Acc: 0.388 [Val] Loss: 3.417 Acc: 0.276 || Corr: -0.598\n",
      "Epoch 7 -> [Train] Loss: 2.554 Acc: 0.422 [Val] Loss: 3.491 Acc: 0.276 || Corr: -0.598\n",
      "Epoch 8 -> [Train] Loss: 2.820 Acc: 0.421 [Val] Loss: 3.204 Acc: 0.270 || Corr: -0.598\n",
      "Epoch 9 -> [Train] Loss: 2.545 Acc: 0.444 [Val] Loss: 3.128 Acc: 0.294 || Corr: -0.598\n",
      "Epoch 10 -> [Train] Loss: 2.670 Acc: 0.440 [Val] Loss: 3.971 Acc: 0.282 || Corr: -0.598\n",
      "Epoch 11 -> [Train] Loss: 4.194 Acc: 0.390 [Val] Loss: 4.064 Acc: 0.307 || Corr: -0.598\n",
      "Epoch 12 -> [Train] Loss: 2.856 Acc: 0.447 [Val] Loss: 3.198 Acc: 0.299 || Corr: -0.598\n",
      "Epoch 13 -> [Train] Loss: 2.798 Acc: 0.470 [Val] Loss: 5.437 Acc: 0.262 || Corr: -0.598\n",
      "Epoch 14 -> [Train] Loss: 2.273 Acc: 0.504 [Val] Loss: 3.493 Acc: 0.318 || Corr: -0.598\n",
      "Epoch 15 -> [Train] Loss: 2.155 Acc: 0.527 [Val] Loss: 5.225 Acc: 0.211 || Corr: -0.598\n",
      "Epoch 16 -> [Train] Loss: 2.528 Acc: 0.503 [Val] Loss: 4.684 Acc: 0.258 || Corr: -0.598\n",
      "Epoch 17 -> [Train] Loss: 2.261 Acc: 0.522 [Val] Loss: 4.182 Acc: 0.289 || Corr: -0.598\n",
      "Epoch 18 -> [Train] Loss: 2.365 Acc: 0.548 [Val] Loss: 3.738 Acc: 0.307 || Corr: -0.598\n",
      "Epoch 19 -> [Train] Loss: 1.969 Acc: 0.550 [Val] Loss: 5.806 Acc: 0.222 || Corr: -0.598\n",
      "Epoch 20 -> [Train] Loss: 1.891 Acc: 0.581 [Val] Loss: 5.554 Acc: 0.258 || Corr: -0.598\n",
      "Epoch 21 -> [Train] Loss: 1.794 Acc: 0.600 [Val] Loss: 3.827 Acc: 0.302 || Corr: -0.598\n",
      "Epoch 22 -> [Train] Loss: 3.084 Acc: 0.503 [Val] Loss: 4.402 Acc: 0.268 || Corr: -0.598\n",
      "Epoch 23 -> [Train] Loss: 2.100 Acc: 0.554 [Val] Loss: 5.647 Acc: 0.246 || Corr: -0.598\n",
      "Epoch 24 -> [Train] Loss: 2.235 Acc: 0.552 [Val] Loss: 4.868 Acc: 0.278 || Corr: -0.598\n",
      "Epoch 25 -> [Train] Loss: 2.972 Acc: 0.511 [Val] Loss: 5.425 Acc: 0.262 || Corr: -0.598\n",
      "Epoch 26 -> [Train] Loss: 2.317 Acc: 0.547 [Val] Loss: 6.325 Acc: 0.227 || Corr: -0.598\n",
      "Epoch 27 -> [Train] Loss: 2.622 Acc: 0.545 [Val] Loss: 6.932 Acc: 0.230 || Corr: -0.598\n",
      "Epoch 28 -> [Train] Loss: 2.111 Acc: 0.588 [Val] Loss: 6.454 Acc: 0.232 || Corr: -0.598\n",
      "Epoch 29 -> [Train] Loss: 2.211 Acc: 0.579 [Val] Loss: 6.292 Acc: 0.222 || Corr: -0.598\n",
      "Epoch 30 -> [Train] Loss: 2.174 Acc: 0.572 [Val] Loss: 6.185 Acc: 0.240 || Corr: -0.598\n",
      "Epoch 31 -> [Train] Loss: 2.109 Acc: 0.586 [Val] Loss: 6.796 Acc: 0.230 || Corr: -0.598\n",
      "Epoch 32 -> [Train] Loss: 2.217 Acc: 0.596 [Val] Loss: 7.058 Acc: 0.212 || Corr: -0.598\n",
      "Epoch 33 -> [Train] Loss: 2.381 Acc: 0.570 [Val] Loss: 8.728 Acc: 0.176 || Corr: -0.598\n",
      "Epoch 34 -> [Train] Loss: 3.130 Acc: 0.539 [Val] Loss: 10.226 Acc: 0.157 || Corr: -0.598\n",
      "Epoch 35 -> [Train] Loss: 2.362 Acc: 0.587 [Val] Loss: 10.229 Acc: 0.157 || Corr: -0.598\n",
      "Epoch 36 -> [Train] Loss: 1.866 Acc: 0.642 [Val] Loss: 10.805 Acc: 0.149 || Corr: -0.598\n",
      "Epoch 37 -> [Train] Loss: 2.265 Acc: 0.594 [Val] Loss: 9.483 Acc: 0.168 || Corr: -0.598\n",
      "Epoch 38 -> [Train] Loss: 2.115 Acc: 0.606 [Val] Loss: 9.606 Acc: 0.161 || Corr: -0.598\n",
      "Epoch 39 -> [Train] Loss: 2.573 Acc: 0.568 [Val] Loss: 8.723 Acc: 0.190 || Corr: -0.598\n",
      "Epoch 40 -> [Train] Loss: 2.909 Acc: 0.547 [Val] Loss: 10.006 Acc: 0.156 || Corr: -0.598\n",
      "Epoch 41 -> [Train] Loss: 2.204 Acc: 0.600 [Val] Loss: 9.176 Acc: 0.169 || Corr: -0.598\n",
      "Epoch 42 -> [Train] Loss: 1.921 Acc: 0.626 [Val] Loss: 9.066 Acc: 0.185 || Corr: -0.598\n",
      "Epoch 43 -> [Train] Loss: 1.723 Acc: 0.638 [Val] Loss: 7.764 Acc: 0.207 || Corr: -0.598\n",
      "Epoch 44 -> [Train] Loss: 1.956 Acc: 0.620 [Val] Loss: 8.334 Acc: 0.232 || Corr: -0.598\n",
      "Epoch 45 -> [Train] Loss: 1.905 Acc: 0.632 [Val] Loss: 7.910 Acc: 0.246 || Corr: -0.598\n",
      "Epoch 46 -> [Train] Loss: 2.841 Acc: 0.575 [Val] Loss: 9.401 Acc: 0.211 || Corr: -0.598\n",
      "Epoch 47 -> [Train] Loss: 2.723 Acc: 0.578 [Val] Loss: 8.702 Acc: 0.263 || Corr: -0.598\n",
      "Epoch 48 -> [Train] Loss: 2.311 Acc: 0.613 [Val] Loss: 9.350 Acc: 0.224 || Corr: -0.598\n",
      "Epoch 49 -> [Train] Loss: 3.127 Acc: 0.586 [Val] Loss: 8.667 Acc: 0.258 || Corr: -0.598\n",
      "Epoch 50 -> [Train] Loss: 2.284 Acc: 0.631 [Val] Loss: 10.009 Acc: 0.228 || Corr: -0.598\n",
      "Epoch 51 -> [Train] Loss: 2.268 Acc: 0.610 [Val] Loss: 8.243 Acc: 0.243 || Corr: -0.598\n",
      "Epoch 52 -> [Train] Loss: 2.633 Acc: 0.619 [Val] Loss: 10.396 Acc: 0.208 || Corr: -0.598\n",
      "Epoch 53 -> [Train] Loss: 2.676 Acc: 0.604 [Val] Loss: 9.774 Acc: 0.185 || Corr: -0.598\n",
      "Epoch 54 -> [Train] Loss: 2.301 Acc: 0.637 [Val] Loss: 7.968 Acc: 0.235 || Corr: -0.598\n",
      "Epoch 55 -> [Train] Loss: 2.385 Acc: 0.641 [Val] Loss: 7.173 Acc: 0.231 || Corr: -0.598\n",
      "Epoch 56 -> [Train] Loss: 1.942 Acc: 0.673 [Val] Loss: 7.892 Acc: 0.207 || Corr: -0.598\n",
      "Epoch 57 -> [Train] Loss: 2.092 Acc: 0.660 [Val] Loss: 7.188 Acc: 0.246 || Corr: -0.598\n",
      "Epoch 58 -> [Train] Loss: 1.937 Acc: 0.662 [Val] Loss: 8.088 Acc: 0.227 || Corr: -0.598\n",
      "Epoch 59 -> [Train] Loss: 2.059 Acc: 0.656 [Val] Loss: 8.163 Acc: 0.224 || Corr: -0.598\n",
      "Epoch 60 -> [Train] Loss: 2.048 Acc: 0.647 [Val] Loss: 8.523 Acc: 0.225 || Corr: -0.598\n",
      "Epoch 61 -> [Train] Loss: 1.887 Acc: 0.665 [Val] Loss: 9.283 Acc: 0.209 || Corr: -0.598\n",
      "Epoch 62 -> [Train] Loss: 1.816 Acc: 0.675 [Val] Loss: 8.836 Acc: 0.218 || Corr: -0.598\n",
      "Epoch 63 -> [Train] Loss: 1.857 Acc: 0.662 [Val] Loss: 8.378 Acc: 0.212 || Corr: -0.598\n",
      "Epoch 64 -> [Train] Loss: 2.159 Acc: 0.662 [Val] Loss: 8.163 Acc: 0.231 || Corr: -0.598\n",
      "Epoch 65 -> [Train] Loss: 2.243 Acc: 0.636 [Val] Loss: 9.752 Acc: 0.237 || Corr: -0.598\n",
      "Epoch 66 -> [Train] Loss: 2.475 Acc: 0.632 [Val] Loss: 10.958 Acc: 0.216 || Corr: -0.598\n",
      "Epoch 67 -> [Train] Loss: 2.547 Acc: 0.619 [Val] Loss: 9.191 Acc: 0.214 || Corr: -0.598\n",
      "Epoch 68 -> [Train] Loss: 3.155 Acc: 0.596 [Val] Loss: 9.910 Acc: 0.211 || Corr: -0.598\n",
      "Epoch 69 -> [Train] Loss: 2.754 Acc: 0.613 [Val] Loss: 9.039 Acc: 0.232 || Corr: -0.598\n",
      "Epoch 70 -> [Train] Loss: 2.324 Acc: 0.641 [Val] Loss: 9.811 Acc: 0.228 || Corr: -0.598\n",
      "Epoch 71 -> [Train] Loss: 2.115 Acc: 0.673 [Val] Loss: 9.338 Acc: 0.229 || Corr: -0.598\n",
      "Epoch 72 -> [Train] Loss: 2.083 Acc: 0.672 [Val] Loss: 9.179 Acc: 0.234 || Corr: -0.598\n",
      "Epoch 73 -> [Train] Loss: 1.931 Acc: 0.691 [Val] Loss: 10.268 Acc: 0.230 || Corr: -0.598\n",
      "Epoch 74 -> [Train] Loss: 1.617 Acc: 0.720 [Val] Loss: 10.578 Acc: 0.232 || Corr: -0.598\n",
      "Epoch 75 -> [Train] Loss: 1.807 Acc: 0.717 [Val] Loss: 9.736 Acc: 0.224 || Corr: -0.598\n",
      "Epoch 76 -> [Train] Loss: 1.437 Acc: 0.745 [Val] Loss: 10.325 Acc: 0.195 || Corr: -0.598\n",
      "Epoch 77 -> [Train] Loss: 1.525 Acc: 0.734 [Val] Loss: 9.601 Acc: 0.233 || Corr: -0.598\n",
      "Epoch 78 -> [Train] Loss: 1.820 Acc: 0.714 [Val] Loss: 10.346 Acc: 0.200 || Corr: -0.598\n",
      "Epoch 79 -> [Train] Loss: 1.829 Acc: 0.707 [Val] Loss: 10.928 Acc: 0.182 || Corr: -0.598\n",
      "Epoch 80 -> [Train] Loss: 1.779 Acc: 0.710 [Val] Loss: 9.789 Acc: 0.209 || Corr: -0.598\n",
      "Epoch 81 -> [Train] Loss: 1.840 Acc: 0.718 [Val] Loss: 12.098 Acc: 0.206 || Corr: -0.598\n",
      "Epoch 82 -> [Train] Loss: 2.228 Acc: 0.694 [Val] Loss: 9.812 Acc: 0.218 || Corr: -0.598\n",
      "Epoch 83 -> [Train] Loss: 2.395 Acc: 0.663 [Val] Loss: 11.696 Acc: 0.225 || Corr: -0.598\n",
      "Epoch 84 -> [Train] Loss: 2.334 Acc: 0.680 [Val] Loss: 9.271 Acc: 0.252 || Corr: -0.598\n",
      "Epoch 85 -> [Train] Loss: 2.036 Acc: 0.693 [Val] Loss: 9.728 Acc: 0.243 || Corr: -0.598\n",
      "Epoch 86 -> [Train] Loss: 1.846 Acc: 0.700 [Val] Loss: 9.259 Acc: 0.251 || Corr: -0.598\n",
      "Epoch 87 -> [Train] Loss: 1.962 Acc: 0.693 [Val] Loss: 11.833 Acc: 0.237 || Corr: -0.598\n",
      "Epoch 88 -> [Train] Loss: 1.721 Acc: 0.717 [Val] Loss: 10.294 Acc: 0.227 || Corr: -0.598\n",
      "Epoch 89 -> [Train] Loss: 1.650 Acc: 0.733 [Val] Loss: 12.049 Acc: 0.206 || Corr: -0.598\n",
      "Epoch 90 -> [Train] Loss: 1.508 Acc: 0.736 [Val] Loss: 9.265 Acc: 0.237 || Corr: -0.598\n",
      "Epoch 91 -> [Train] Loss: 1.633 Acc: 0.731 [Val] Loss: 11.316 Acc: 0.216 || Corr: -0.598\n",
      "Epoch 92 -> [Train] Loss: 1.695 Acc: 0.725 [Val] Loss: 9.693 Acc: 0.214 || Corr: -0.598\n",
      "Epoch 93 -> [Train] Loss: 1.257 Acc: 0.761 [Val] Loss: 10.846 Acc: 0.193 || Corr: -0.598\n",
      "Epoch 94 -> [Train] Loss: 1.174 Acc: 0.766 [Val] Loss: 13.052 Acc: 0.154 || Corr: -0.598\n",
      "Epoch 95 -> [Train] Loss: 1.371 Acc: 0.751 [Val] Loss: 12.596 Acc: 0.173 || Corr: -0.598\n",
      "Epoch 96 -> [Train] Loss: 1.537 Acc: 0.730 [Val] Loss: 12.818 Acc: 0.164 || Corr: -0.598\n",
      "Epoch 97 -> [Train] Loss: 1.690 Acc: 0.742 [Val] Loss: 13.923 Acc: 0.164 || Corr: -0.598\n",
      "Epoch 98 -> [Train] Loss: 1.737 Acc: 0.732 [Val] Loss: 16.672 Acc: 0.150 || Corr: -0.598\n",
      "Epoch 99 -> [Train] Loss: 2.441 Acc: 0.700 [Val] Loss: 12.626 Acc: 0.199 || Corr: -0.598\n",
      "Epoch 100 -> [Train] Loss: 2.116 Acc: 0.716 [Val] Loss: 11.968 Acc: 0.201 || Corr: -0.598\n",
      "Epoch 101 -> [Train] Loss: 1.852 Acc: 0.729 [Val] Loss: 12.837 Acc: 0.185 || Corr: -0.598\n",
      "Epoch 102 -> [Train] Loss: 1.625 Acc: 0.749 [Val] Loss: 12.385 Acc: 0.197 || Corr: -0.598\n",
      "Epoch 103 -> [Train] Loss: 1.578 Acc: 0.749 [Val] Loss: 13.125 Acc: 0.195 || Corr: -0.598\n",
      "Epoch 104 -> [Train] Loss: 1.860 Acc: 0.729 [Val] Loss: 14.006 Acc: 0.178 || Corr: -0.598\n",
      "Epoch 105 -> [Train] Loss: 1.951 Acc: 0.710 [Val] Loss: 13.808 Acc: 0.190 || Corr: -0.598\n",
      "Epoch 106 -> [Train] Loss: 2.306 Acc: 0.703 [Val] Loss: 14.772 Acc: 0.192 || Corr: -0.598\n",
      "Epoch 107 -> [Train] Loss: 2.096 Acc: 0.723 [Val] Loss: 14.904 Acc: 0.183 || Corr: -0.598\n",
      "Epoch 108 -> [Train] Loss: 1.696 Acc: 0.748 [Val] Loss: 15.649 Acc: 0.179 || Corr: -0.598\n",
      "Epoch 109 -> [Train] Loss: 1.919 Acc: 0.731 [Val] Loss: 17.171 Acc: 0.167 || Corr: -0.598\n",
      "Epoch 110 -> [Train] Loss: 1.895 Acc: 0.741 [Val] Loss: 12.944 Acc: 0.202 || Corr: -0.598\n",
      "Epoch 111 -> [Train] Loss: 2.078 Acc: 0.717 [Val] Loss: 10.784 Acc: 0.238 || Corr: -0.598\n",
      "Epoch 112 -> [Train] Loss: 1.753 Acc: 0.739 [Val] Loss: 10.837 Acc: 0.227 || Corr: -0.598\n",
      "Epoch 113 -> [Train] Loss: 1.584 Acc: 0.749 [Val] Loss: 12.097 Acc: 0.194 || Corr: -0.598\n",
      "Epoch 114 -> [Train] Loss: 1.391 Acc: 0.760 [Val] Loss: 12.238 Acc: 0.181 || Corr: -0.598\n",
      "Epoch 115 -> [Train] Loss: 1.424 Acc: 0.765 [Val] Loss: 15.412 Acc: 0.174 || Corr: -0.598\n",
      "Epoch 116 -> [Train] Loss: 1.679 Acc: 0.749 [Val] Loss: 15.704 Acc: 0.182 || Corr: -0.598\n",
      "Epoch 117 -> [Train] Loss: 2.168 Acc: 0.711 [Val] Loss: 13.730 Acc: 0.215 || Corr: -0.598\n",
      "Epoch 118 -> [Train] Loss: 2.511 Acc: 0.696 [Val] Loss: 15.033 Acc: 0.188 || Corr: -0.598\n",
      "Epoch 119 -> [Train] Loss: 1.917 Acc: 0.729 [Val] Loss: 12.935 Acc: 0.225 || Corr: -0.598\n",
      "Epoch 120 -> [Train] Loss: 1.692 Acc: 0.732 [Val] Loss: 15.067 Acc: 0.181 || Corr: -0.598\n",
      "Epoch 121 -> [Train] Loss: 1.923 Acc: 0.714 [Val] Loss: 14.696 Acc: 0.196 || Corr: -0.598\n",
      "Epoch 122 -> [Train] Loss: 2.277 Acc: 0.692 [Val] Loss: 11.567 Acc: 0.238 || Corr: -0.598\n",
      "Epoch 123 -> [Train] Loss: 1.824 Acc: 0.729 [Val] Loss: 17.963 Acc: 0.172 || Corr: -0.598\n",
      "Epoch 124 -> [Train] Loss: 2.314 Acc: 0.695 [Val] Loss: 16.576 Acc: 0.185 || Corr: -0.598\n",
      "Epoch 125 -> [Train] Loss: 2.114 Acc: 0.708 [Val] Loss: 16.216 Acc: 0.195 || Corr: -0.598\n",
      "Epoch 126 -> [Train] Loss: 2.142 Acc: 0.707 [Val] Loss: 12.046 Acc: 0.251 || Corr: -0.598\n",
      "Epoch 127 -> [Train] Loss: 2.190 Acc: 0.720 [Val] Loss: 13.830 Acc: 0.239 || Corr: -0.598\n",
      "Epoch 128 -> [Train] Loss: 2.087 Acc: 0.716 [Val] Loss: 11.271 Acc: 0.263 || Corr: -0.598\n",
      "Epoch 129 -> [Train] Loss: 1.820 Acc: 0.730 [Val] Loss: 12.728 Acc: 0.263 || Corr: -0.598\n",
      "Epoch 130 -> [Train] Loss: 2.070 Acc: 0.726 [Val] Loss: 12.398 Acc: 0.248 || Corr: -0.598\n",
      "Epoch 131 -> [Train] Loss: 2.195 Acc: 0.712 [Val] Loss: 12.158 Acc: 0.280 || Corr: -0.598\n",
      "Epoch 132 -> [Train] Loss: 3.250 Acc: 0.682 [Val] Loss: 12.807 Acc: 0.237 || Corr: -0.598\n",
      "Epoch 133 -> [Train] Loss: 2.411 Acc: 0.709 [Val] Loss: 13.048 Acc: 0.222 || Corr: -0.598\n",
      "Epoch 134 -> [Train] Loss: 2.093 Acc: 0.720 [Val] Loss: 12.148 Acc: 0.242 || Corr: -0.598\n",
      "Epoch 135 -> [Train] Loss: 2.462 Acc: 0.710 [Val] Loss: 14.074 Acc: 0.212 || Corr: -0.598\n",
      "Epoch 136 -> [Train] Loss: 2.234 Acc: 0.719 [Val] Loss: 15.191 Acc: 0.210 || Corr: -0.598\n",
      "Epoch 137 -> [Train] Loss: 1.987 Acc: 0.739 [Val] Loss: 15.783 Acc: 0.204 || Corr: -0.598\n",
      "Epoch 138 -> [Train] Loss: 2.132 Acc: 0.723 [Val] Loss: 15.705 Acc: 0.205 || Corr: -0.598\n",
      "Epoch 139 -> [Train] Loss: 2.359 Acc: 0.708 [Val] Loss: 15.186 Acc: 0.206 || Corr: -0.598\n",
      "Epoch 140 -> [Train] Loss: 1.945 Acc: 0.726 [Val] Loss: 14.717 Acc: 0.196 || Corr: -0.598\n",
      "Epoch 141 -> [Train] Loss: 1.959 Acc: 0.727 [Val] Loss: 15.090 Acc: 0.212 || Corr: -0.598\n",
      "Epoch 142 -> [Train] Loss: 2.007 Acc: 0.714 [Val] Loss: 16.531 Acc: 0.192 || Corr: -0.598\n",
      "Epoch 143 -> [Train] Loss: 2.424 Acc: 0.693 [Val] Loss: 15.075 Acc: 0.223 || Corr: -0.598\n",
      "Epoch 144 -> [Train] Loss: 2.349 Acc: 0.696 [Val] Loss: 14.963 Acc: 0.228 || Corr: -0.598\n",
      "Epoch 145 -> [Train] Loss: 3.167 Acc: 0.644 [Val] Loss: 16.581 Acc: 0.212 || Corr: -0.598\n",
      "Epoch 146 -> [Train] Loss: 2.972 Acc: 0.670 [Val] Loss: 15.281 Acc: 0.209 || Corr: -0.598\n",
      "Epoch 147 -> [Train] Loss: 2.956 Acc: 0.685 [Val] Loss: 14.273 Acc: 0.228 || Corr: -0.598\n",
      "Epoch 148 -> [Train] Loss: 2.974 Acc: 0.676 [Val] Loss: 14.579 Acc: 0.221 || Corr: -0.598\n",
      "Epoch 149 -> [Train] Loss: 2.447 Acc: 0.712 [Val] Loss: 12.831 Acc: 0.250 || Corr: -0.598\n",
      "Epoch 150 -> [Train] Loss: 2.463 Acc: 0.718 [Val] Loss: 12.309 Acc: 0.259 || Corr: -0.598\n",
      "Epoch 151 -> [Train] Loss: 1.871 Acc: 0.750 [Val] Loss: 11.944 Acc: 0.267 || Corr: -0.598\n",
      "Epoch 152 -> [Train] Loss: 2.043 Acc: 0.745 [Val] Loss: 12.413 Acc: 0.259 || Corr: -0.598\n",
      "Epoch 153 -> [Train] Loss: 1.623 Acc: 0.770 [Val] Loss: 13.231 Acc: 0.238 || Corr: -0.598\n",
      "Epoch 154 -> [Train] Loss: 1.548 Acc: 0.786 [Val] Loss: 17.241 Acc: 0.201 || Corr: -0.598\n",
      "Epoch 155 -> [Train] Loss: 1.817 Acc: 0.772 [Val] Loss: 17.331 Acc: 0.216 || Corr: -0.598\n",
      "Epoch 156 -> [Train] Loss: 1.990 Acc: 0.761 [Val] Loss: 16.449 Acc: 0.217 || Corr: -0.598\n",
      "Epoch 157 -> [Train] Loss: 1.729 Acc: 0.786 [Val] Loss: 13.707 Acc: 0.249 || Corr: -0.598\n",
      "Epoch 158 -> [Train] Loss: 1.838 Acc: 0.782 [Val] Loss: 12.995 Acc: 0.246 || Corr: -0.598\n",
      "Epoch 159 -> [Train] Loss: 1.917 Acc: 0.779 [Val] Loss: 11.750 Acc: 0.254 || Corr: -0.598\n",
      "Epoch 160 -> [Train] Loss: 1.584 Acc: 0.793 [Val] Loss: 12.955 Acc: 0.238 || Corr: -0.598\n",
      "Epoch 161 -> [Train] Loss: 1.827 Acc: 0.780 [Val] Loss: 12.015 Acc: 0.252 || Corr: -0.598\n",
      "Epoch 162 -> [Train] Loss: 1.710 Acc: 0.774 [Val] Loss: 11.053 Acc: 0.273 || Corr: -0.598\n",
      "Epoch 163 -> [Train] Loss: 2.080 Acc: 0.758 [Val] Loss: 11.604 Acc: 0.256 || Corr: -0.598\n",
      "Epoch 164 -> [Train] Loss: 1.809 Acc: 0.775 [Val] Loss: 12.959 Acc: 0.246 || Corr: -0.598\n",
      "Epoch 165 -> [Train] Loss: 1.576 Acc: 0.783 [Val] Loss: 12.668 Acc: 0.258 || Corr: -0.598\n",
      "Epoch 166 -> [Train] Loss: 1.582 Acc: 0.787 [Val] Loss: 12.024 Acc: 0.256 || Corr: -0.598\n",
      "Epoch 167 -> [Train] Loss: 1.395 Acc: 0.802 [Val] Loss: 12.049 Acc: 0.254 || Corr: -0.598\n",
      "Epoch 168 -> [Train] Loss: 1.205 Acc: 0.813 [Val] Loss: 11.622 Acc: 0.277 || Corr: -0.598\n",
      "Epoch 169 -> [Train] Loss: 1.261 Acc: 0.811 [Val] Loss: 12.390 Acc: 0.267 || Corr: -0.598\n",
      "Epoch 170 -> [Train] Loss: 1.336 Acc: 0.803 [Val] Loss: 14.200 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 171 -> [Train] Loss: 1.428 Acc: 0.789 [Val] Loss: 13.769 Acc: 0.243 || Corr: -0.598\n",
      "Epoch 172 -> [Train] Loss: 1.698 Acc: 0.788 [Val] Loss: 14.187 Acc: 0.228 || Corr: -0.598\n",
      "Epoch 173 -> [Train] Loss: 1.446 Acc: 0.794 [Val] Loss: 15.285 Acc: 0.216 || Corr: -0.598\n",
      "Epoch 174 -> [Train] Loss: 1.316 Acc: 0.805 [Val] Loss: 12.954 Acc: 0.256 || Corr: -0.598\n",
      "Epoch 175 -> [Train] Loss: 1.329 Acc: 0.806 [Val] Loss: 13.040 Acc: 0.250 || Corr: -0.598\n",
      "Epoch 176 -> [Train] Loss: 1.266 Acc: 0.804 [Val] Loss: 12.862 Acc: 0.248 || Corr: -0.598\n",
      "Epoch 177 -> [Train] Loss: 1.211 Acc: 0.814 [Val] Loss: 12.230 Acc: 0.249 || Corr: -0.598\n",
      "Epoch 178 -> [Train] Loss: 1.268 Acc: 0.811 [Val] Loss: 13.458 Acc: 0.227 || Corr: -0.598\n",
      "Epoch 179 -> [Train] Loss: 1.364 Acc: 0.792 [Val] Loss: 12.688 Acc: 0.242 || Corr: -0.598\n",
      "Epoch 180 -> [Train] Loss: 1.370 Acc: 0.791 [Val] Loss: 12.779 Acc: 0.244 || Corr: -0.598\n",
      "Epoch 181 -> [Train] Loss: 1.620 Acc: 0.788 [Val] Loss: 12.404 Acc: 0.250 || Corr: -0.598\n",
      "Epoch 182 -> [Train] Loss: 1.467 Acc: 0.800 [Val] Loss: 13.844 Acc: 0.247 || Corr: -0.598\n",
      "Epoch 183 -> [Train] Loss: 1.503 Acc: 0.797 [Val] Loss: 14.792 Acc: 0.248 || Corr: -0.598\n",
      "Epoch 184 -> [Train] Loss: 1.451 Acc: 0.787 [Val] Loss: 14.720 Acc: 0.251 || Corr: -0.598\n",
      "Epoch 185 -> [Train] Loss: 1.499 Acc: 0.798 [Val] Loss: 14.690 Acc: 0.256 || Corr: -0.598\n",
      "Epoch 186 -> [Train] Loss: 1.517 Acc: 0.795 [Val] Loss: 12.243 Acc: 0.287 || Corr: -0.598\n",
      "Epoch 187 -> [Train] Loss: 1.702 Acc: 0.783 [Val] Loss: 11.313 Acc: 0.294 || Corr: -0.598\n",
      "Epoch 188 -> [Train] Loss: 1.653 Acc: 0.791 [Val] Loss: 13.272 Acc: 0.269 || Corr: -0.598\n",
      "Epoch 189 -> [Train] Loss: 1.659 Acc: 0.791 [Val] Loss: 15.585 Acc: 0.244 || Corr: -0.598\n",
      "Epoch 190 -> [Train] Loss: 1.783 Acc: 0.777 [Val] Loss: 14.449 Acc: 0.251 || Corr: -0.598\n",
      "Epoch 191 -> [Train] Loss: 2.082 Acc: 0.767 [Val] Loss: 14.651 Acc: 0.265 || Corr: -0.598\n",
      "Epoch 192 -> [Train] Loss: 1.926 Acc: 0.784 [Val] Loss: 13.564 Acc: 0.279 || Corr: -0.598\n",
      "Epoch 193 -> [Train] Loss: 2.010 Acc: 0.776 [Val] Loss: 13.773 Acc: 0.268 || Corr: -0.598\n",
      "Epoch 194 -> [Train] Loss: 1.986 Acc: 0.777 [Val] Loss: 15.404 Acc: 0.257 || Corr: -0.598\n",
      "Epoch 195 -> [Train] Loss: 1.690 Acc: 0.804 [Val] Loss: 13.897 Acc: 0.278 || Corr: -0.598\n",
      "Epoch 196 -> [Train] Loss: 1.474 Acc: 0.806 [Val] Loss: 15.116 Acc: 0.263 || Corr: -0.598\n",
      "Epoch 197 -> [Train] Loss: 1.588 Acc: 0.807 [Val] Loss: 14.763 Acc: 0.274 || Corr: -0.598\n",
      "Epoch 198 -> [Train] Loss: 1.781 Acc: 0.791 [Val] Loss: 14.599 Acc: 0.282 || Corr: -0.598\n",
      "Epoch 199 -> [Train] Loss: 1.467 Acc: 0.808 [Val] Loss: 16.741 Acc: 0.262 || Corr: -0.598\n",
      "Epoch 200 -> [Train] Loss: 1.382 Acc: 0.824 [Val] Loss: 17.047 Acc: 0.238 || Corr: -0.598\n",
      "Epoch 201 -> [Train] Loss: 1.294 Acc: 0.829 [Val] Loss: 17.576 Acc: 0.236 || Corr: -0.598\n",
      "Epoch 202 -> [Train] Loss: 1.110 Acc: 0.840 [Val] Loss: 16.183 Acc: 0.244 || Corr: -0.598\n",
      "Epoch 203 -> [Train] Loss: 1.032 Acc: 0.839 [Val] Loss: 18.110 Acc: 0.214 || Corr: -0.598\n",
      "Epoch 204 -> [Train] Loss: 0.951 Acc: 0.855 [Val] Loss: 16.528 Acc: 0.249 || Corr: -0.598\n",
      "Epoch 205 -> [Train] Loss: 0.834 Acc: 0.864 [Val] Loss: 16.235 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 206 -> [Train] Loss: 0.934 Acc: 0.848 [Val] Loss: 16.584 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 207 -> [Train] Loss: 0.873 Acc: 0.858 [Val] Loss: 14.785 Acc: 0.250 || Corr: -0.598\n",
      "Epoch 208 -> [Train] Loss: 0.824 Acc: 0.865 [Val] Loss: 15.685 Acc: 0.264 || Corr: -0.598\n",
      "Epoch 209 -> [Train] Loss: 0.848 Acc: 0.860 [Val] Loss: 15.805 Acc: 0.275 || Corr: -0.598\n",
      "Epoch 210 -> [Train] Loss: 0.835 Acc: 0.853 [Val] Loss: 16.996 Acc: 0.251 || Corr: -0.598\n",
      "Epoch 211 -> [Train] Loss: 0.777 Acc: 0.868 [Val] Loss: 14.050 Acc: 0.271 || Corr: -0.598\n",
      "Epoch 212 -> [Train] Loss: 0.876 Acc: 0.860 [Val] Loss: 16.767 Acc: 0.259 || Corr: -0.598\n",
      "Epoch 213 -> [Train] Loss: 0.861 Acc: 0.860 [Val] Loss: 15.399 Acc: 0.259 || Corr: -0.598\n",
      "Epoch 214 -> [Train] Loss: 0.699 Acc: 0.873 [Val] Loss: 16.804 Acc: 0.251 || Corr: -0.598\n",
      "Epoch 215 -> [Train] Loss: 0.721 Acc: 0.872 [Val] Loss: 17.431 Acc: 0.208 || Corr: -0.598\n",
      "Epoch 216 -> [Train] Loss: 0.765 Acc: 0.859 [Val] Loss: 16.496 Acc: 0.236 || Corr: -0.598\n",
      "Epoch 217 -> [Train] Loss: 0.770 Acc: 0.860 [Val] Loss: 17.454 Acc: 0.241 || Corr: -0.598\n",
      "Epoch 218 -> [Train] Loss: 0.861 Acc: 0.858 [Val] Loss: 15.083 Acc: 0.239 || Corr: -0.598\n",
      "Epoch 219 -> [Train] Loss: 0.773 Acc: 0.867 [Val] Loss: 13.943 Acc: 0.252 || Corr: -0.598\n",
      "Epoch 220 -> [Train] Loss: 0.745 Acc: 0.871 [Val] Loss: 13.065 Acc: 0.277 || Corr: -0.598\n",
      "Epoch 221 -> [Train] Loss: 0.719 Acc: 0.872 [Val] Loss: 15.606 Acc: 0.234 || Corr: -0.598\n",
      "Epoch 222 -> [Train] Loss: 0.789 Acc: 0.865 [Val] Loss: 15.486 Acc: 0.247 || Corr: -0.598\n",
      "Epoch 223 -> [Train] Loss: 0.711 Acc: 0.873 [Val] Loss: 16.730 Acc: 0.234 || Corr: -0.598\n",
      "Epoch 224 -> [Train] Loss: 0.976 Acc: 0.843 [Val] Loss: 18.468 Acc: 0.221 || Corr: -0.598\n",
      "Epoch 225 -> [Train] Loss: 0.893 Acc: 0.859 [Val] Loss: 17.403 Acc: 0.222 || Corr: -0.598\n",
      "Epoch 226 -> [Train] Loss: 1.112 Acc: 0.829 [Val] Loss: 20.801 Acc: 0.202 || Corr: -0.598\n",
      "Epoch 227 -> [Train] Loss: 0.815 Acc: 0.859 [Val] Loss: 20.465 Acc: 0.206 || Corr: -0.598\n",
      "Epoch 228 -> [Train] Loss: 0.956 Acc: 0.839 [Val] Loss: 18.124 Acc: 0.219 || Corr: -0.598\n",
      "Epoch 229 -> [Train] Loss: 0.872 Acc: 0.853 [Val] Loss: 14.927 Acc: 0.239 || Corr: -0.598\n",
      "Epoch 230 -> [Train] Loss: 0.951 Acc: 0.850 [Val] Loss: 17.626 Acc: 0.238 || Corr: -0.598\n",
      "Epoch 231 -> [Train] Loss: 1.119 Acc: 0.830 [Val] Loss: 19.539 Acc: 0.220 || Corr: -0.598\n",
      "Epoch 232 -> [Train] Loss: 1.221 Acc: 0.823 [Val] Loss: 15.793 Acc: 0.240 || Corr: -0.598\n",
      "Epoch 233 -> [Train] Loss: 1.326 Acc: 0.811 [Val] Loss: 20.372 Acc: 0.238 || Corr: -0.598\n",
      "Epoch 234 -> [Train] Loss: 1.388 Acc: 0.805 [Val] Loss: 16.205 Acc: 0.254 || Corr: -0.598\n",
      "Epoch 235 -> [Train] Loss: 1.625 Acc: 0.791 [Val] Loss: 18.408 Acc: 0.238 || Corr: -0.598\n",
      "Epoch 236 -> [Train] Loss: 1.749 Acc: 0.783 [Val] Loss: 18.124 Acc: 0.246 || Corr: -0.598\n",
      "Epoch 237 -> [Train] Loss: 1.477 Acc: 0.802 [Val] Loss: 21.344 Acc: 0.238 || Corr: -0.598\n",
      "Epoch 238 -> [Train] Loss: 1.778 Acc: 0.778 [Val] Loss: 19.999 Acc: 0.256 || Corr: -0.598\n",
      "Epoch 239 -> [Train] Loss: 1.854 Acc: 0.777 [Val] Loss: 20.627 Acc: 0.271 || Corr: -0.598\n",
      "Epoch 240 -> [Train] Loss: 1.947 Acc: 0.780 [Val] Loss: 20.458 Acc: 0.249 || Corr: -0.598\n",
      "Epoch 241 -> [Train] Loss: 1.740 Acc: 0.796 [Val] Loss: 18.372 Acc: 0.271 || Corr: -0.598\n",
      "Epoch 242 -> [Train] Loss: 1.608 Acc: 0.803 [Val] Loss: 18.986 Acc: 0.248 || Corr: -0.598\n",
      "Epoch 243 -> [Train] Loss: 1.767 Acc: 0.800 [Val] Loss: 20.311 Acc: 0.238 || Corr: -0.598\n",
      "Epoch 244 -> [Train] Loss: 1.778 Acc: 0.803 [Val] Loss: 20.331 Acc: 0.252 || Corr: -0.598\n",
      "Epoch 245 -> [Train] Loss: 1.895 Acc: 0.807 [Val] Loss: 20.997 Acc: 0.252 || Corr: -0.598\n",
      "Epoch 246 -> [Train] Loss: 1.950 Acc: 0.813 [Val] Loss: 21.716 Acc: 0.244 || Corr: -0.598\n",
      "Epoch 247 -> [Train] Loss: 1.470 Acc: 0.821 [Val] Loss: 21.720 Acc: 0.241 || Corr: -0.598\n",
      "Epoch 248 -> [Train] Loss: 1.541 Acc: 0.817 [Val] Loss: 22.853 Acc: 0.242 || Corr: -0.598\n",
      "Epoch 249 -> [Train] Loss: 1.460 Acc: 0.826 [Val] Loss: 23.318 Acc: 0.224 || Corr: -0.598\n",
      "Epoch 250 -> [Train] Loss: 1.615 Acc: 0.813 [Val] Loss: 21.491 Acc: 0.249 || Corr: -0.598\n",
      "Epoch 251 -> [Train] Loss: 1.655 Acc: 0.806 [Val] Loss: 21.407 Acc: 0.245 || Corr: -0.598\n",
      "Epoch 252 -> [Train] Loss: 1.356 Acc: 0.829 [Val] Loss: 23.106 Acc: 0.238 || Corr: -0.598\n",
      "Epoch 253 -> [Train] Loss: 1.535 Acc: 0.817 [Val] Loss: 21.689 Acc: 0.241 || Corr: -0.598\n",
      "Epoch 254 -> [Train] Loss: 1.882 Acc: 0.804 [Val] Loss: 21.095 Acc: 0.246 || Corr: -0.598\n",
      "Epoch 255 -> [Train] Loss: 1.777 Acc: 0.811 [Val] Loss: 19.250 Acc: 0.259 || Corr: -0.598\n",
      "Epoch 256 -> [Train] Loss: 1.352 Acc: 0.835 [Val] Loss: 20.186 Acc: 0.226 || Corr: -0.598\n",
      "Epoch 257 -> [Train] Loss: 1.309 Acc: 0.834 [Val] Loss: 20.586 Acc: 0.233 || Corr: -0.598\n",
      "Epoch 258 -> [Train] Loss: 1.266 Acc: 0.839 [Val] Loss: 20.021 Acc: 0.242 || Corr: -0.598\n",
      "Epoch 259 -> [Train] Loss: 1.285 Acc: 0.839 [Val] Loss: 20.819 Acc: 0.241 || Corr: -0.598\n",
      "Epoch 260 -> [Train] Loss: 1.365 Acc: 0.836 [Val] Loss: 20.556 Acc: 0.256 || Corr: -0.598\n",
      "Epoch 261 -> [Train] Loss: 1.329 Acc: 0.830 [Val] Loss: 21.445 Acc: 0.242 || Corr: -0.598\n",
      "Epoch 262 -> [Train] Loss: 1.244 Acc: 0.838 [Val] Loss: 22.153 Acc: 0.239 || Corr: -0.598\n",
      "Epoch 263 -> [Train] Loss: 1.336 Acc: 0.833 [Val] Loss: 22.213 Acc: 0.241 || Corr: -0.598\n",
      "Epoch 264 -> [Train] Loss: 1.487 Acc: 0.830 [Val] Loss: 21.206 Acc: 0.233 || Corr: -0.598\n",
      "Epoch 265 -> [Train] Loss: 1.801 Acc: 0.806 [Val] Loss: 19.040 Acc: 0.254 || Corr: -0.598\n",
      "Epoch 266 -> [Train] Loss: 1.548 Acc: 0.825 [Val] Loss: 19.502 Acc: 0.249 || Corr: -0.598\n",
      "Epoch 267 -> [Train] Loss: 1.437 Acc: 0.836 [Val] Loss: 17.870 Acc: 0.266 || Corr: -0.598\n",
      "Epoch 268 -> [Train] Loss: 1.184 Acc: 0.848 [Val] Loss: 20.962 Acc: 0.259 || Corr: -0.598\n",
      "Epoch 269 -> [Train] Loss: 1.116 Acc: 0.841 [Val] Loss: 21.335 Acc: 0.251 || Corr: -0.598\n",
      "Epoch 270 -> [Train] Loss: 1.305 Acc: 0.841 [Val] Loss: 18.494 Acc: 0.281 || Corr: -0.598\n",
      "Epoch 271 -> [Train] Loss: 1.400 Acc: 0.832 [Val] Loss: 19.114 Acc: 0.249 || Corr: -0.598\n",
      "Epoch 272 -> [Train] Loss: 1.413 Acc: 0.832 [Val] Loss: 20.285 Acc: 0.252 || Corr: -0.598\n",
      "Epoch 273 -> [Train] Loss: 1.482 Acc: 0.829 [Val] Loss: 23.224 Acc: 0.232 || Corr: -0.598\n",
      "Epoch 274 -> [Train] Loss: 1.980 Acc: 0.800 [Val] Loss: 20.556 Acc: 0.261 || Corr: -0.598\n",
      "Epoch 275 -> [Train] Loss: 1.774 Acc: 0.829 [Val] Loss: 21.538 Acc: 0.265 || Corr: -0.598\n",
      "Epoch 276 -> [Train] Loss: 1.430 Acc: 0.844 [Val] Loss: 19.761 Acc: 0.270 || Corr: -0.598\n",
      "Epoch 277 -> [Train] Loss: 1.368 Acc: 0.838 [Val] Loss: 20.156 Acc: 0.266 || Corr: -0.598\n",
      "Epoch 278 -> [Train] Loss: 1.402 Acc: 0.834 [Val] Loss: 19.905 Acc: 0.255 || Corr: -0.598\n",
      "Epoch 279 -> [Train] Loss: 1.613 Acc: 0.825 [Val] Loss: 19.143 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 280 -> [Train] Loss: 1.352 Acc: 0.839 [Val] Loss: 20.364 Acc: 0.255 || Corr: -0.598\n",
      "Epoch 281 -> [Train] Loss: 1.514 Acc: 0.833 [Val] Loss: 18.358 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 282 -> [Train] Loss: 1.543 Acc: 0.833 [Val] Loss: 18.557 Acc: 0.275 || Corr: -0.598\n",
      "Epoch 283 -> [Train] Loss: 1.612 Acc: 0.827 [Val] Loss: 16.534 Acc: 0.305 || Corr: -0.598\n",
      "Epoch 284 -> [Train] Loss: 1.494 Acc: 0.839 [Val] Loss: 20.551 Acc: 0.269 || Corr: -0.598\n",
      "Epoch 285 -> [Train] Loss: 1.388 Acc: 0.834 [Val] Loss: 18.524 Acc: 0.293 || Corr: -0.598\n",
      "Epoch 286 -> [Train] Loss: 1.455 Acc: 0.838 [Val] Loss: 20.336 Acc: 0.269 || Corr: -0.598\n",
      "Epoch 287 -> [Train] Loss: 1.287 Acc: 0.843 [Val] Loss: 21.559 Acc: 0.285 || Corr: -0.598\n",
      "Epoch 288 -> [Train] Loss: 1.320 Acc: 0.842 [Val] Loss: 22.154 Acc: 0.276 || Corr: -0.598\n",
      "Epoch 289 -> [Train] Loss: 1.124 Acc: 0.856 [Val] Loss: 22.538 Acc: 0.259 || Corr: -0.598\n",
      "Epoch 290 -> [Train] Loss: 1.046 Acc: 0.865 [Val] Loss: 20.637 Acc: 0.261 || Corr: -0.598\n",
      "Epoch 291 -> [Train] Loss: 1.117 Acc: 0.857 [Val] Loss: 23.093 Acc: 0.224 || Corr: -0.598\n",
      "Epoch 292 -> [Train] Loss: 1.198 Acc: 0.850 [Val] Loss: 21.752 Acc: 0.239 || Corr: -0.598\n",
      "Epoch 293 -> [Train] Loss: 1.295 Acc: 0.846 [Val] Loss: 21.848 Acc: 0.238 || Corr: -0.598\n",
      "Epoch 294 -> [Train] Loss: 1.134 Acc: 0.854 [Val] Loss: 22.053 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 295 -> [Train] Loss: 1.162 Acc: 0.849 [Val] Loss: 21.538 Acc: 0.262 || Corr: -0.598\n",
      "Epoch 296 -> [Train] Loss: 1.185 Acc: 0.848 [Val] Loss: 21.822 Acc: 0.261 || Corr: -0.598\n",
      "Epoch 297 -> [Train] Loss: 1.185 Acc: 0.848 [Val] Loss: 23.952 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 298 -> [Train] Loss: 1.184 Acc: 0.856 [Val] Loss: 21.084 Acc: 0.268 || Corr: -0.598\n",
      "Epoch 299 -> [Train] Loss: 1.323 Acc: 0.852 [Val] Loss: 22.436 Acc: 0.265 || Corr: -0.598\n",
      "Epoch 300 -> [Train] Loss: 1.443 Acc: 0.836 [Val] Loss: 20.210 Acc: 0.271 || Corr: -0.598\n",
      "Epoch 301 -> [Train] Loss: 1.341 Acc: 0.843 [Val] Loss: 19.433 Acc: 0.262 || Corr: -0.598\n",
      "Epoch 302 -> [Train] Loss: 1.468 Acc: 0.839 [Val] Loss: 19.288 Acc: 0.263 || Corr: -0.598\n",
      "Epoch 303 -> [Train] Loss: 1.476 Acc: 0.834 [Val] Loss: 21.360 Acc: 0.251 || Corr: -0.598\n",
      "Epoch 304 -> [Train] Loss: 1.351 Acc: 0.844 [Val] Loss: 24.453 Acc: 0.238 || Corr: -0.598\n",
      "Epoch 305 -> [Train] Loss: 1.496 Acc: 0.842 [Val] Loss: 22.320 Acc: 0.242 || Corr: -0.598\n",
      "Epoch 306 -> [Train] Loss: 1.300 Acc: 0.853 [Val] Loss: 19.776 Acc: 0.271 || Corr: -0.598\n",
      "Epoch 307 -> [Train] Loss: 0.912 Acc: 0.876 [Val] Loss: 17.919 Acc: 0.256 || Corr: -0.598\n",
      "Epoch 308 -> [Train] Loss: 1.070 Acc: 0.864 [Val] Loss: 18.857 Acc: 0.258 || Corr: -0.598\n",
      "Epoch 309 -> [Train] Loss: 0.741 Acc: 0.896 [Val] Loss: 23.785 Acc: 0.240 || Corr: -0.598\n",
      "Epoch 310 -> [Train] Loss: 0.722 Acc: 0.895 [Val] Loss: 21.315 Acc: 0.261 || Corr: -0.598\n",
      "Epoch 311 -> [Train] Loss: 0.848 Acc: 0.881 [Val] Loss: 19.070 Acc: 0.269 || Corr: -0.598\n",
      "Epoch 312 -> [Train] Loss: 0.660 Acc: 0.901 [Val] Loss: 22.818 Acc: 0.215 || Corr: -0.598\n",
      "Epoch 313 -> [Train] Loss: 0.751 Acc: 0.888 [Val] Loss: 20.849 Acc: 0.261 || Corr: -0.598\n",
      "Epoch 314 -> [Train] Loss: 0.669 Acc: 0.893 [Val] Loss: 18.280 Acc: 0.289 || Corr: -0.598\n",
      "Epoch 315 -> [Train] Loss: 0.625 Acc: 0.900 [Val] Loss: 17.056 Acc: 0.278 || Corr: -0.598\n",
      "Epoch 316 -> [Train] Loss: 0.540 Acc: 0.910 [Val] Loss: 21.268 Acc: 0.257 || Corr: -0.598\n",
      "Epoch 317 -> [Train] Loss: 0.538 Acc: 0.905 [Val] Loss: 21.055 Acc: 0.270 || Corr: -0.598\n",
      "Epoch 318 -> [Train] Loss: 0.553 Acc: 0.906 [Val] Loss: 17.565 Acc: 0.270 || Corr: -0.598\n",
      "Epoch 319 -> [Train] Loss: 0.576 Acc: 0.906 [Val] Loss: 22.090 Acc: 0.266 || Corr: -0.598\n",
      "Epoch 320 -> [Train] Loss: 0.538 Acc: 0.909 [Val] Loss: 21.984 Acc: 0.264 || Corr: -0.598\n",
      "Epoch 321 -> [Train] Loss: 0.477 Acc: 0.912 [Val] Loss: 23.706 Acc: 0.251 || Corr: -0.598\n",
      "Epoch 322 -> [Train] Loss: 0.590 Acc: 0.907 [Val] Loss: 19.293 Acc: 0.270 || Corr: -0.598\n",
      "Epoch 323 -> [Train] Loss: 0.528 Acc: 0.907 [Val] Loss: 19.768 Acc: 0.279 || Corr: -0.598\n",
      "Epoch 324 -> [Train] Loss: 0.567 Acc: 0.909 [Val] Loss: 18.356 Acc: 0.270 || Corr: -0.598\n",
      "Epoch 325 -> [Train] Loss: 0.684 Acc: 0.896 [Val] Loss: 20.278 Acc: 0.249 || Corr: -0.598\n",
      "Epoch 326 -> [Train] Loss: 0.494 Acc: 0.911 [Val] Loss: 21.788 Acc: 0.254 || Corr: -0.598\n",
      "Epoch 327 -> [Train] Loss: 0.616 Acc: 0.896 [Val] Loss: 19.387 Acc: 0.266 || Corr: -0.598\n",
      "Epoch 328 -> [Train] Loss: 0.573 Acc: 0.905 [Val] Loss: 23.235 Acc: 0.250 || Corr: -0.598\n",
      "Epoch 329 -> [Train] Loss: 0.463 Acc: 0.918 [Val] Loss: 18.886 Acc: 0.270 || Corr: -0.598\n",
      "Epoch 330 -> [Train] Loss: 0.479 Acc: 0.915 [Val] Loss: 23.931 Acc: 0.242 || Corr: -0.598\n",
      "Epoch 331 -> [Train] Loss: 0.887 Acc: 0.888 [Val] Loss: 19.365 Acc: 0.270 || Corr: -0.598\n",
      "Epoch 332 -> [Train] Loss: 0.713 Acc: 0.892 [Val] Loss: 19.117 Acc: 0.273 || Corr: -0.598\n",
      "Epoch 333 -> [Train] Loss: 0.822 Acc: 0.882 [Val] Loss: 22.456 Acc: 0.254 || Corr: -0.598\n",
      "Epoch 334 -> [Train] Loss: 0.479 Acc: 0.923 [Val] Loss: 22.036 Acc: 0.264 || Corr: -0.598\n",
      "Epoch 335 -> [Train] Loss: 0.483 Acc: 0.919 [Val] Loss: 17.936 Acc: 0.273 || Corr: -0.598\n",
      "Epoch 336 -> [Train] Loss: 0.681 Acc: 0.898 [Val] Loss: 18.482 Acc: 0.286 || Corr: -0.598\n",
      "Epoch 337 -> [Train] Loss: 0.546 Acc: 0.914 [Val] Loss: 18.413 Acc: 0.258 || Corr: -0.598\n",
      "Epoch 338 -> [Train] Loss: 0.642 Acc: 0.903 [Val] Loss: 19.417 Acc: 0.267 || Corr: -0.598\n",
      "Epoch 339 -> [Train] Loss: 0.627 Acc: 0.901 [Val] Loss: 19.961 Acc: 0.256 || Corr: -0.598\n",
      "Epoch 340 -> [Train] Loss: 0.518 Acc: 0.917 [Val] Loss: 19.840 Acc: 0.273 || Corr: -0.598\n",
      "Epoch 341 -> [Train] Loss: 0.609 Acc: 0.912 [Val] Loss: 18.363 Acc: 0.284 || Corr: -0.598\n",
      "Epoch 342 -> [Train] Loss: 0.602 Acc: 0.900 [Val] Loss: 21.956 Acc: 0.249 || Corr: -0.598\n",
      "Epoch 343 -> [Train] Loss: 0.477 Acc: 0.914 [Val] Loss: 21.875 Acc: 0.250 || Corr: -0.598\n",
      "Epoch 344 -> [Train] Loss: 0.556 Acc: 0.913 [Val] Loss: 20.626 Acc: 0.251 || Corr: -0.598\n",
      "Epoch 345 -> [Train] Loss: 0.569 Acc: 0.910 [Val] Loss: 21.578 Acc: 0.274 || Corr: -0.598\n",
      "Epoch 346 -> [Train] Loss: 0.504 Acc: 0.917 [Val] Loss: 22.108 Acc: 0.239 || Corr: -0.598\n",
      "Epoch 347 -> [Train] Loss: 0.480 Acc: 0.920 [Val] Loss: 21.982 Acc: 0.248 || Corr: -0.598\n",
      "Epoch 348 -> [Train] Loss: 0.463 Acc: 0.919 [Val] Loss: 22.851 Acc: 0.240 || Corr: -0.598\n",
      "Epoch 349 -> [Train] Loss: 0.554 Acc: 0.907 [Val] Loss: 22.544 Acc: 0.255 || Corr: -0.598\n",
      "Epoch 350 -> [Train] Loss: 0.637 Acc: 0.902 [Val] Loss: 21.769 Acc: 0.230 || Corr: -0.598\n",
      "Epoch 351 -> [Train] Loss: 0.673 Acc: 0.898 [Val] Loss: 23.061 Acc: 0.259 || Corr: -0.598\n",
      "Epoch 352 -> [Train] Loss: 0.639 Acc: 0.904 [Val] Loss: 24.852 Acc: 0.225 || Corr: -0.598\n",
      "Epoch 353 -> [Train] Loss: 0.615 Acc: 0.904 [Val] Loss: 24.189 Acc: 0.244 || Corr: -0.598\n",
      "Epoch 354 -> [Train] Loss: 0.679 Acc: 0.897 [Val] Loss: 22.269 Acc: 0.243 || Corr: -0.598\n",
      "Epoch 355 -> [Train] Loss: 0.524 Acc: 0.912 [Val] Loss: 20.451 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 356 -> [Train] Loss: 0.585 Acc: 0.905 [Val] Loss: 20.221 Acc: 0.250 || Corr: -0.598\n",
      "Epoch 357 -> [Train] Loss: 0.570 Acc: 0.905 [Val] Loss: 18.539 Acc: 0.262 || Corr: -0.598\n",
      "Epoch 358 -> [Train] Loss: 0.748 Acc: 0.891 [Val] Loss: 19.328 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 359 -> [Train] Loss: 0.725 Acc: 0.886 [Val] Loss: 18.275 Acc: 0.270 || Corr: -0.598\n",
      "Epoch 360 -> [Train] Loss: 0.569 Acc: 0.912 [Val] Loss: 19.153 Acc: 0.241 || Corr: -0.598\n",
      "Epoch 361 -> [Train] Loss: 0.810 Acc: 0.889 [Val] Loss: 20.099 Acc: 0.246 || Corr: -0.598\n",
      "Epoch 362 -> [Train] Loss: 0.787 Acc: 0.896 [Val] Loss: 21.781 Acc: 0.222 || Corr: -0.598\n",
      "Epoch 363 -> [Train] Loss: 0.863 Acc: 0.888 [Val] Loss: 21.939 Acc: 0.230 || Corr: -0.598\n",
      "Epoch 364 -> [Train] Loss: 0.697 Acc: 0.897 [Val] Loss: 22.069 Acc: 0.224 || Corr: -0.598\n",
      "Epoch 365 -> [Train] Loss: 0.700 Acc: 0.903 [Val] Loss: 24.648 Acc: 0.214 || Corr: -0.598\n",
      "Epoch 366 -> [Train] Loss: 0.727 Acc: 0.900 [Val] Loss: 22.115 Acc: 0.231 || Corr: -0.598\n",
      "Epoch 367 -> [Train] Loss: 0.702 Acc: 0.901 [Val] Loss: 24.136 Acc: 0.206 || Corr: -0.598\n",
      "Epoch 368 -> [Train] Loss: 0.702 Acc: 0.897 [Val] Loss: 23.110 Acc: 0.220 || Corr: -0.598\n",
      "Epoch 369 -> [Train] Loss: 0.752 Acc: 0.893 [Val] Loss: 24.530 Acc: 0.213 || Corr: -0.598\n",
      "Epoch 370 -> [Train] Loss: 0.675 Acc: 0.902 [Val] Loss: 20.764 Acc: 0.235 || Corr: -0.598\n",
      "Epoch 371 -> [Train] Loss: 0.653 Acc: 0.905 [Val] Loss: 24.083 Acc: 0.215 || Corr: -0.598\n",
      "Epoch 372 -> [Train] Loss: 0.617 Acc: 0.910 [Val] Loss: 24.845 Acc: 0.202 || Corr: -0.598\n",
      "Epoch 373 -> [Train] Loss: 0.664 Acc: 0.905 [Val] Loss: 24.969 Acc: 0.209 || Corr: -0.598\n",
      "Epoch 374 -> [Train] Loss: 0.620 Acc: 0.907 [Val] Loss: 30.597 Acc: 0.174 || Corr: -0.598\n",
      "Epoch 375 -> [Train] Loss: 0.737 Acc: 0.901 [Val] Loss: 27.769 Acc: 0.197 || Corr: -0.598\n",
      "Epoch 376 -> [Train] Loss: 0.889 Acc: 0.883 [Val] Loss: 25.159 Acc: 0.197 || Corr: -0.598\n",
      "Epoch 377 -> [Train] Loss: 0.645 Acc: 0.907 [Val] Loss: 20.083 Acc: 0.245 || Corr: -0.598\n",
      "Epoch 378 -> [Train] Loss: 0.643 Acc: 0.907 [Val] Loss: 17.840 Acc: 0.273 || Corr: -0.598\n",
      "Epoch 379 -> [Train] Loss: 0.666 Acc: 0.904 [Val] Loss: 20.591 Acc: 0.235 || Corr: -0.598\n",
      "Epoch 380 -> [Train] Loss: 0.785 Acc: 0.890 [Val] Loss: 23.832 Acc: 0.221 || Corr: -0.598\n",
      "Epoch 381 -> [Train] Loss: 0.689 Acc: 0.899 [Val] Loss: 21.154 Acc: 0.240 || Corr: -0.598\n",
      "Epoch 382 -> [Train] Loss: 0.771 Acc: 0.901 [Val] Loss: 19.641 Acc: 0.256 || Corr: -0.598\n",
      "Epoch 383 -> [Train] Loss: 0.701 Acc: 0.910 [Val] Loss: 18.270 Acc: 0.273 || Corr: -0.598\n",
      "Epoch 384 -> [Train] Loss: 0.711 Acc: 0.903 [Val] Loss: 20.600 Acc: 0.246 || Corr: -0.598\n",
      "Epoch 385 -> [Train] Loss: 0.720 Acc: 0.908 [Val] Loss: 20.272 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 386 -> [Train] Loss: 1.085 Acc: 0.885 [Val] Loss: 19.404 Acc: 0.268 || Corr: -0.598\n",
      "Epoch 387 -> [Train] Loss: 0.844 Acc: 0.898 [Val] Loss: 20.687 Acc: 0.275 || Corr: -0.598\n",
      "Epoch 388 -> [Train] Loss: 0.889 Acc: 0.891 [Val] Loss: 18.452 Acc: 0.272 || Corr: -0.598\n",
      "Epoch 389 -> [Train] Loss: 1.066 Acc: 0.877 [Val] Loss: 20.066 Acc: 0.249 || Corr: -0.598\n",
      "Epoch 390 -> [Train] Loss: 0.887 Acc: 0.891 [Val] Loss: 21.697 Acc: 0.226 || Corr: -0.598\n",
      "Epoch 391 -> [Train] Loss: 1.294 Acc: 0.873 [Val] Loss: 21.290 Acc: 0.249 || Corr: -0.598\n",
      "Epoch 392 -> [Train] Loss: 1.182 Acc: 0.868 [Val] Loss: 21.232 Acc: 0.254 || Corr: -0.598\n",
      "Epoch 393 -> [Train] Loss: 1.186 Acc: 0.870 [Val] Loss: 20.968 Acc: 0.250 || Corr: -0.598\n",
      "Epoch 394 -> [Train] Loss: 1.140 Acc: 0.880 [Val] Loss: 20.165 Acc: 0.259 || Corr: -0.598\n",
      "Epoch 395 -> [Train] Loss: 0.976 Acc: 0.888 [Val] Loss: 18.780 Acc: 0.260 || Corr: -0.598\n",
      "Epoch 396 -> [Train] Loss: 1.255 Acc: 0.864 [Val] Loss: 20.798 Acc: 0.250 || Corr: -0.598\n",
      "Epoch 397 -> [Train] Loss: 0.949 Acc: 0.887 [Val] Loss: 18.829 Acc: 0.288 || Corr: -0.598\n",
      "Epoch 398 -> [Train] Loss: 0.981 Acc: 0.883 [Val] Loss: 20.694 Acc: 0.252 || Corr: -0.598\n",
      "Epoch 399 -> [Train] Loss: 0.869 Acc: 0.884 [Val] Loss: 20.734 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 400 -> [Train] Loss: 0.988 Acc: 0.871 [Val] Loss: 20.645 Acc: 0.250 || Corr: -0.598\n",
      "Epoch 401 -> [Train] Loss: 1.093 Acc: 0.868 [Val] Loss: 20.999 Acc: 0.256 || Corr: -0.598\n",
      "Epoch 402 -> [Train] Loss: 1.138 Acc: 0.867 [Val] Loss: 25.579 Acc: 0.219 || Corr: -0.598\n",
      "Epoch 403 -> [Train] Loss: 0.996 Acc: 0.872 [Val] Loss: 21.608 Acc: 0.254 || Corr: -0.598\n",
      "Epoch 404 -> [Train] Loss: 1.082 Acc: 0.873 [Val] Loss: 23.375 Acc: 0.233 || Corr: -0.598\n",
      "Epoch 405 -> [Train] Loss: 0.965 Acc: 0.880 [Val] Loss: 22.934 Acc: 0.233 || Corr: -0.598\n",
      "Epoch 406 -> [Train] Loss: 1.333 Acc: 0.860 [Val] Loss: 19.049 Acc: 0.269 || Corr: -0.598\n",
      "Epoch 407 -> [Train] Loss: 1.187 Acc: 0.873 [Val] Loss: 19.699 Acc: 0.263 || Corr: -0.598\n",
      "Epoch 408 -> [Train] Loss: 0.970 Acc: 0.883 [Val] Loss: 21.339 Acc: 0.257 || Corr: -0.598\n",
      "Epoch 409 -> [Train] Loss: 1.100 Acc: 0.880 [Val] Loss: 20.055 Acc: 0.260 || Corr: -0.598\n",
      "Epoch 410 -> [Train] Loss: 1.423 Acc: 0.873 [Val] Loss: 19.877 Acc: 0.273 || Corr: -0.598\n",
      "Epoch 411 -> [Train] Loss: 1.686 Acc: 0.862 [Val] Loss: 20.150 Acc: 0.263 || Corr: -0.598\n",
      "Epoch 412 -> [Train] Loss: 1.400 Acc: 0.883 [Val] Loss: 21.156 Acc: 0.256 || Corr: -0.598\n",
      "Epoch 413 -> [Train] Loss: 1.195 Acc: 0.891 [Val] Loss: 21.100 Acc: 0.246 || Corr: -0.598\n",
      "Epoch 414 -> [Train] Loss: 1.095 Acc: 0.889 [Val] Loss: 22.678 Acc: 0.254 || Corr: -0.598\n",
      "Epoch 415 -> [Train] Loss: 1.175 Acc: 0.890 [Val] Loss: 25.194 Acc: 0.231 || Corr: -0.598\n",
      "Epoch 416 -> [Train] Loss: 1.167 Acc: 0.880 [Val] Loss: 23.729 Acc: 0.236 || Corr: -0.598\n",
      "Epoch 417 -> [Train] Loss: 1.107 Acc: 0.892 [Val] Loss: 20.331 Acc: 0.257 || Corr: -0.598\n",
      "Epoch 418 -> [Train] Loss: 1.192 Acc: 0.875 [Val] Loss: 24.381 Acc: 0.232 || Corr: -0.598\n",
      "Epoch 419 -> [Train] Loss: 0.987 Acc: 0.888 [Val] Loss: 19.918 Acc: 0.254 || Corr: -0.598\n",
      "Epoch 420 -> [Train] Loss: 0.900 Acc: 0.894 [Val] Loss: 22.013 Acc: 0.249 || Corr: -0.598\n",
      "Epoch 421 -> [Train] Loss: 1.182 Acc: 0.875 [Val] Loss: 23.173 Acc: 0.247 || Corr: -0.598\n",
      "Epoch 422 -> [Train] Loss: 1.067 Acc: 0.886 [Val] Loss: 23.512 Acc: 0.245 || Corr: -0.598\n",
      "Epoch 423 -> [Train] Loss: 1.248 Acc: 0.874 [Val] Loss: 20.963 Acc: 0.265 || Corr: -0.598\n",
      "Epoch 424 -> [Train] Loss: 1.238 Acc: 0.876 [Val] Loss: 21.095 Acc: 0.263 || Corr: -0.598\n",
      "Epoch 425 -> [Train] Loss: 1.639 Acc: 0.864 [Val] Loss: 21.735 Acc: 0.270 || Corr: -0.598\n",
      "Epoch 426 -> [Train] Loss: 1.193 Acc: 0.872 [Val] Loss: 23.785 Acc: 0.250 || Corr: -0.598\n",
      "Epoch 427 -> [Train] Loss: 1.030 Acc: 0.888 [Val] Loss: 22.417 Acc: 0.247 || Corr: -0.598\n",
      "Epoch 428 -> [Train] Loss: 1.111 Acc: 0.882 [Val] Loss: 21.930 Acc: 0.258 || Corr: -0.598\n",
      "Epoch 429 -> [Train] Loss: 0.987 Acc: 0.889 [Val] Loss: 22.036 Acc: 0.252 || Corr: -0.598\n",
      "Epoch 430 -> [Train] Loss: 0.949 Acc: 0.891 [Val] Loss: 23.994 Acc: 0.246 || Corr: -0.598\n",
      "Epoch 431 -> [Train] Loss: 0.756 Acc: 0.905 [Val] Loss: 22.827 Acc: 0.250 || Corr: -0.598\n",
      "Epoch 432 -> [Train] Loss: 0.793 Acc: 0.893 [Val] Loss: 26.037 Acc: 0.224 || Corr: -0.598\n",
      "Epoch 433 -> [Train] Loss: 0.673 Acc: 0.907 [Val] Loss: 22.989 Acc: 0.245 || Corr: -0.598\n",
      "Epoch 434 -> [Train] Loss: 0.792 Acc: 0.899 [Val] Loss: 21.706 Acc: 0.259 || Corr: -0.598\n",
      "Epoch 435 -> [Train] Loss: 0.727 Acc: 0.908 [Val] Loss: 24.030 Acc: 0.250 || Corr: -0.598\n",
      "Epoch 436 -> [Train] Loss: 0.700 Acc: 0.905 [Val] Loss: 22.167 Acc: 0.266 || Corr: -0.598\n",
      "Epoch 437 -> [Train] Loss: 0.821 Acc: 0.902 [Val] Loss: 22.129 Acc: 0.262 || Corr: -0.598\n",
      "Epoch 438 -> [Train] Loss: 0.737 Acc: 0.903 [Val] Loss: 23.116 Acc: 0.254 || Corr: -0.598\n",
      "Epoch 439 -> [Train] Loss: 0.849 Acc: 0.894 [Val] Loss: 20.419 Acc: 0.282 || Corr: -0.598\n",
      "Epoch 440 -> [Train] Loss: 0.899 Acc: 0.895 [Val] Loss: 21.378 Acc: 0.262 || Corr: -0.598\n",
      "Epoch 441 -> [Train] Loss: 0.868 Acc: 0.897 [Val] Loss: 21.803 Acc: 0.267 || Corr: -0.598\n",
      "Epoch 442 -> [Train] Loss: 0.947 Acc: 0.882 [Val] Loss: 23.232 Acc: 0.265 || Corr: -0.598\n",
      "Epoch 443 -> [Train] Loss: 0.897 Acc: 0.886 [Val] Loss: 21.156 Acc: 0.279 || Corr: -0.598\n",
      "Epoch 444 -> [Train] Loss: 0.892 Acc: 0.885 [Val] Loss: 21.974 Acc: 0.286 || Corr: -0.598\n",
      "Epoch 445 -> [Train] Loss: 0.784 Acc: 0.895 [Val] Loss: 22.218 Acc: 0.276 || Corr: -0.598\n",
      "Epoch 446 -> [Train] Loss: 0.921 Acc: 0.894 [Val] Loss: 23.136 Acc: 0.286 || Corr: -0.598\n",
      "Epoch 447 -> [Train] Loss: 0.822 Acc: 0.893 [Val] Loss: 26.002 Acc: 0.266 || Corr: -0.598\n",
      "Epoch 448 -> [Train] Loss: 0.757 Acc: 0.897 [Val] Loss: 23.143 Acc: 0.296 || Corr: -0.598\n",
      "Epoch 449 -> [Train] Loss: 0.623 Acc: 0.910 [Val] Loss: 26.793 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 450 -> [Train] Loss: 0.663 Acc: 0.905 [Val] Loss: 25.674 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 451 -> [Train] Loss: 0.705 Acc: 0.907 [Val] Loss: 24.227 Acc: 0.274 || Corr: -0.598\n",
      "Epoch 452 -> [Train] Loss: 0.732 Acc: 0.898 [Val] Loss: 26.742 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 453 -> [Train] Loss: 0.731 Acc: 0.908 [Val] Loss: 25.249 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 454 -> [Train] Loss: 0.808 Acc: 0.901 [Val] Loss: 26.790 Acc: 0.258 || Corr: -0.598\n",
      "Epoch 455 -> [Train] Loss: 0.658 Acc: 0.913 [Val] Loss: 26.407 Acc: 0.258 || Corr: -0.598\n",
      "Epoch 456 -> [Train] Loss: 0.746 Acc: 0.903 [Val] Loss: 26.768 Acc: 0.251 || Corr: -0.598\n",
      "Epoch 457 -> [Train] Loss: 0.861 Acc: 0.900 [Val] Loss: 28.371 Acc: 0.238 || Corr: -0.598\n",
      "Epoch 458 -> [Train] Loss: 0.781 Acc: 0.904 [Val] Loss: 27.879 Acc: 0.233 || Corr: -0.598\n",
      "Epoch 459 -> [Train] Loss: 0.787 Acc: 0.907 [Val] Loss: 24.877 Acc: 0.259 || Corr: -0.598\n",
      "Epoch 460 -> [Train] Loss: 0.532 Acc: 0.922 [Val] Loss: 26.111 Acc: 0.229 || Corr: -0.598\n",
      "Epoch 461 -> [Train] Loss: 0.464 Acc: 0.928 [Val] Loss: 24.217 Acc: 0.242 || Corr: -0.598\n",
      "Epoch 462 -> [Train] Loss: 0.422 Acc: 0.935 [Val] Loss: 27.418 Acc: 0.219 || Corr: -0.598\n",
      "Epoch 463 -> [Train] Loss: 0.416 Acc: 0.937 [Val] Loss: 27.369 Acc: 0.234 || Corr: -0.598\n",
      "Epoch 464 -> [Train] Loss: 0.509 Acc: 0.932 [Val] Loss: 25.942 Acc: 0.244 || Corr: -0.598\n",
      "Epoch 465 -> [Train] Loss: 0.489 Acc: 0.933 [Val] Loss: 23.584 Acc: 0.244 || Corr: -0.598\n",
      "Epoch 466 -> [Train] Loss: 0.492 Acc: 0.928 [Val] Loss: 24.345 Acc: 0.256 || Corr: -0.598\n",
      "Epoch 467 -> [Train] Loss: 0.505 Acc: 0.930 [Val] Loss: 24.079 Acc: 0.256 || Corr: -0.598\n",
      "Epoch 468 -> [Train] Loss: 0.475 Acc: 0.926 [Val] Loss: 25.412 Acc: 0.249 || Corr: -0.598\n",
      "Epoch 469 -> [Train] Loss: 0.363 Acc: 0.940 [Val] Loss: 25.499 Acc: 0.255 || Corr: -0.598\n",
      "Epoch 470 -> [Train] Loss: 0.411 Acc: 0.934 [Val] Loss: 23.865 Acc: 0.253 || Corr: -0.598\n",
      "Epoch 471 -> [Train] Loss: 0.429 Acc: 0.933 [Val] Loss: 27.800 Acc: 0.256 || Corr: -0.598\n",
      "Epoch 472 -> [Train] Loss: 0.451 Acc: 0.929 [Val] Loss: 27.158 Acc: 0.218 || Corr: -0.598\n",
      "Epoch 473 -> [Train] Loss: 0.374 Acc: 0.941 [Val] Loss: 29.359 Acc: 0.212 || Corr: -0.598\n",
      "Epoch 474 -> [Train] Loss: 0.449 Acc: 0.930 [Val] Loss: 25.865 Acc: 0.234 || Corr: -0.598\n",
      "Epoch 475 -> [Train] Loss: 1.051 Acc: 0.913 [Val] Loss: 27.506 Acc: 0.231 || Corr: -0.598\n",
      "Epoch 476 -> [Train] Loss: 0.420 Acc: 0.935 [Val] Loss: 28.350 Acc: 0.222 || Corr: -0.598\n",
      "Epoch 477 -> [Train] Loss: 0.481 Acc: 0.928 [Val] Loss: 26.397 Acc: 0.241 || Corr: -0.598\n",
      "Epoch 478 -> [Train] Loss: 0.517 Acc: 0.925 [Val] Loss: 26.432 Acc: 0.252 || Corr: -0.598\n",
      "Epoch 479 -> [Train] Loss: 0.327 Acc: 0.939 [Val] Loss: 29.365 Acc: 0.225 || Corr: -0.598\n",
      "Epoch 480 -> [Train] Loss: 0.609 Acc: 0.917 [Val] Loss: 25.171 Acc: 0.242 || Corr: -0.598\n",
      "Epoch 481 -> [Train] Loss: 0.373 Acc: 0.940 [Val] Loss: 26.419 Acc: 0.231 || Corr: -0.598\n",
      "Epoch 482 -> [Train] Loss: 0.452 Acc: 0.933 [Val] Loss: 25.128 Acc: 0.251 || Corr: -0.598\n",
      "Epoch 483 -> [Train] Loss: 0.389 Acc: 0.942 [Val] Loss: 27.383 Acc: 0.215 || Corr: -0.598\n",
      "Epoch 484 -> [Train] Loss: 0.299 Acc: 0.946 [Val] Loss: 26.080 Acc: 0.225 || Corr: -0.598\n",
      "Epoch 485 -> [Train] Loss: 0.401 Acc: 0.939 [Val] Loss: 24.693 Acc: 0.229 || Corr: -0.598\n",
      "Epoch 486 -> [Train] Loss: 0.357 Acc: 0.943 [Val] Loss: 26.452 Acc: 0.229 || Corr: -0.598\n",
      "Epoch 487 -> [Train] Loss: 0.354 Acc: 0.942 [Val] Loss: 23.388 Acc: 0.250 || Corr: -0.598\n",
      "Epoch 488 -> [Train] Loss: 0.312 Acc: 0.949 [Val] Loss: 27.660 Acc: 0.212 || Corr: -0.598\n",
      "Epoch 489 -> [Train] Loss: 0.354 Acc: 0.940 [Val] Loss: 22.178 Acc: 0.257 || Corr: -0.598\n",
      "Epoch 490 -> [Train] Loss: 0.371 Acc: 0.937 [Val] Loss: 24.693 Acc: 0.237 || Corr: -0.598\n",
      "Epoch 491 -> [Train] Loss: 0.363 Acc: 0.941 [Val] Loss: 27.427 Acc: 0.230 || Corr: -0.598\n",
      "Epoch 492 -> [Train] Loss: 0.322 Acc: 0.942 [Val] Loss: 27.411 Acc: 0.256 || Corr: -0.598\n",
      "Epoch 493 -> [Train] Loss: 0.419 Acc: 0.929 [Val] Loss: 24.467 Acc: 0.236 || Corr: -0.598\n",
      "Epoch 494 -> [Train] Loss: 0.436 Acc: 0.929 [Val] Loss: 24.531 Acc: 0.254 || Corr: -0.598\n",
      "Epoch 495 -> [Train] Loss: 0.313 Acc: 0.947 [Val] Loss: 24.716 Acc: 0.257 || Corr: -0.598\n",
      "Epoch 496 -> [Train] Loss: 0.358 Acc: 0.941 [Val] Loss: 29.203 Acc: 0.230 || Corr: -0.598\n",
      "Epoch 497 -> [Train] Loss: 0.481 Acc: 0.929 [Val] Loss: 26.094 Acc: 0.258 || Corr: -0.598\n",
      "Epoch 498 -> [Train] Loss: 0.379 Acc: 0.938 [Val] Loss: 27.735 Acc: 0.240 || Corr: -0.598\n",
      "Epoch 499 -> [Train] Loss: 0.391 Acc: 0.936 [Val] Loss: 23.838 Acc: 0.265 || Corr: -0.598\n",
      "CPU times: user 7h 29min, sys: 2h 35min 49s, total: 10h 4min 49s\n",
      "Wall time: 57min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(config.epochs):\n",
    "    ## Training\n",
    "    for batch in dst_train_rdy.as_numpy_iterator():\n",
    "        new_state = train_step(state, batch)\n",
    "        new_state = new_state.replace(params=clip_layer(new_state.params, \"GDN\", a_min=0))\n",
    "        params_diff = jax.tree_map(lambda x, y: jnp.mean((x-y)**2), state.params, new_state.params)\n",
    "        state = new_state\n",
    "        wandb.log(unfreeze(params_diff), commit=False)\n",
    "        # state = compute_metrics(state=state, batch=batch)\n",
    "        # break\n",
    "\n",
    "    ## Log the metrics\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"train_{name}\"].append(value)\n",
    "    \n",
    "    ## Empty the metrics\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "    ## Evaluation (Classification)\n",
    "    for batch in dst_val_rdy.as_numpy_iterator():\n",
    "        state = val_step(state=state, batch=batch)\n",
    "        # break\n",
    "    for name, value in state.metrics.compute().items():\n",
    "        metrics_history[f\"val_{name}\"].append(value)\n",
    "    state = state.replace(metrics=state.metrics.empty())\n",
    "\n",
    "    ## Evaluation (Correlation)\n",
    "    correlation = obtain_correlation(state, dst_tid2013.as_numpy_iterator())\n",
    "    metrics_history[\"correlation\"].append(correlation)\n",
    "    \n",
    "    ## Checkpointing\n",
    "    if metrics_history[\"val_loss\"][-1] <= min(metrics_history[\"val_loss\"]):\n",
    "        orbax_checkpointer.save(os.path.join(wandb.run.dir, \"model-best\"), state, save_args=save_args, force=True) # force=True means allow overwritting.\n",
    "\n",
    "    wandb.log({f\"{k}\": wandb.Histogram(v) for k, v in flatten_params(state.params).items()}, commit=False)\n",
    "    wandb.log({\"epoch\": epoch+1, **{name:values[-1] for name, values in metrics_history.items()}})\n",
    "    print(f'Epoch {epoch} -> [Train] Loss: {metrics_history[\"train_loss\"][-1]:.3f} Acc: {metrics_history[\"train_accuracy\"][-1]:.3f} [Val] Loss: {metrics_history[\"val_loss\"][-1]:.3f} Acc: {metrics_history[\"val_accuracy\"][-1]:.3f} || Corr: {metrics_history[\"correlation\"][-1]:.3f}')\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orbax_checkpointer.save(os.path.join(wandb.run.dir, \"model-final\"), state, save_args=save_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cuda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da5141a55de43f9a5c077a362efe5e2ae0cb795b0fc8676e62dbd4f64287ec27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
